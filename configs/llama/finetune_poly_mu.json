{
    "dataset": "alpaca",
    "optimizer": "adamw",
    "learning_rate": 3e-4,
    "micro_batch_size": 1,
    "exp_name": "alpaca_lora_poly_mu",
    "train_dir": "tmp",
    "precision": "bf16",
    "train_batch_size": 32,
    "model": "yahma/llama-7b-hf",
    "lora_rank": 4,
    "num_train_epochs": 3,
    "n_skills": 8,
    "max_input_length": 512,
    "eval_every": 400,
    "warmup_steps": 100,
    "poly_selector": "poly",
    "poly_selector_use_distances": 0,
    "poly_selector_cluster_temp": 0.1,
    "model_modifier": "poly_lora",
    "lora_modules": ".*attn",
    "lora_layers": "v_proj|q_proj|k_proj|o_proj",
    "trainable_param_names": ".*lora_[ab].*",
    "switch_to_average": 1
}