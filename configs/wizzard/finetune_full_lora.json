{       
    "dataset": "wizzard",
    "optimizer": "adamw", 
    "learning_rate": 2e-5, 
    "micro_batch_size": 40,        
    "exp_name": "alpaca_lora_wizzard_full_r4",
    "precision": "bf16",    
    "train_batch_size": 80,
    "model": "yahma/llama-7b-hf",
    "lora_rank": 4,  
    "rank": 4,         
    "num_train_epochs": 3,
    "n_skills": 1,
    "lora_alpha": 16,
    "eval_every": 400,
    "warmup_steps": 100,
    "lora_dropout": 0.05,
    "weight_decay": 0.00,
    "max_input_length": 1028, 
    "use_4_bit_backbone": true,
    "model_modifier": "poly_lora",  
    "lora_modules": ".*attn",  
    "lora_layers": "v_proj|q_proj|k_proj|o_proj",
    "trainable_param_names": ".*lora_[ab].*"             
}