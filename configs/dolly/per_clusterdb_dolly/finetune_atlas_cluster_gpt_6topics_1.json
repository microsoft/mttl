{
    "dataset": "db_dolly", 
    "use_test_set": 1,
    "fast_dev_run": 0,
    "optimizer": "adamw",     
    "learning_rate": 3e-5, 
    "micro_batch_size": 4, 
    "train_only_cluster": 0,               
    "prune_unused_loras": 0,  
    "exp_name": "alpaca_lora_db_dolly_c0",
    "precision": "bf16",  
    "train_batch_size": 24,
    "validation_portion": 0.1,
    "merge_A_B_seperately": 0,
    "model": "yahma/llama-7b-hf",
    "lora_rank": 4,  
    "num_train_epochs": 5,
    "n_skills": 1,
    "share_lora_a": 0,
    "max_input_length": 512,  
    "eval_every": 20,
    "warmup_steps": 20,    
    "same_lora_init": 0,    
    "lora_dropout": 0.05,
    "weight_decay": 0.00,
    "share_lora_at_attn": 0,
    "poly_selector":"cluster_soft",   
    "poly_selector_use_distances": 0,  
    "poly_selector_cluster_temp": 0.1,     
    "model_modifier": "poly_lora",  
    "lora_modules": ".*attn",  
    "lora_layers": "v_proj|q_proj|k_proj|o_proj",   
    "example_to_ids_path": "/mnt/amlt_code/inst_follow/cluster_infos/db_dolly.pkl",
    "trainable_param_names": ".*lora.*",
    "output_dir":"/home/v-oostapenko/logs/llama_alpaca/lora_5k_6topics_c0"
}