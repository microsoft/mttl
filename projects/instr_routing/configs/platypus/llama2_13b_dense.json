{
    "lora_rank": 4,    
    "model": "meta-llama/Llama-2-13b-hf",
    "dataset": "platypus",
    "lora_dropout": 0.05,
    "weight_decay": 0.00,
    "learning_rate": 3e-4,
    "n_skills": 1,  
    "load_in_8bit": 1,  
    "model_family":"gpt", 
    "router_temperature": 0.1,
    "router_teacher_temperature": 0.1,


    "predict_batch_size": 2,
    "micro_batch_size": 1,  
    "train_batch_size": 16,
    "max_input_length": 4096, 
    "model_modifier": "lora",
    "modify_modules": ".*mlp.*",  
    "modify_layers": "gate_proj|down_proj|up_proj",
    "trainable_param_names": ".*lora_[ab].*",  
    "eval_every":400, 
    "num_train_epochs":1,
        
    "eval_hellaswag": 0,
    "eval_arc": 0, 
    "eval_truthfulqa": 0
}
