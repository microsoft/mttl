# Trains a private library of experts for Phi-2

if [ -z "${LIBRARY_PATH}" ]; then
    echo "Error: LIBRARY_PATH is not set, must be of the form hf://username/library_name, or local://path/to/library"
    exit 1
fi

if [ -z "${DATASET_PATH}" ]; then
    echo "Error: DATASET_PATH is not set, must be a valid dataset on huggingface, create the dataset with cli_dataset_create.py"
    exit 1
fi

for task_name in "race_middle_Select_the_best_answer_no_instructions_"  "drop_2_0_0"  "adversarial_qa_dbidaf_generate_question"  "social_i_qa_Show_choices_and_generate_answer"  "qasc_is_correct_2"  "race_middle_Taking_a_test"  "quoref_Guess_Title_For_Context"  "duorc_ParaphraseRC_generate_question_by_answer"  "adversarial_qa_droberta_tell_what_it_is"  "adversarial_qa_dbert_tell_what_it_is"  "wiki_qa_Topic_Prediction_Question_Only"  "wiki_hop_original_choose_best_object_affirmative_3"  "definite_pronoun_resolution_1_1_0"  "super_glue_record_1_0_2"  "unified_qa_science_inst"  "duorc_SelfRC_decide_worth_it"  "wiqa_does_the_supposed_perturbation_have_an_effect"  "cos_e_v1_11_rationale"  "kilt_tasks_hotpotqa_complex_question"  "glue_stsb_2_0_0"  "wiqa_what_might_be_the_first_step_of_the_process"  "app_reviews_convert_to_rating"  "wiki_hop_original_generate_object"  "quartz_read_passage_below_choose"  "race_middle_Write_a_multi_choice_question_for_the_following_article"  "dbpedia_14_given_a_list_of_category_what_does_the_title_belong_to"  "adversarial_qa_dbert_question_context_answer"  "quoref_Given_Context_Answer_Question"  "multi_news_1_0_0"  "duorc_ParaphraseRC_extract_answer"  "anli_r3_0_1_0"  "quoref_Answer_Friend_Question"  "quail_context_question_answer_description_id"  "piqa_1_0_0"  "bool_q_1_0_0"  "ropes_background_new_situation_answer"  "cos_e_v1_11_question_option_description_id"  "race_high_Taking_a_test"  "duorc_SelfRC_answer_question"  "adversarial_qa_dbidaf_question_context_answer"  "true_case"  "cnn_dailymail_3_4_0"  "duorc_ParaphraseRC_build_story_around_qa"  "qasc_is_correct_1"  "web_questions_whats_the_answer"  "race_high_Is_this_the_right_answer"  "wmt16_translate_tr_en_1_0_0"  "quail_context_question_description_text"  "quail_context_question_description_answer_id"  "gem_web_nlg_en_1_1_0"  "adversarial_qa_dbidaf_answer_the_following_q"  "wiki_hop_original_generate_subject_and_object"  "qasc_qa_with_separated_facts_1"  "wiki_hop_original_choose_best_object_affirmative_1"  "paws_wiki_1_1_0"  "duorc_ParaphraseRC_question_answering"  "wiki_qa_found_on_google"  "ag_news_subset_1_0_0"  "wmt16_translate_de_en_1_0_0"  "dream_generate_last_utterance"  "wiki_qa_Jeopardy_style"  "quail_no_prompt_text"  "dream_read_the_following_conversation_and_answer_the_question"  "quoref_Answer_Test"  "race_middle_Is_this_the_right_answer"  "web_questions_question_answer"  "wiki_hop_original_choose_best_object_affirmative_2"  "ropes_plain_background_situation"  "social_i_qa_I_was_wondering"  "squad_v1_1_3_0_0"  "quarel_choose_between"  "wmt16_translate_ro_en_1_0_0"  "coqa_1_0_0"  "duorc_SelfRC_extract_answer"  "adversarial_qa_droberta_generate_question"  "wiki_qa_automatic_system"  "duorc_ParaphraseRC_answer_question"  "qasc_qa_with_separated_facts_4"  "wiki_hop_original_explain_relation"  "sciq_Multiple_Choice"  "quarel_testing_students"  "qasc_qa_with_separated_facts_5"  "fix_punct"  "duorc_ParaphraseRC_movie_director"  "para_crawl_enes"  "duorc_ParaphraseRC_decide_worth_it"  "wiki_hop_original_choose_best_object_interrogative_1"  "cos_e_v1_11_description_question_option_text"  "sciq_Multiple_Choice_Question_First"  "ropes_prompt_bottom_hint_beginning"  "wmt14_translate_fr_en_1_0_0"  "wiki_hop_original_generate_subject"  "ai2_arc_ARC_Easy_1_0_0"  "anli_r1_0_1_0"  "adversarial_qa_dbidaf_tell_what_it_is"  "quail_context_description_question_text"  "quarel_logic_test"  "quoref_Context_Contains_Answer"  "quail_description_context_question_text"  "wiqa_what_might_be_the_last_step_of_the_process"  "quartz_answer_question_below"  "super_glue_copa_1_0_2"  "cos_e_v1_11_question_option_description_text"  "quail_context_description_question_answer_id"  "winogrande_1_1_0"  "yelp_polarity_reviews_0_2_0"  "snli_1_1_0"  "dream_generate_first_utterance"  "duorc_SelfRC_question_answering"  "wiqa_which_of_the_following_is_the_supposed_perturbation"  "adversarial_qa_dbert_answer_the_following_q"  "quoref_What_Is_The_Answer"  "ropes_prompt_beginning"  "quac_1_0_0"  "race_high_Select_the_best_answer_no_instructions_"  "adversarial_qa_droberta_based_on"  "quartz_answer_question_based_on"  "quoref_Found_Context_Online"  "race_high_Select_the_best_answer"  "ropes_new_situation_background_answer"  "kilt_tasks_hotpotqa_combining_facts"  "sciq_Direct_Question_Closed_Book_"  "quail_context_description_question_answer_text"  "qasc_qa_with_combined_facts_1"  "wiki_bio_comprehension"  "race_middle_Write_a_multi_choice_question_options_given_"  "quail_no_prompt_id"  "race_high_Write_a_multi_choice_question_for_the_following_article"  "natural_questions_open_1_0_0"  "super_glue_wsc_fixed_1_0_2"  "quoref_Read_And_Extract_"  "ai2_arc_ARC_Challenge_1_0_0"  "wiqa_what_is_the_final_step_of_the_following_process"  "gigaword_1_2_0"  "ropes_prompt_mix"  "anli_r2_0_1_0"  "super_glue_rte_1_0_2"  "adversarial_qa_droberta_question_context_answer"  "gem_dart_1_1_0"  "duorc_SelfRC_title_generation"  "social_i_qa_Generate_answer"  "glue_wnli_2_0_0"  "kilt_tasks_hotpotqa_formulate"  "dbpedia_14_pick_one_category_for_the_following_text"  "wiki_qa_Topic_Prediction_Answer_Only"  "quartz_given_the_fact_answer_the_q"  "trec_1_0_0"  "sciq_Multiple_Choice_Closed_Book_"  "wiki_qa_exercise"  "glue_qqp_2_0_0"  "dream_answer_to_dialogue"  "dbpedia_14_given_list_what_category_does_the_paragraph_belong_to"  "squad_v2_0_3_0_0"  "duorc_SelfRC_build_story_around_qa"  "wiki_qa_Topic_Prediction_Question_and_Answer_Pair"  "web_questions_short_general_knowledge_q"  "social_i_qa_Show_choices_and_generate_index"  "gem_wiki_lingua_english_en_1_1_0"  "quail_context_question_answer_description_text"  "wiki_qa_Direct_Answer_to_Question"  "trivia_qa_rc_1_1_0"  "glue_mrpc_2_0_0"  "cos_e_v1_11_explain_why_human"  "quartz_use_info_from_paragraph_question"  "quartz_use_info_from_question_paragraph"  "quail_description_context_question_answer_id"  "huggingface_xsum"  "wiki_hop_original_choose_best_object_interrogative_2"  "kilt_tasks_hotpotqa_straighforward_qa"  "quartz_paragraph_question_plain_concat"  "cosmos_qa_1_0_0"  "ropes_prompt_bottom_no_hint"  "glue_sst2_2_0_0"  "social_i_qa_Generate_the_question_from_the_answer"  "math_dataset_algebra__linear_1d_1_0_0"  "aeslc_1_0_0"  "duorc_ParaphraseRC_title_generation"  "race_high_Read_the_article_and_answer_the_question_no_option_"  "wiqa_effect_with_label_answer"  "wiki_qa_Is_This_True_"  "super_glue_cb_1_0_2"  "glue_cola_2_0_0"  "quoref_Find_Answer"  "quarel_heres_a_story"  "app_reviews_generate_review"  "ropes_read_background_situation"  "duorc_SelfRC_generate_question"  "duorc_SelfRC_generate_question_by_answer"  "cos_e_v1_11_question_description_option_text"  "adversarial_qa_dbidaf_based_on"  "adversarial_qa_droberta_answer_the_following_q"  "wmt16_translate_fi_en_1_0_0"  "race_middle_Read_the_article_and_answer_the_question_no_option_"  "quoref_Guess_Answer"  "adversarial_qa_dbert_generate_question"  "wiki_qa_Decide_good_answer"  "cos_e_v1_11_aligned_with_common_sense"  "quarel_do_not_use"  "wiqa_effect_with_string_answer"  "qasc_qa_with_separated_facts_3"  "cos_e_v1_11_question_description_option_id"  "race_high_Write_a_multi_choice_question_options_given_"  "duorc_ParaphraseRC_generate_question"  "wiki_bio_what_content"  "race_middle_Select_the_best_answer_generate_span_"  "quail_context_question_description_answer_text"  "social_i_qa_Check_if_a_random_answer_is_valid_or_not"  "dream_baseline"  "qasc_qa_with_separated_facts_2"  "ropes_plain_bottom_hint"  "cos_e_v1_11_description_question_option_id"  "cos_e_v1_11_generate_explanation_given_text"  "race_high_Select_the_best_answer_generate_span_"  "ropes_given_background_situation"  "sciq_Direct_Question"  "wiki_qa_Generate_Question_from_Topic"  "gem_common_gen_1_1_0"  "imdb_reviews_plain_text_1_0_0"  "word_segment"  "openbookqa_0_1_0"  "super_glue_wic_1_0_2"  "dbpedia_14_given_a_choice_of_categories_"  "ropes_plain_no_background"  "duorc_SelfRC_movie_director"  "lambada_1_0_0"  "wiki_bio_guess_person"  "web_questions_get_the_answer"  "ropes_background_situation_middle"  "kilt_tasks_hotpotqa_final_exam"  "quoref_Answer_Question_Given_Context"  "super_glue_multirc_1_0_2"  "glue_mnli_2_0_0"  "gem_e2e_nlg_1_1_0"  "cos_e_v1_11_i_think"  "adversarial_qa_dbert_based_on"  "hellaswag_1_1_0"  "race_middle_Select_the_best_answer"  "wiki_bio_key_content"  "quail_description_context_question_answer_text"  "wiqa_what_is_the_missing_first_step"  "app_reviews_convert_to_star_rating"  "app_reviews_categorize_rating_using_review"  "quartz_having_read_above_passage"  "wiki_bio_who"  "web_questions_potential_correct_answer"  "glue_qnli_2_0_0"  "cot_qasc_ii"  "cot_esnli"  "cot_creak_ii"  "cot_sensemaking"  "cot_gsm8k_ii"  "stream_aqua"  "cot_gsm8k"  "cot_ecqa"  "cot_esnli_ii"  "cot_strategyqa"  "cot_sensemaking_ii"  "stream_qed"  "cot_strategyqa_ii"  "stream_qed_ii"  "cot_creak"  "cot_qasc"  "stream_aqua_ii"  "cot_ecqa_ii"; do
    CUDA_VISIBLE_DEVICES=0 python train_experts_main.py \
        -c configs/models/phi-2_hf.json \
        -k remove_phi_eval_tasks=False \
        output_dir=output/${task_name}/ \
        finetune_task_name=${task_name} \
        expert_name=${task_name} \
        dataset=$DATASET_PATH \
        library_id=$LIBRARY_PATH
done
