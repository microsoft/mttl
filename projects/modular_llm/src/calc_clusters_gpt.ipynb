{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/comp_3.9/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/anaconda/envs/comp_3.9/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda9SetDeviceEi'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering modifier...lora\n",
      "Registering modifier...poly\n",
      "Registering modifier...per_token_poly\n",
      "Registering modifier...skilled\n",
      "Registering modifier...kv_adapter\n",
      "Registering modifier...poly_kv_adapter\n",
      "Registering modifier...prompt_tuning\n",
      "Registering modifier...poly_prompt_tuning\n",
      "Registering modifier...hard_prompt\n",
      "Registering multi-expert selector...poly_router\n",
      "Registering multi-expert selector...moe_rkhs_router\n",
      "Registering multi-expert selector...zero_router\n",
      "Registering multi-expert selector...zero_per_token_router\n",
      "Registering multi-expert selector...poly_router_dir\n",
      "Registering multi-expert selector...uniform\n",
      "Registering multi-expert selector...info_selector\n",
      "Registering multi-expert selector...task_selector\n",
      "Registering multi-expert selector...kv_task_selector\n",
      "Registering multi-expert selector...kv_concat_selector\n",
      "Registering multi-expert selector...kv_norm_selector\n",
      "Registering multi-expert selector...kv_concat_norm_selector\n",
      "Registering multi-expert selector...kv_task_norm_selector\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from collections import defaultdict\n",
    "from mttl.models.library.utils import get_svd_embedding\n",
    "\n",
    "from mttl.models.library.expert_library import LocalExpertLibrary,  HFExpertLibrary\n",
    "from mttl.models.library.library_transforms import SVDEmbeddingTransform, SVDEmbeddingTransformConfig\n",
    "from huggingface_hub import login, HfApi\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create clsuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/v-oostapenko/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "hf_api_key = os.environ[\"HF_TOKEN\"]\n",
    "login(token=hf_api_key)\n",
    "user = HfApi(token=hf_api_key).whoami()\n",
    "# hf_repo_id = \"ostapeno/library-gptneo_1B_flan_2ep\"\n",
    "# hf_repo_id = \"ostapeno/library-gptneo_1B_flan_2ep_underparam\"\n",
    "# hf_repo_id = \"ostapeno/library-stablelm_flan_5ep\"\n",
    "hf_repo_id = \"sordonia/library-phi_2-v3-2epc\"\n",
    "\n",
    "local_lib_location = f\"/tmp/{hf_repo_id}\"\n",
    "if not os.path.exists(local_lib_location):\n",
    "    os.makedirs(local_lib_location)\n",
    "    expert_lib: LocalExpertLibrary = LocalExpertLibrary.from_expert_library(\n",
    "        HFExpertLibrary(hf_repo_id), local_lib_location\n",
    "    )\n",
    "else:\n",
    "    expert_lib: LocalExpertLibrary = LocalExpertLibrary(local_lib_location)\n",
    "\n",
    "# import json\n",
    "# task_set_path = \"/home/v-oostapenko/dev/lucas_mttl/projects/wiki_experts/task_sets/flan_tasks.json\"\n",
    "# flan256 = json.load(open(task_set_path))[\"flan256\"]\n",
    "\n",
    "# for expert_name, expert in list(expert_lib.data.items()):\n",
    "#     if expert.expert_task_name not in flan256:\n",
    "#         expert_lib.remove_expert(expert_name)\n",
    "# assert len(expert_lib) == 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"glue_sst2_2_0_0\"\n",
      "\"dream_read_the_following_conversation_and_answer_the_question\"\n",
      "\"race_middle_Read_the_article_and_answer_the_question_no_option_\"\n",
      "\"adversarial_qa_droberta_generate_question\"\n",
      "\"adversarial_qa_dbidaf_question_context_answer\"\n",
      "\"app_reviews_convert_to_star_rating\"\n",
      "\"race_high_Select_the_best_answer\"\n",
      "\"super_glue_rte_1_0_2\"\n",
      "\"true_case\"\n",
      "\"wiqa_what_might_be_the_first_step_of_the_process\"\n",
      "\"quail_description_context_question_answer_id\"\n",
      "\"quail_context_question_description_text\"\n",
      "\"stream_qed\"\n",
      "\"huggingface_xsum\"\n",
      "\"cos_e_v1_11_question_option_description_text\"\n",
      "\"wiqa_what_is_the_final_step_of_the_following_process\"\n",
      "\"ropes_background_new_situation_answer\"\n",
      "\"wiki_qa_found_on_google\"\n",
      "\"cot_esnli\"\n",
      "\"social_i_qa_Show_choices_and_generate_answer\"\n",
      "\"cot_gsm8k\"\n",
      "\"app_reviews_categorize_rating_using_review\"\n",
      "\"cot_sensemaking\"\n",
      "\"trec_1_0_0\"\n",
      "\"super_glue_wic_1_0_2\"\n",
      "\"ropes_prompt_bottom_no_hint\"\n",
      "\"quartz_answer_question_based_on\"\n",
      "\"super_glue_record_1_0_2\"\n",
      "\"yelp_polarity_reviews_0_2_0\"\n",
      "\"race_middle_Is_this_the_right_answer\"\n",
      "\"quoref_Context_Contains_Answer\"\n",
      "\"cos_e_v1_11_rationale\"\n",
      "\"natural_questions_open_1_0_0\"\n",
      "\"ropes_plain_background_situation\"\n",
      "\"web_questions_whats_the_answer\"\n",
      "\"race_high_Read_the_article_and_answer_the_question_no_option_\"\n",
      "\"anli_r3_0_1_0\"\n",
      "\"duorc_SelfRC_generate_question_by_answer\"\n",
      "\"quoref_Find_Answer\"\n",
      "\"duorc_ParaphraseRC_movie_director\"\n",
      "\"sciq_Direct_Question_Closed_Book_\"\n",
      "\"qasc_qa_with_separated_facts_3\"\n",
      "\"lambada_1_0_0\"\n",
      "\"quartz_given_the_fact_answer_the_q\"\n",
      "\"super_glue_cb_1_0_2\"\n",
      "\"quartz_answer_question_below\"\n",
      "\"duorc_ParaphraseRC_answer_question\"\n",
      "\"wmt16_translate_ro_en_1_0_0\"\n",
      "\"dream_generate_last_utterance\"\n",
      "\"wiki_qa_Topic_Prediction_Answer_Only\"\n",
      "\"kilt_tasks_hotpotqa_final_exam\"\n",
      "\"glue_cola_2_0_0\"\n",
      "\"race_high_Select_the_best_answer_no_instructions_\"\n",
      "\"quail_context_description_question_answer_id\"\n",
      "\"ag_news_subset_1_0_0\"\n",
      "\"paws_wiki_1_1_0\"\n",
      "\"sciq_Multiple_Choice\"\n",
      "\"wiki_qa_Direct_Answer_to_Question\"\n",
      "\"gem_dart_1_1_0\"\n",
      "\"cos_e_v1_11_generate_explanation_given_text\"\n",
      "\"wiki_hop_original_generate_object\"\n",
      "\"race_high_Taking_a_test\"\n",
      "\"wiqa_what_might_be_the_last_step_of_the_process\"\n",
      "\"wiki_bio_key_content\"\n",
      "\"quoref_Found_Context_Online\"\n",
      "\"super_glue_wsc_fixed_1_0_2\"\n",
      "\"wiqa_does_the_supposed_perturbation_have_an_effect\"\n",
      "\"adversarial_qa_droberta_tell_what_it_is\"\n",
      "\"cos_e_v1_11_question_description_option_text\"\n",
      "\"gem_common_gen_1_1_0\"\n",
      "\"quoref_Read_And_Extract_\"\n",
      "\"cot_creak\"\n",
      "\"cot_gsm8k_ii\"\n",
      "\"duorc_ParaphraseRC_title_generation\"\n",
      "\"wiki_qa_Is_This_True_\"\n",
      "\"math_dataset_algebra__linear_1d_1_0_0\"\n",
      "\"unified_qa_science_inst\"\n",
      "\"quartz_use_info_from_question_paragraph\"\n",
      "\"web_questions_question_answer\"\n",
      "\"duorc_ParaphraseRC_decide_worth_it\"\n",
      "\"stream_aqua\"\n",
      "\"dbpedia_14_pick_one_category_for_the_following_text\"\n",
      "\"super_glue_multirc_1_0_2\"\n",
      "\"dbpedia_14_given_a_choice_of_categories_\"\n",
      "\"sciq_Direct_Question\"\n",
      "\"kilt_tasks_hotpotqa_combining_facts\"\n",
      "\"quoref_What_Is_The_Answer\"\n",
      "\"web_questions_short_general_knowledge_q\"\n",
      "\"qasc_qa_with_separated_facts_2\"\n",
      "\"wiqa_which_of_the_following_is_the_supposed_perturbation\"\n",
      "\"cnn_dailymail_3_4_0\"\n",
      "\"duorc_ParaphraseRC_generate_question\"\n",
      "\"race_middle_Select_the_best_answer\"\n",
      "\"kilt_tasks_hotpotqa_straighforward_qa\"\n",
      "\"duorc_SelfRC_build_story_around_qa\"\n",
      "\"adversarial_qa_dbidaf_generate_question\"\n",
      "\"snli_1_1_0\"\n",
      "\"app_reviews_convert_to_rating\"\n",
      "\"wiki_hop_original_choose_best_object_affirmative_3\"\n",
      "\"quail_context_question_description_answer_id\"\n",
      "\"cos_e_v1_11_i_think\"\n",
      "\"quoref_Guess_Title_For_Context\"\n",
      "\"quac_1_0_0\"\n",
      "\"cos_e_v1_11_question_option_description_id\"\n",
      "\"quoref_Answer_Test\"\n",
      "\"wiki_hop_original_choose_best_object_interrogative_1\"\n",
      "\"duorc_SelfRC_question_answering\"\n",
      "\"wiki_hop_original_explain_relation\"\n",
      "\"ropes_new_situation_background_answer\"\n",
      "\"dbpedia_14_given_list_what_category_does_the_paragraph_belong_to\"\n",
      "\"race_high_Is_this_the_right_answer\"\n",
      "\"quail_description_context_question_answer_text\"\n",
      "\"cot_strategyqa\"\n",
      "\"ropes_given_background_situation\"\n",
      "\"quail_context_question_answer_description_text\"\n",
      "\"cot_ecqa_ii\"\n",
      "\"ropes_prompt_bottom_hint_beginning\"\n",
      "\"gem_wiki_lingua_english_en_1_1_0\"\n",
      "\"glue_qqp_2_0_0\"\n",
      "\"fix_punct\"\n",
      "\"wiqa_effect_with_string_answer\"\n",
      "\"adversarial_qa_droberta_based_on\"\n",
      "\"imdb_reviews_plain_text_1_0_0\"\n",
      "\"race_high_Select_the_best_answer_generate_span_\"\n",
      "\"race_middle_Select_the_best_answer_generate_span_\"\n",
      "\"race_middle_Write_a_multi_choice_question_for_the_following_article\"\n",
      "\"quarel_do_not_use\"\n",
      "\"duorc_SelfRC_title_generation\"\n",
      "\"qasc_qa_with_separated_facts_5\"\n",
      "\"wiki_qa_exercise\"\n",
      "\"duorc_ParaphraseRC_generate_question_by_answer\"\n",
      "\"web_questions_get_the_answer\"\n",
      "\"wiki_hop_original_choose_best_object_affirmative_1\"\n",
      "\"duorc_ParaphraseRC_extract_answer\"\n",
      "\"dream_baseline\"\n",
      "\"adversarial_qa_dbert_answer_the_following_q\"\n",
      "\"gigaword_1_2_0\"\n",
      "\"ropes_prompt_beginning\"\n",
      "\"quail_context_question_answer_description_id\"\n",
      "\"duorc_SelfRC_answer_question\"\n",
      "\"kilt_tasks_hotpotqa_complex_question\"\n",
      "\"quartz_having_read_above_passage\"\n",
      "\"quail_context_description_question_answer_text\"\n",
      "\"cos_e_v1_11_question_description_option_id\"\n",
      "\"ropes_read_background_situation\"\n",
      "\"wiki_hop_original_choose_best_object_interrogative_2\"\n",
      "\"dbpedia_14_given_a_list_of_category_what_does_the_title_belong_to\"\n",
      "\"gem_web_nlg_en_1_1_0\"\n",
      "\"adversarial_qa_droberta_question_context_answer\"\n",
      "\"qasc_qa_with_separated_facts_1\"\n",
      "\"wiki_qa_automatic_system\"\n",
      "\"ropes_plain_bottom_hint\"\n",
      "\"duorc_SelfRC_decide_worth_it\"\n",
      "\"duorc_ParaphraseRC_question_answering\"\n",
      "\"cos_e_v1_11_explain_why_human\"\n",
      "\"word_segment\"\n",
      "\"cot_creak_ii\"\n",
      "\"anli_r2_0_1_0\"\n",
      "\"cos_e_v1_11_description_question_option_text\"\n",
      "\"quarel_heres_a_story\"\n",
      "\"qasc_qa_with_combined_facts_1\"\n",
      "\"app_reviews_generate_review\"\n",
      "\"wiki_bio_what_content\"\n",
      "\"race_high_Write_a_multi_choice_question_for_the_following_article\"\n",
      "\"qasc_is_correct_1\"\n",
      "\"quoref_Answer_Question_Given_Context\"\n",
      "\"squad_v2_0_3_0_0\"\n",
      "\"web_questions_potential_correct_answer\"\n",
      "\"trivia_qa_rc_1_1_0\"\n",
      "\"wmt16_translate_de_en_1_0_0\"\n",
      "\"cos_e_v1_11_description_question_option_id\"\n",
      "\"wiki_hop_original_generate_subject\"\n",
      "\"ropes_plain_no_background\"\n",
      "\"quarel_choose_between\"\n",
      "\"stream_qed_ii\"\n",
      "\"wiki_bio_guess_person\"\n",
      "\"anli_r1_0_1_0\"\n",
      "\"quail_context_description_question_text\"\n",
      "\"cot_ecqa\"\n",
      "\"quail_context_question_description_answer_text\"\n",
      "\"wiki_bio_who\"\n",
      "\"wiki_qa_Topic_Prediction_Question_Only\"\n",
      "\"glue_stsb_2_0_0\"\n",
      "\"cos_e_v1_11_aligned_with_common_sense\"\n",
      "\"aeslc_1_0_0\"\n",
      "\"dream_generate_first_utterance\"\n",
      "\"wmt16_translate_fi_en_1_0_0\"\n",
      "\"adversarial_qa_dbidaf_answer_the_following_q\"\n",
      "\"dream_answer_to_dialogue\"\n",
      "\"glue_qnli_2_0_0\"\n",
      "\"adversarial_qa_droberta_answer_the_following_q\"\n",
      "\"cot_sensemaking_ii\"\n",
      "\"adversarial_qa_dbert_tell_what_it_is\"\n",
      "\"glue_mnli_2_0_0\"\n",
      "\"quail_description_context_question_text\"\n",
      "\"super_glue_copa_1_0_2\"\n",
      "\"social_i_qa_Check_if_a_random_answer_is_valid_or_not\"\n",
      "\"social_i_qa_Generate_the_question_from_the_answer\"\n",
      "\"social_i_qa_Show_choices_and_generate_index\"\n",
      "\"kilt_tasks_hotpotqa_formulate\"\n",
      "\"gem_e2e_nlg_1_1_0\"\n",
      "\"para_crawl_enes\"\n",
      "\"duorc_SelfRC_extract_answer\"\n",
      "\"sciq_Multiple_Choice_Closed_Book_\"\n",
      "\"race_high_Write_a_multi_choice_question_options_given_\"\n",
      "\"race_middle_Taking_a_test\"\n",
      "\"social_i_qa_I_was_wondering\"\n",
      "\"adversarial_qa_dbert_generate_question\"\n",
      "\"quoref_Guess_Answer\"\n",
      "\"race_middle_Write_a_multi_choice_question_options_given_\"\n",
      "\"quartz_use_info_from_paragraph_question\"\n",
      "\"quoref_Answer_Friend_Question\"\n",
      "\"qasc_is_correct_2\"\n",
      "\"wmt14_translate_fr_en_1_0_0\"\n",
      "\"quarel_testing_students\"\n",
      "\"wiki_hop_original_choose_best_object_affirmative_2\"\n",
      "\"qasc_qa_with_separated_facts_4\"\n",
      "\"duorc_SelfRC_movie_director\"\n",
      "\"wiki_qa_Topic_Prediction_Question_and_Answer_Pair\"\n",
      "\"cosmos_qa_1_0_0\"\n",
      "\"cot_esnli_ii\"\n",
      "\"quail_no_prompt_id\"\n",
      "\"wmt16_translate_tr_en_1_0_0\"\n",
      "\"wiki_qa_Decide_good_answer\"\n",
      "\"wiki_qa_Jeopardy_style\"\n",
      "\"adversarial_qa_dbert_based_on\"\n",
      "\"duorc_SelfRC_generate_question\"\n",
      "\"wiki_qa_Generate_Question_from_Topic\"\n",
      "\"wiki_hop_original_generate_subject_and_object\"\n",
      "\"adversarial_qa_dbidaf_based_on\"\n",
      "\"wiqa_what_is_the_missing_first_step\"\n",
      "\"quartz_read_passage_below_choose\"\n",
      "\"definite_pronoun_resolution_1_1_0\"\n",
      "\"quail_no_prompt_text\"\n",
      "\"wiqa_effect_with_label_answer\"\n",
      "\"drop_2_0_0\"\n",
      "\"race_middle_Select_the_best_answer_no_instructions_\"\n",
      "\"glue_wnli_2_0_0\"\n",
      "\"wiki_bio_comprehension\"\n",
      "\"glue_mrpc_2_0_0\"\n",
      "\"cot_qasc\"\n",
      "\"adversarial_qa_dbert_question_context_answer\"\n",
      "\"quoref_Given_Context_Answer_Question\"\n",
      "\"coqa_1_0_0\"\n",
      "\"quartz_paragraph_question_plain_concat\"\n",
      "\"adversarial_qa_dbidaf_tell_what_it_is\"\n",
      "\"ropes_prompt_mix\"\n",
      "\"social_i_qa_Generate_answer\"\n",
      "\"cot_strategyqa_ii\"\n",
      "\"quarel_logic_test\"\n",
      "\"duorc_ParaphraseRC_build_story_around_qa\"\n",
      "\"stream_aqua_ii\"\n",
      "\"multi_news_1_0_0\"\n",
      "\"ropes_background_situation_middle\"\n",
      "\"sciq_Multiple_Choice_Question_First\"\n",
      "\"squad_v1_1_3_0_0\"\n"
     ]
    }
   ],
   "source": [
    "for name, exp in expert_lib.data.items():\n",
    "    # print task\n",
    "    print(f\"\\\"{exp.expert_task_name}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:04<00:00, 53.89it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_embeddings():\n",
    "    svd_embedder = SVDEmbeddingTransform(\n",
    "        SVDEmbeddingTransformConfig(sparsity_threshold=0.1),\n",
    "        random_state=42,\n",
    "    )\n",
    "    embeddings, svd = svd_embedder.transform(expert_lib, persist=True, force=True)\n",
    "    del svd_embedder\n",
    "    return embeddings, svd\n",
    "\n",
    "\n",
    "embeds = expert_lib.get_auxiliary_data(\"embeddings\")\n",
    "# if len(embeds) == 0:\n",
    "print(\"creating embeddings\")\n",
    "_, svd = create_embeddings()\n",
    "\n",
    "# module to embedding\n",
    "module2embed = {}\n",
    "for n, m in expert_lib.items():\n",
    "    module2embed[n] = get_svd_embedding(expert_lib, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the embeddings as a numpy array\n",
    "embeddings = np.array(list(module2embed.values()))\n",
    "cosine_sim_matrix = cosine_similarity(embeddings, embeddings)\n",
    "K = 5\n",
    "kmeans = KMeans(n_clusters=K, init=\"k-means++\", n_init=10, random_state=42)\n",
    "kmeans.fit(cosine_sim_matrix)\n",
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clusters \u001b[38;5;241m=\u001b[39m \u001b[43mdefaultdict\u001b[49m(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Print the cluster labels for each embedding\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(module2embed\u001b[38;5;241m.\u001b[39mkeys(), cluster_labels):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "clusters = defaultdict(list)\n",
    "# Print the cluster labels for each embedding\n",
    "for key, label in zip(module2embed.keys(), cluster_labels):\n",
    "    clusters[label].append(key)\n",
    "\n",
    "for c, l in clusters.items():\n",
    "    # print(f\"Cluster {c} has {len(l)} elements\")\n",
    "    print(f\"c{c}o{K}_2e = {l}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced k-means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from kmeans_pytorch import KMeans\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "num_clusters = 10\n",
    "balanced = True\n",
    "debug = True\n",
    "# set random seed\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "embeddings = np.array(list(module2embed.values()))\n",
    "cosine_sim_matrix = cosine_similarity(embeddings, embeddings)\n",
    "X = torch.from_numpy(embeddings) #torch.from_numpy(cosine_sim_matrix).to(device)\n",
    "kmeans = KMeans(n_clusters=num_clusters, device=device, balanced=balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cuda:0..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d263d454c96a4d7382306ea44c28d27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[running kmeans]: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster_ids = kmeans.fit(\n",
    "    X=X, distance='cosine', iter_limit=100, tqdm_flag=True, online=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 4, 9, 2, 6, 1, 9, 8, 7, 5, 9, 9, 0, 0, 4, 2, 3, 8, 8, 8, 1, 8, 4, 7,\n",
       "        7, 3, 5, 0, 7, 9, 3, 5, 2, 3, 2, 9, 8, 6, 3, 6, 5, 5, 0, 5, 0, 5, 6, 7,\n",
       "        0, 2, 2, 7, 9, 9, 7, 7, 4, 2, 1, 5, 1, 9, 5, 0, 3, 7, 5, 6, 4, 1, 3, 8,\n",
       "        1, 6, 8, 7, 2, 5, 2, 6, 1, 1, 0, 1, 5, 2, 3, 2, 4, 5, 0, 6, 9, 2, 0, 2,\n",
       "        8, 1, 1, 9, 5, 0, 0, 4, 3, 1, 6, 1, 3, 1, 9, 9, 8, 3, 9, 4, 3, 0, 7, 7,\n",
       "        5, 6, 7, 9, 9, 0, 4, 6, 5, 8, 6, 2, 1, 6, 4, 3, 7, 3, 9, 6, 2, 5, 9, 4,\n",
       "        3, 1, 1, 1, 6, 4, 8, 3, 6, 6, 5, 7, 8, 8, 4, 4, 4, 2, 0, 0, 8, 3, 0, 2,\n",
       "        2, 7, 4, 1, 3, 4, 0, 1, 8, 9, 4, 9, 1, 2, 7, 5, 0, 0, 7, 3, 0, 8, 3, 4,\n",
       "        6, 8, 9, 7, 8, 2, 4, 2, 1, 7, 6, 4, 0, 9, 8, 2, 3, 0, 5, 3, 8, 7, 4, 1,\n",
       "        4, 6, 2, 0, 8, 9, 7, 8, 2, 6, 6, 2, 1, 6, 5, 5, 7, 9, 5, 0, 9, 8, 0, 7,\n",
       "        4, 6, 3, 0, 5, 6, 3, 8, 0, 4, 0, 1, 0, 3, 5, 0])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c7o10_2e = ['glue_sst2_2_0_0', 'true_case', 'trec_1_0_0', 'super_glue_wic_1_0_2', 'yelp_polarity_reviews_0_2_0', 'wmt16_translate_ro_en_1_0_0', 'glue_cola_2_0_0', 'ag_news_subset_1_0_0', 'paws_wiki_1_1_0', 'super_glue_wsc_fixed_1_0_2', 'math_dataset_algebra__linear_1d_1_0_0', 'glue_qqp_2_0_0', 'fix_punct', 'imdb_reviews_plain_text_1_0_0', 'gigaword_1_2_0', 'word_segment', 'wmt16_translate_de_en_1_0_0', 'glue_stsb_2_0_0', 'wmt16_translate_fi_en_1_0_0', 'super_glue_copa_1_0_2', 'para_crawl_enes', 'wmt14_translate_fr_en_1_0_0', 'wmt16_translate_tr_en_1_0_0', 'definite_pronoun_resolution_1_1_0', 'glue_mrpc_2_0_0']\n",
      "c4o10_2e = ['dream_read_the_following_conversation_and_answer_the_question', 'cos_e_v1_11_question_option_description_text', 'cot_sensemaking', 'sciq_Multiple_Choice', 'cos_e_v1_11_question_description_option_text', 'qasc_qa_with_separated_facts_2', 'cos_e_v1_11_question_option_description_id', 'cot_ecqa_ii', 'quarel_do_not_use', 'dream_baseline', 'cos_e_v1_11_question_description_option_id', 'qasc_qa_with_separated_facts_1', 'cos_e_v1_11_description_question_option_text', 'quarel_heres_a_story', 'qasc_qa_with_combined_facts_1', 'cos_e_v1_11_description_question_option_id', 'quarel_choose_between', 'cot_ecqa', 'cot_sensemaking_ii', 'social_i_qa_Show_choices_and_generate_index', 'sciq_Multiple_Choice_Closed_Book_', 'quarel_testing_students', 'qasc_qa_with_separated_facts_4', 'cot_qasc', 'quarel_logic_test']\n",
      "c9o10_2e = ['race_middle_Read_the_article_and_answer_the_question_no_option_', 'race_high_Select_the_best_answer', 'quail_description_context_question_answer_id', 'quail_context_question_description_text', 'race_middle_Is_this_the_right_answer', 'race_high_Read_the_article_and_answer_the_question_no_option_', 'race_high_Select_the_best_answer_no_instructions_', 'quail_context_description_question_answer_id', 'race_high_Taking_a_test', 'race_middle_Select_the_best_answer', 'quail_context_question_description_answer_id', 'race_high_Is_this_the_right_answer', 'quail_description_context_question_answer_text', 'quail_context_question_answer_description_text', 'race_high_Select_the_best_answer_generate_span_', 'race_middle_Select_the_best_answer_generate_span_', 'quail_context_question_answer_description_id', 'quail_context_description_question_answer_text', 'quail_context_description_question_text', 'quail_context_question_description_answer_text', 'quail_description_context_question_text', 'race_middle_Taking_a_test', 'quail_no_prompt_id', 'quail_no_prompt_text', 'race_middle_Select_the_best_answer_no_instructions_']\n",
      "c2o10_2e = ['adversarial_qa_droberta_generate_question', 'wiqa_what_is_the_final_step_of_the_following_process', 'natural_questions_open_1_0_0', 'web_questions_whats_the_answer', 'wiki_qa_Topic_Prediction_Answer_Only', 'kilt_tasks_hotpotqa_final_exam', 'wiki_qa_Direct_Answer_to_Question', 'unified_qa_science_inst', 'web_questions_question_answer', 'kilt_tasks_hotpotqa_combining_facts', 'web_questions_short_general_knowledge_q', 'kilt_tasks_hotpotqa_straighforward_qa', 'adversarial_qa_dbidaf_generate_question', 'web_questions_get_the_answer', 'kilt_tasks_hotpotqa_complex_question', 'app_reviews_generate_review', 'web_questions_potential_correct_answer', 'trivia_qa_rc_1_1_0', 'wiki_qa_Topic_Prediction_Question_Only', 'social_i_qa_Generate_the_question_from_the_answer', 'kilt_tasks_hotpotqa_formulate', 'adversarial_qa_dbert_generate_question', 'wiki_qa_Topic_Prediction_Question_and_Answer_Pair', 'wiki_qa_Jeopardy_style', 'wiki_qa_Generate_Question_from_Topic']\n",
      "c6o10_2e = ['adversarial_qa_dbidaf_question_context_answer', 'duorc_SelfRC_generate_question_by_answer', 'duorc_ParaphraseRC_movie_director', 'duorc_ParaphraseRC_answer_question', 'adversarial_qa_droberta_tell_what_it_is', 'duorc_ParaphraseRC_title_generation', 'duorc_ParaphraseRC_decide_worth_it', 'duorc_ParaphraseRC_generate_question', 'duorc_SelfRC_question_answering', 'adversarial_qa_droberta_based_on', 'duorc_SelfRC_title_generation', 'duorc_ParaphraseRC_generate_question_by_answer', 'duorc_ParaphraseRC_extract_answer', 'duorc_SelfRC_answer_question', 'adversarial_qa_droberta_question_context_answer', 'duorc_SelfRC_decide_worth_it', 'duorc_ParaphraseRC_question_answering', 'adversarial_qa_dbert_tell_what_it_is', 'duorc_SelfRC_extract_answer', 'duorc_SelfRC_movie_director', 'adversarial_qa_dbert_based_on', 'duorc_SelfRC_generate_question', 'adversarial_qa_dbidaf_based_on', 'adversarial_qa_dbert_question_context_answer', 'adversarial_qa_dbidaf_tell_what_it_is']\n",
      "c1o10_2e = ['app_reviews_convert_to_star_rating', 'cot_gsm8k', 'gem_dart_1_1_0', 'wiki_hop_original_generate_object', 'gem_common_gen_1_1_0', 'cot_gsm8k_ii', 'stream_aqua', 'dbpedia_14_pick_one_category_for_the_following_text', 'dbpedia_14_given_a_choice_of_categories_', 'app_reviews_convert_to_rating', 'wiki_hop_original_choose_best_object_affirmative_3', 'wiki_hop_original_choose_best_object_interrogative_1', 'wiki_hop_original_explain_relation', 'dbpedia_14_given_list_what_category_does_the_paragraph_belong_to', 'wiki_hop_original_choose_best_object_affirmative_1', 'wiki_hop_original_choose_best_object_interrogative_2', 'dbpedia_14_given_a_list_of_category_what_does_the_title_belong_to', 'gem_web_nlg_en_1_1_0', 'wiki_hop_original_generate_subject', 'wiki_bio_guess_person', 'wiki_bio_who', 'gem_e2e_nlg_1_1_0', 'wiki_hop_original_choose_best_object_affirmative_2', 'wiki_hop_original_generate_subject_and_object', 'stream_aqua_ii']\n",
      "c8o10_2e = ['super_glue_rte_1_0_2', 'wiki_qa_found_on_google', 'cot_esnli', 'social_i_qa_Show_choices_and_generate_answer', 'app_reviews_categorize_rating_using_review', 'anli_r3_0_1_0', 'cot_creak', 'wiki_qa_Is_This_True_', 'snli_1_1_0', 'cot_strategyqa', 'wiki_qa_exercise', 'wiki_qa_automatic_system', 'cot_creak_ii', 'anli_r2_0_1_0', 'qasc_is_correct_1', 'anli_r1_0_1_0', 'glue_qnli_2_0_0', 'glue_mnli_2_0_0', 'social_i_qa_Check_if_a_random_answer_is_valid_or_not', 'social_i_qa_I_was_wondering', 'qasc_is_correct_2', 'cot_esnli_ii', 'wiki_qa_Decide_good_answer', 'glue_wnli_2_0_0', 'social_i_qa_Generate_answer']\n",
      "c5o10_2e = ['wiqa_what_might_be_the_first_step_of_the_process', 'quartz_answer_question_based_on', 'cos_e_v1_11_rationale', 'sciq_Direct_Question_Closed_Book_', 'qasc_qa_with_separated_facts_3', 'quartz_given_the_fact_answer_the_q', 'quartz_answer_question_below', 'cos_e_v1_11_generate_explanation_given_text', 'wiqa_what_might_be_the_last_step_of_the_process', 'wiqa_does_the_supposed_perturbation_have_an_effect', 'quartz_use_info_from_question_paragraph', 'sciq_Direct_Question', 'wiqa_which_of_the_following_is_the_supposed_perturbation', 'cos_e_v1_11_i_think', 'wiqa_effect_with_string_answer', 'qasc_qa_with_separated_facts_5', 'quartz_having_read_above_passage', 'cos_e_v1_11_explain_why_human', 'cos_e_v1_11_aligned_with_common_sense', 'quartz_use_info_from_paragraph_question', 'wiqa_what_is_the_missing_first_step', 'quartz_read_passage_below_choose', 'wiqa_effect_with_label_answer', 'quartz_paragraph_question_plain_concat', 'sciq_Multiple_Choice_Question_First']\n",
      "c0o10_2e = ['stream_qed', 'huggingface_xsum', 'super_glue_record_1_0_2', 'lambada_1_0_0', 'super_glue_cb_1_0_2', 'dream_generate_last_utterance', 'wiki_bio_key_content', 'super_glue_multirc_1_0_2', 'cnn_dailymail_3_4_0', 'duorc_SelfRC_build_story_around_qa', 'quoref_Guess_Title_For_Context', 'quac_1_0_0', 'gem_wiki_lingua_english_en_1_1_0', 'race_middle_Write_a_multi_choice_question_for_the_following_article', 'wiki_bio_what_content', 'race_high_Write_a_multi_choice_question_for_the_following_article', 'squad_v2_0_3_0_0', 'stream_qed_ii', 'aeslc_1_0_0', 'dream_generate_first_utterance', 'dream_answer_to_dialogue', 'race_high_Write_a_multi_choice_question_options_given_', 'race_middle_Write_a_multi_choice_question_options_given_', 'cosmos_qa_1_0_0', 'drop_2_0_0', 'wiki_bio_comprehension', 'coqa_1_0_0', 'cot_strategyqa_ii', 'duorc_ParaphraseRC_build_story_around_qa', 'multi_news_1_0_0', 'squad_v1_1_3_0_0']\n",
      "c3o10_2e = ['ropes_background_new_situation_answer', 'ropes_prompt_bottom_no_hint', 'quoref_Context_Contains_Answer', 'ropes_plain_background_situation', 'quoref_Find_Answer', 'quoref_Found_Context_Online', 'quoref_Read_And_Extract_', 'quoref_What_Is_The_Answer', 'quoref_Answer_Test', 'ropes_new_situation_background_answer', 'ropes_given_background_situation', 'ropes_prompt_bottom_hint_beginning', 'adversarial_qa_dbert_answer_the_following_q', 'ropes_prompt_beginning', 'ropes_read_background_situation', 'ropes_plain_bottom_hint', 'quoref_Answer_Question_Given_Context', 'ropes_plain_no_background', 'adversarial_qa_dbidaf_answer_the_following_q', 'adversarial_qa_droberta_answer_the_following_q', 'quoref_Guess_Answer', 'quoref_Answer_Friend_Question', 'quoref_Given_Context_Answer_Question', 'ropes_prompt_mix', 'ropes_background_situation_middle']\n"
     ]
    }
   ],
   "source": [
    "clusters = defaultdict(list)\n",
    "# Print the cluster labels for each embedding\n",
    "for key, label in zip(module2embed.keys(), list(cluster_ids)):\n",
    "    clusters[label.item()].append(key)\n",
    "\n",
    "for c, l in clusters.items():\n",
    "    # print(f\"Cluster {c} has {len(l)} elements\")\n",
    "    print(f\"c{c}o{num_clusters}_2e = {l}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rand score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47611902409439244"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_rand_score(cluster_labels, cluster_labels_direct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save svd file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_api_key = os.environ[\"HF_TOKEN\"]\n",
    "login(token=hf_api_key)\n",
    "user = HfApi(token=hf_api_key).whoami()\n",
    "hf_repo_id = \"ostapeno/library-phi_2-v3-10-flan-clusters\"\n",
    "local_lib_location = f\"/tmp/{hf_repo_id}\"\n",
    "if not os.path.exists(local_lib_location):\n",
    "    os.makedirs(local_lib_location)\n",
    "    expert_lib: LocalExpertLibrary = LocalExpertLibrary.from_expert_library(\n",
    "        HFExpertLibrary(hf_repo_id), local_lib_location\n",
    "    )\n",
    "else:\n",
    "    expert_lib: LocalExpertLibrary = LocalExpertLibrary(local_lib_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, svd = create_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload embeddings\n",
    "remote_lib = HFExpertLibrary.from_expert_library(\n",
    "    expert_lib,\n",
    "    hf_repo_id,\n",
    "    force=True,\n",
    "    upload_aux_data=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# To save\n",
    "pickle.dump(svd, open(\"svd.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLayground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp_3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
