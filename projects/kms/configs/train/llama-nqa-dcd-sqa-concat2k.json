{
    "loss_function": "dcd",
    "dataset_type": "concat_km",
    "max_concat_tokens": 2048,
    "dataset": "sordonia/nqa_summaries_qa_llama-3.1-8B",
    "max_input_length": 4096,
    "eval_every_n_epoch": -1,
    "save_every": 50,
    "eval_every": 50,
    "total_steps": 1500,
    "num_train_epochs": -1,
    "use_only_type": "summary,qa",
    "split_train_dev_on": "output",
    "lora_rank": 16,
    "lora_alpha": 16,
    "lora_dropout": 0.05,
    "weight_decay": 0.0,
    "model_modifier": "lora",
    "modify_layers": "q_proj|k_proj|v_proj|o_proj|gate_proj|up_proj|down_proj",
    "trainable_param_names": ".*lora_[ab].*",
    "learning_rate": 1e-3,
    "micro_batch_size": 4,
    "train_batch_size": 8,
    "predict_batch_size": 8,
    "precision": "bf16",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "model_family": "gpt",
    "optimizer": "adamw",
    "warmup_proportion": 0.06,
    "max_output_length": 64,
    "padding_side": "right",
    "truncation_side": "left",
    "attn_implementation": "flash_attention_2",
    "evaluate_on": "nqa",
    "task_name_field": "document_id"
}