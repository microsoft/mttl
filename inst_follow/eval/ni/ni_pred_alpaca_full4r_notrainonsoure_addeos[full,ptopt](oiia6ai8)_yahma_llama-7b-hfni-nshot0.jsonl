{"id": "task393-a0728867ea124129a69ce05dc4b44a60", "prediction": " The physician misdiagnosed the patient, and the patient's condition worsened.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-634c747785084befad5b6f9edccf6c1d", "prediction": " The man signed the document, so he could now legally use the document.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-44494a3f28b943869e4b069773da19f5", "prediction": " The police officer dropped the gun, so he could use his hands to arrest the suspect.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-40569651e2da4669bc604e26f42dba81", "prediction": " I was confused by the professor's lecture, so I decided to ask for clarification.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-f5f9c616c2784a38862248fc1a9a33f0", "prediction": " The book was deemed inappropriate for children, so it was banned from libraries and bookstores.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-3157d5b5fc4f4247a7ff95ad824e854e", "prediction": " The parents recognized their daughter's creativity, so they decided to encourage her to pursue her passion for art.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-8956d8cfe3554bc7b7bebbd76f0c6c15", "prediction": " The boy opened the jar.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-fc0d664c02a84bd58dd572c2a437a815", "prediction": " The toddler became cranky, so the parents decided to take him to the park to burn off some energy.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-e8489d4ae7164447800000854ce3a703", "prediction": " The terrorist set off the bomb, so the entire city was destroyed.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-f9443e232f2d44de9fa0dee9ba0fdbeb", "prediction": " The authorities vowed to protect the identity of the crime victim, so they could continue to investigate the case without fear of compromising the victim's safety.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-25a14763cf7544ef921d31b8ed7acaf5", "prediction": " The suspect was convicted and sentenced to prison.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-8c61a72f2b384d16818c9bfe28e3d322", "prediction": " The product was heavily advertised, so it became a popular choice among consumers.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-758b4cad36ff4f8b9e25b5e783c5a542", "prediction": " I pushed the gas pedal, so I accelerated.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-bb701a8ccaec4b48ba2de6c0c9738e93", "prediction": " The motorcyclist twisted the throttle, so he accelerated.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-77c4677d03ed4a42891f75ba291096a2", "prediction": " The grape juice fermented, so it became wine.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-411c0f68cc874bc19c0bb03eccbfbd14", "prediction": " The man hated his new haircut, so he decided to shave it all off.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-d11795119a7c4d21b49131cba46220cf", "prediction": " The clock stopped ticking, so I don't know what time it is.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-775d3b4c22614fd7b6022125a71c77af", "prediction": " I rubbed the soap between my hands, so I felt clean.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-bd42f83eaf024498bde697726b4551ee", "prediction": " The teacher gave the student a warning.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-8968aa6364c64a54b5028b8697978a1f", "prediction": " The police aimed their weapons at the fugitive, so they could capture him.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-7ff41b34eec142a5951dd8453975634f", "prediction": " The man dropped food on the floor, so he had to clean it up.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-efca5ba474e447d3802989cdb634d274", "prediction": " The cowgirl threw her lasso towards the horse, so the horse was caught.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-d689b547d531479e9c376bbb494a54cb", "prediction": " The host served dinner to his guests, so they could enjoy the meal.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-b4f683272d934ca2acb47a14f5c9b2c0", "prediction": " The patient was dehydrated, so the doctor gave him some water to drink.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-13eb5d19874049a38534d3b327c38686", "prediction": " The girl was angry with her friend, so she decided to break up with her.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-c6691fd5ee014937832ea374289431d3", "prediction": " The group overlooked the woman's faux pas, so they decided to ignore it.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-f166e8b38409438182ecd14e286fd576", "prediction": " I apologized to the stranger for bumping into him.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-2023e0caaad7474c8f7e7bc920654d94", "prediction": " The girl was able to complete the puzzle.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-96ba7790ac2e409e9f1a19f079327616", "prediction": " The fugitive hid from the police, so he could escape and evade capture.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-735d6d6fcb3b497e9c71af9501398596", "prediction": " The girl looked it up in the dictionary to find its meaning.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-7557c231a65746e18e1bbc4b70b647c8", "prediction": " I watered the seeds in the soil, so they would grow.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-f9086ed7c3cc40969e11319b988976bd", "prediction": " The papers were disorganized, so I decided to organize them.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-8587d6be05b644b5bcb700c69132e609", "prediction": " I tipped the bottle, so the liquid inside spilled out.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-79822d8c71c346c8bcb63d2abe444b57", "prediction": " The man became disabled, so he could no longer work.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-f9cfd50d1ba24fd781ffdbc92844473a", "prediction": " The woman felt ashamed of scar on her face, so she decided to cover it up with makeup.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-e50a098c2c6c47468f138084b75ea423", "prediction": " She bought a new house with the money.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-234203ca94dc477b99c69e2306a7b5cc", "prediction": " I finished a page of the book, so I can now start reading the next page.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-a13b64d341124f4184a0ec0fce505d72", "prediction": " The flashlight was dead, so I went to the store to buy a new one.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-07801d24016a4597947281a02e08c29d", "prediction": " The doctor x-rayed the patient's arm, so the patient's arm was found to be broken.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-f8cca69d247a4f0c9a34749de8145d11", "prediction": " The seamstress pushed the threaded needle into the fabric, and the fabric was now stitched together.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-3d6d8f182a7343648c8b87676d4c74b7", "prediction": " I called the police to report the incident.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-b3ba8110756b437ca2c308f94c393ecb", "prediction": " The dog ran away scared.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-230a1d81aa1c459c84f22fb648debe82", "prediction": " The woman upgraded her computer hardware, so she could run more demanding applications and games.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-e686a173451d47ddaa56fc0797bfd05b", "prediction": " The tenant went to the landlord's office to get a spare key.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-91c98e0ad8114149b2dcd4d08c751c35", "prediction": " I went to the doctor to get it checked.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-49edfac08b024a40969bd7984c75efc4", "prediction": " I scratched my skin, so I got an itchy rash.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-0d2ad12c957a486b8d705336ad91fe09", "prediction": " The girl performed in a dance recital, so she received a standing ovation from the audience.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-2214e3dcbf17432b85abb6ceefaa8aea", "prediction": " I turned up the volume and sang along.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-f583a4399e0a411b99201305841768f1", "prediction": " The photographer got the child to smile, so he could capture the perfect shot.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-1a0b7e2fe7e14e1fa370ea61c3358e81", "prediction": " The police were called to investigate the burglary.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-4759913505b2406e84bc998d70099787", "prediction": " The baby was wailing in his crib, so his parents rushed to his room to comfort him.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-0b92885bbd5b40e0bf4a0e8f3aef7caa", "prediction": " The elderly woman suffered a stroke, so she was rushed to the hospital for immediate medical attention.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-461162a52d7b44ffa95c94a16541ca03", "prediction": " The woman felt compelled to help someone in need, so she went to the rescue of the person in distress.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-d4c22d4e1c074f819320e98e6de74d4d", "prediction": " The man begged for forgiveness, so he could continue his life with a clean conscience.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-f3d62705cf4143239c4bbc8ac974f937", "prediction": " I decided to walk around it.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-49ac549280724f529ddd804c5ee561b1", "prediction": " The man's eye became infected, so he went to the doctor to get it treated.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-5af7231185834e79a302bcf4e7940d58", "prediction": " The man slid the razor across his chin, so he could shave his face.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-35c9920818f84aa4b4ab41aa04777805", "prediction": " I held the door for the woman entering behind me, so she could enter the building.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-197e5d3a28e943ac8ca857d477695187", "prediction": " The woman shivered as she got out the pool, so she wrapped herself in a towel and went inside to warm up.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-b9bccf26221449a0b07ec56e55fc97d1", "prediction": " The woman smiled and walked towards the source of the pleasant smell.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-9b36ce67e210468fb558f4775227c877", "prediction": " I wiped the mirror clean with a towel.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-35cf43ed2b834d9391133080c206b186", "prediction": " The bureau drawers were cluttered, so I decided to organize them.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-206c6e9409fb4e8b9a4ac946e9d5a2fa", "prediction": " The nurse prepared the needle for the patient's injection, so the patient would be able to receive the injection.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-4c3ede64c3b147ac912c7cfe6597aed0", "prediction": " The teacher took roll, so the students were able to see their names on the attendance sheet.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-f07189c65da84a24b06dcd03bbc5890f", "prediction": " The driver rotated the steering wheel, so the car turned left.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-b842abd533aa422ca78e2486d3abb666", "prediction": " The boy murmured his reply to his mother, so he would not be heard by his father.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-518d00142f8340bf8f0d74397c193ee7", "prediction": " The homeowners decided to move to a more secluded neighborhood.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-14c7e906fd80421f834782220c3ad308", "prediction": " The pendulum swung back and forth.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-84cd874f228f49b2bacc58b4f77662f1", "prediction": " The navy bombed the ship, so the ship sank.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-fe7ed4c42e624d21854bbe11af84b6fc", "prediction": " The bodybuilder lifted weights, so he became stronger.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-34ec2e2fc7cb4df584f41eac58940876", "prediction": " The child kicked the stack of blocks, so the blocks fell down.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-0522cd8a047b45ee9fbc6ea7c1b534ea", "prediction": " The girl felt dizzy, so she sat down to rest.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-a2dbfffec118447fb200ac15741a8516", "prediction": " The cook stirred the ingredients in the bowl, so the mixture was ready to be served.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-ecad2bd1487d4b16a238d2a826bacfd3", "prediction": " The man looked at his watch, so he knew it was time to leave.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-3bddef1c1f2c46dd9112a166e3c14c46", "prediction": " We decided to go to the movie theatre instead.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-24278deac4cb4ce3879fdb37cad7326e", "prediction": " The pond froze over for the winter, so the animals could not drink from it.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-fb8925dbc4264ae89067b3c680c2a40f", "prediction": " The woman felt lonely, so she decided to go out and meet some new people.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-b4d1b85d9a5d4431a05a81086377059b", "prediction": " I opened the package and found a gift inside.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-319e42b1a39f4a05807b3123f2809bb7", "prediction": " The archeologist dug up the site, so they discovered ancient artifacts.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-47264cf28ddb4f9899de5aaa2a2a7fdc", "prediction": " The offender was sent back to prison.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-f20bb7952d3447a39d6673df89f9a8d9", "prediction": " The surfer rode the wave to shore.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-8fc80d593c8d43a1b551dc38e01f030d", "prediction": " The ball landed on the ground.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-ff3f98042c5d4a638aa13fee0c64b451", "prediction": " I poured water on my sleeping friend, so he woke up.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-fff520b48721447abec197c26b0051dc", "prediction": " The crowd gave the band a standing ovation, so they decided to play an encore.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-29d7b574d6f44e3fa0702e7425151c38", "prediction": " The speaker disarmed her debate opponent, so she won the debate.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-80a1870384794d489c4f96530a34ba29", "prediction": " I decided to eat something to cover up the smell.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-2d6db97ece3b40ea8e97688c62a4853c", "prediction": " I turned on the lights.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-281a491477304a2d931d38e2ad6fd110", "prediction": " The man threw his empty can onto the street, so he could help the environment.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-5334fd563b3d4717962026324f579854", "prediction": " The girl went down the hill on her bike, so she reached the bottom of the hill.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-4bb7fe86a8b8476391b717cd586609b1", "prediction": " The man was lost, so he asked for directions.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-0409b20a11644d8daeb99229130feead", "prediction": " The woman's ring slipped off in the shower, so she had to go back to the store to buy a new one.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-41a0d0f5c11a44a3865c05e8d283da41", "prediction": " The boaters set off a flare, so they could be seen from a distance.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-d52ae7c697ec4a0db2709ee60ab5d7ca", "prediction": " I got angry and yelled at him for being rude.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-6de8f7587303481aa9b98f89a3d1249d", "prediction": " The baby pulled the mother's hair, so she screamed in pain.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-91fa6cb8d3c44560b8bfa5e5a46844d1", "prediction": " The woman's date wanted to look like a gentleman, so he wore a suit and tie.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-c140b8da6bc9441ab8cba6cbe0b8afda", "prediction": " The woman dangled the biscuit above the dog, so the dog licked it up.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-ceab7ee80ec444b0b7ae7bb732bac827", "prediction": " The man contemplated the painting, so he decided to buy it.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-2f912ae3b2be493081f5a6268ed8e0bc", "prediction": " I stood on one foot, so I could balance myself.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-fd603900c6ee4412b643d1ce6fe48b55", "prediction": " The woman became a devout follower of her religion.", "task_name": "task393_plausible_result_generation"}
{"id": "task393-dddcb1afe9b2416c9fc21b59f2b5f015", "prediction": " The wood became smooth.", "task_name": "task393_plausible_result_generation"}
{"id": "task1640-fc93ada7658542b9bcb89b3b7e9f7a43", "prediction": " True. The paragraph discusses the use of storyboards and wordless novels in filmmaking.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-d2c6cefd033e4b009f7ee61fb4ae0c64", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-6c2cda9a892c4afdabec31a6ba4e0b3d", "prediction": " True. The smallest percentage mentioned in the text is 5%.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-551e2e0fcc494d389bb137f250c405ad", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-a046402f9b8f4394b38c10a9705d3ff6", "prediction": " True.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-e00c54af593742daa6836981893d6d9c", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-34e112a59f704e5ab2d2d5f2d7219e8c", "prediction": " False. The question is not answerable from the given paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-db83ea51bbdd4e60acd78852d5a6330d", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-8e900a47d80c43cd8f463727e0a85aff", "prediction": " False. The question is not answerable from the given paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-76e8e2751a5d429c831448d57bdb0085", "prediction": " True. The question is answerable from the paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-f894fa537d8b4b1294ded66c84177e19", "prediction": " True. The question refers to the same work, Geschichte des Hellenismus.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-99e429be23374ff1b01cff1a0bbdab51", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-9c4e05c6ffae4b84bfd7a874b5db1160", "prediction": " False. The question is not answerable from the given paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-81a7d59735464fffa8cf9437bc12975c", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-36fd6f0facef4de8bf64a2d6f1d0f191", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-0865cdfe60a9471ca6be7a0138d3d92c", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-f467fa24eeb744f2b3622ab88185329f", "prediction": " True. The catalyst for exposure was Arena Bowl XII being televised nationally as part of ABC's old Wide World of Sports.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-5e070a2fd2d44b0ea820547a8c460d04", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-5a747ef62f6b4645913114c7d40166dd", "prediction": " True. The question is answerable from the paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-7ba4d83a9eff474e97efb2391aa265c2", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-2791e0329f3348efbd51103777aa2fa3", "prediction": " True. The paragraph provides information about the federal constitutional initiative, which allows citizens to put a constitutional amendment to a national vote.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-cc53fe524c9145de9d090cf092dc238c", "prediction": " False. The question is not answerable from the given paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-fc5ee3414fd648c3a3e6838d592ef536", "prediction": " True. The question is answerable from the paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-0976f623f79d4b31945c800e7b72068b", "prediction": " True.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-273e2df2653d4e76a3392ae956fc1d76", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-b4a41eb017a647eb9a8a4c1a3b035b61", "prediction": " True.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-200acfdb843c4e9982cb1d34c11970e2", "prediction": " False.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-094905e93b584a9782cba4f293ca3715", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-504c0715a0de4046bfcdba879e9a7644", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-2cb0a21313f24d5d8ee61c1b9531cf29", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-4df07972f8994210b8fc41947b50cf1b", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-879b0a4ec80f474e84d2bf0ba2d6dba4", "prediction": " True.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-dbf653159ecc4b03be7c5e14309d8b6e", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-bef34c3606bc48efaff53231a31dce82", "prediction": " True. The number of non-Swiss women who finished tertiary schooling is greater than 50,418.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-36020e1409994a0fb6eab9e91cc237f1", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-2ed21fc8f65348baae0cbf4e1b1395eb", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-16d77bb7b4004cd89db1ffc2dbbc0a4d", "prediction": " True. The Russo-Japanese War ended in 1905, and the Anglo-Russian Entente was signed in 1907.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-361a63d58158465cbc19ef64e9e76d8e", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-0e96da32021f4827ab5b5706601a71f6", "prediction": " False. The question is not answerable from the paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-1caee41acb0a449f89c292f199f71b61", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-fb77f1a0d1f24034acfd3e799fea99f6", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-7414e7cd57bf41358b0287b4e82ffa64", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-ef7b86898adc4dd49ed7bb7713a5111c", "prediction": " True. The question is answerable from the paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-8aa09a66f27c4664831ee9cd8ee9d4d6", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-f999cae087bb4114b47dadb86611ab0c", "prediction": " False. The group mentioned first is \"clerics\".", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-912e81bc4bf7436c8f6bd079d7829f96", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-35aac7bd4bc54aea8b1c4cf11e29fc82", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-284523193b544a7e9c0e895047f2eca4", "prediction": " True. The question is answerable from the paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-f88e68f90c87488e8a02fb08eb9cdebc", "prediction": " True.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-dd55b6cd86d245cb982c2386cbfa213c", "prediction": " True. The question is answerable from the paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-cc809851d47e4e5593a05ae2ab10e38b", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-42bb8cb8040740d387419135e2f28070", "prediction": " True. The question is answerable from the paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-ce62fd6176bb40f387aea4fb81869c87", "prediction": " True. The Estonian War of Independence was a war fought between the Baltic German nobility and the Bolsheviks in 1918-1920.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-feb3ee41a78d420794cdb599e780989d", "prediction": " True. The question is answerable from the paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-cc264fad9de543ff8854ad48e37943cf", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-ddf64da0dd144776bfded93f080251e0", "prediction": " True. The correct answer is \"payload-weight\".", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-e52857c0531f47b0a5db329c2ddbe8bf", "prediction": " True.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-9bba753c3511443eba14b7fc4cba6f39", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-2077a7b83bfb4bc5be8b63dcce05b622", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-3da1a617726c405a96e38df3ee579da2", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-febcff48253541648fa7eae8c4dbd931", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-5d2f1152ef804e27a00a24b2f0801298", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-fc98e8ef791649e8b094f07829e04e0c", "prediction": " True.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-10f44fcafebb49e198f9f71222e05a1e", "prediction": " False. The question is not answerable from the paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-4ed3fe9ff1744d9b8345f21d30ace1c0", "prediction": " True. The question is answerable from the paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-8327727821764def90cfe9bcb1728401", "prediction": " True. The question is answerable from the paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-9a18af4b35d446689c4ac21431c9d8df", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-5fa1129965f146648b22f62f7da87de2", "prediction": " True. The question is answerable from the paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-84a708094a424897b80998e44495e6d5", "prediction": " False. The question is not answerable from the paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-79e56793c0264aeaa54f9f21a4ed1432", "prediction": " True. The question is answerable from the paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-e87fa17d8ecd46dc9663c2c709a2f39f", "prediction": " True. The third mentioned location is a home.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-c3fae45784864335aab93465d65b6bc2", "prediction": " True.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-ff0fa86c972e4fb8b51d95cc5d841af1", "prediction": " True. The question is answerable from the paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-b56823d1f0ee4b36a418967cae886aa7", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-a0ba3009d27242d286b6a88d9d0abcb4", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-057964d44b5d4b32b00dce4e907076e8", "prediction": " False. The question is not answerable from the given paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-118c756f3f3041d0af31b6eedee537d0", "prediction": " True. The question is answerable from the paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-adade7779d144a308f161cc76c5d164a", "prediction": " True.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-988ddfc55a2d4c50988cbfb4c3955531", "prediction": " True.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-7306849123a94442aa93e4c96c0dc692", "prediction": " False. The question is not answerable from the given paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-f9614395a4a647e284bede5c118e5d5f", "prediction": " True. The question is answerable from the paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-76976b598f134f5d9cf8b29ddaacc36a", "prediction": " True.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-74283eab826245a69e95a40c50510903", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-8c13c735445e43c2be990d39dd96f2ea", "prediction": " False. The paragraph does not provide information about the amount of rainfall in the city in the winter.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-622b1b2069674aff8c13d8bb2ec8b1dd", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-ad75fd8e664e4bf991a78f424c27ee07", "prediction": " True.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-c52ca5d97617480b9493a669e9cb7483", "prediction": " True. The paragraph states that 34.1% of all households were made up of individuals.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-b4cd7f2403af4783a7c978b60a01b7e1", "prediction": " True. The paragraph mentions that the city acquired strategic importance after the Byzantine invasion of the southwestern Iberian peninsula in 554. This suggests that the El Templo was in existence at that time.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-485ffdb8c28f4e6d96699bead92cc975", "prediction": " True.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-3cf609567f3d451bacfccfecc79f9b9d", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-77d5dcd57a6048d1b301cd1b8ecdda89", "prediction": " False. The Alfred P. Murrah Federal Building was destroyed by a fertilizer bomb manufactured and detonated by Tim", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-df95549c9b9840349aa34297b89b6dfb", "prediction": " True. The paragraph mentions that investment banks and hedge funds used financial innovation to enable large wagers to be made, far beyond the actual value of the underlying mortgage loans, using derivatives called credit default swaps, collateralized debt obligations and synthetic CDOs.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-b4432b44d9004e55913d135600ca9c6f", "prediction": " False. The question is not answerable from the given paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-78ab194358564be8a9c14866f8400465", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-6c63590105cd45ef9a78124bda85bcd1", "prediction": " True. The Migration Act allows time spent on Norfolk Island to count towards becoming an Australian citizen.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-9f12cbaf3054408eb82e1ed2e6b0442e", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-3d6d7aff944549a4b2dcf5499b940173", "prediction": " True", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-3dfb31c8bb5649878bf27da9c46f3229", "prediction": " True.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-ded3db5c757b471cb0478557de777608", "prediction": " True. The question is answerable from the paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task1640-9b9e5b00b0ca4e8fbe26eda5966c6951", "prediction": " True. The question is answerable from the paragraph.", "task_name": "task1640_aqa1.0_answerable_unanswerable_question_classification"}
{"id": "task102-02e80f9106a74cd69188da363de8a491", "prediction": " \"A dog wags its tail.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-7a8ca68b33e645159843f6a338d8eef6", "prediction": " \"I went canoeing on the lake with my friends, paddling our way through the water.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-5da80f4ec3564b0a80091855285685ab", "prediction": " \"I went to the train station to catch the train.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-86e007b9aed3447b8820779fd701043b", "prediction": " \"Today, I ate hay and fed the horse.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-38ffba293cbc426b8657ce08668f1ce8", "prediction": " \"Today, I watched a match on my fan.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-3d024ebf2c16494faf3141f3c4658258", "prediction": " \"Today, I went to the lake and saw the beautiful mountain range surrounding it.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-b8a4c19331e04cc7b6f17bea12bf5be9", "prediction": " \"A dog lies on a rug.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-f60f207b24924bc0a1b7a476a0415f76", "prediction": " \"Today, I hung a painting on the wall.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-9bf9dd837015411693f426693e74160c", "prediction": " \"I am carrying a tray of food to the table.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-fbe0e30d04d946669798edbc85cf8939", "prediction": " \"I went to the stadium to watch a game.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-ddd079b7cf0642a3a6bfd5899971ea6c", "prediction": " \"A cat licks its paw while sitting on the couch.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-017d31a9e5ae4864b0a921625cc7805a", "prediction": " \"In the room, there is a tile on the wall.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-e6dd4f27b1b44aee9690f47d5a404076", "prediction": " \"I went canoeing on the lake and then walked along the shore.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-e2c488223f564cc398e40f30e2f604e9", "prediction": " \"Today, I went skiing on the mountain.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-5f68da8e4adb400c8ba3a52aec96330b", "prediction": " \"Today, I went for a drive on the lake in my boat.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-bb7b7cb1fb1f4cb1b545d738677e2aac", "prediction": " \"Today, I ate grass while my horse grazed.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-793cd08ac45043748060b051f3da2abe", "prediction": " \"Come to the track to train.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-b7b2ddc433ab48beae768336432603a9", "prediction": " \"Today, I am going to the track to train.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-0a990fdf92624b37aa50a5f5626ff27f", "prediction": " \"I left the station and took the train to work.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-2dce14d05d154810b3098eabf81fc32c", "prediction": " \"A passenger is waiting at the train station for the next train to arrive.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-c3a27c76dae945fe8e2c48f2fde3d0fd", "prediction": " \"I arrived at the train station.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-1b5f269cf9a94e62986c9dc8bd367bbc", "prediction": " \"I sat on the train station today.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-ed5d123f6d7b4a06af1eb6b97d1bb9b1", "prediction": " \"A man is pulling a wagon with a horse.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-d7a506121f974ff59d4ca10b3a2b7fe8", "prediction": " \"At the train station, people are waiting for the train to arrive.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-55b10d10645c43369978cbb59f3417ee", "prediction": " \"On a typical day, people run on the runway while sitting on a plane.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-78025e355f7841e4b7e2d194ca9572e9", "prediction": " \"It was a beautiful day, and the clouds were flying high in the sky. A plane was taking off, leaving a trail of smoke behind it.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-b924530cc9e247db9c1295d40dba3e1f", "prediction": " \"A dog herds a flock of sheep.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-27726e59c754474682799147b702dfc4", "prediction": " \"On a sunny day at the beach, a group of friends are sitting on a boat, enjoying the view and the warm weather.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-71e469e1ac364d4fb6e4085243b939ff", "prediction": " \"Come to the train station to catch the train.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-0c6408d686cc40c682c5c9c6f7fbdc9f", "prediction": " \"It was a beautiful day, with a cloudy sky.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-fef6672144554151826a562092f90a1c", "prediction": " \"Today, I ate an elephant made of grass.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-b655198837874466be67f49cd0af38e5", "prediction": " \"On a typical day, a family spends time together.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-71059e10a5914ab6a36b00b3a3df1e19", "prediction": " \"I washed my hands in the bathroom, and noticed the tile on the wall.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-7e31eed95f3744a6be135a25b407aac4", "prediction": " \"It was a sunny day and the sun was shining through the window.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-d24514b2e10b4378b2694550e444ea89", "prediction": " \"Come to the window to see the light streaming in.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-447fc98730324c1299f063769fe4b3f0", "prediction": " \"I went to the train station to catch the train.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-fb495ef61c7f4eb7a613e6e1878d7f6e", "prediction": " \"I went to the building and saw the beautiful cloud in the sky.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-c18a25bd13f046e88b6ed82846c1c67c", "prediction": " \"Today, I went to the zoo and saw a giraffe eating grass.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-86c85eff204448b79813a343a120531d", "prediction": " \"A cat lies on the grass.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-9c8552f1ca3443eb8edf55d2401b6d47", "prediction": " \"I spent the day at the beach, relaxing and enjoying the sunshine.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-87fdccda3ce94464ba5b247a32fb6d74", "prediction": " \"Today, I am eating corn on the cob for lunch.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-48bbe2f039ef4fc6b135e03bbad36888", "prediction": " \"Today, I ate hay while watching sheep graze in the field.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-bee286ae953d40f88fba90b8ca894049", "prediction": " \"In the morning, a student sits at a computer and works on their assignments.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-ea0ac1126ac448aca10e4ea421791b98", "prediction": " \"At the airport, people wait for their luggage to arrive.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-adb1087d95b04bf59d4cec95e83d56a8", "prediction": " \"On a sunny day, people are walking across the bridge to catch a ferry to the island.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-a26bbc3780c947438b416b2f4c374f91", "prediction": " \"Today, I went to the flower shop to buy a bouquet for my bride.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-11f5f9292df94cffabe8c0d1450061a5", "prediction": " \"A couple is standing in front of a cake, talking about their plans for the day.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-6aafe6ad2ae04b92b38d1624aef4ea5d", "prediction": " \"A cat stares at the window.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-41bfa0cf63ba4958a2e8619ae39862fe", "prediction": " \"A nurse is taking care of a patient in a hospital.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-40d402e314af41099679b8a9e98cf32d", "prediction": " \"On a cold winter day, I wrapped myself in a blanket and cuddled with my dog while lying on the couch.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-f9b3232975f9477aa443c8270e8bad45", "prediction": " \"A farmer is plowing his field.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-6e75a31cb48a4483bcf7f5479d93fdb9", "prediction": " \"On a cold winter day, I wrapped myself in a blanket and cuddled with my cat while lying on the couch.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-b1f16d21e65e4a3099fe5ae55a614be3", "prediction": " \"In the room, I painted the wall with a beautiful painting.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-96273466e8d34ef897e848f9d2bd2c71", "prediction": " \"Today, I went skiing on the slope.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-a27754ef59c84dd5a2301257167524e2", "prediction": " \"Today, my daughter smiled at her mother.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-bb3822c7a929497a9fa92cd52ed47edb", "prediction": " \"On a typical day, a family gathers with their friends.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-c1a2dfe33dcb4a40abc29f4beaf34ad1", "prediction": " \"A farmer is using a horse to pull a plow in the field.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-2901be24d22347499248435d4112b2f9", "prediction": " \"I had a nice meal with my friends at the table.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-890fe5887faa4b488fc963223ee328b0", "prediction": " \"Today, a construction worker is using a crane to build a new building.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-324b3b15b5a94931b2e8d71da8f355f4", "prediction": " \"I tied a bow on my tie before heading out to the party.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-36b4e3a3ae66461fae8e2231a5304a37", "prediction": " \"On a sunny day, a family went to the park for a picnic.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-621f225eb0374534ace3f52bb7683229", "prediction": " \"Today, I reflected on the sunlight and enjoyed the water.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-e75beadf738b45bba9aadebee945c590", "prediction": " \"A passenger sits on a train.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-368d6e3e98434b08ac7f0175b4c48669", "prediction": " \"In the room, there is a clock on the wall.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-a6addf14b78c464db25b4c3fb5002e47", "prediction": " \"A dog is chewing on a bone.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-5725d6ea62dc44629aa966225a578bfd", "prediction": " \"At the airport, people are waiting for their plane to take off.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-3cc15ce0009d46569c7dd41465a96356", "prediction": " \"On a sunny day, a skier is skiing down a slope.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-e5372913e9cd4fcd85f9a7cee73a30ab", "prediction": " \"Today, I went skiing on the slope.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-1a1643dca2ca47e0af8593e043b07178", "prediction": " \"I had a salad for lunch today, made with fresh vegetables from the bowl.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-7d083525a84b47069d634ff98be0a2ca", "prediction": " \"In the morning, I woke up and had a nice sleep on my comfortable pillow.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-dd9b4a64ad18477fb782ff60ee6f87a8", "prediction": " \"In the pasture, a herd of sheep grazes peacefully.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-6f5c4dd65bea44a4865de1e3e014c0da", "prediction": " \"On the beach, I collected shells.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-5f865189daf248a48c7fb5fb17caf14f", "prediction": " \"At the airport, I boarded the plane.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-0a6fcb3fbd5643daa50de45c24b66ec6", "prediction": " \"A passenger waits for their flight.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-6383cc759e464aa3bda19f3e6aa1bbb6", "prediction": " \"I went to the beach and played in the water with my friends.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-308d70219ee14fc9b42e29f977a81ab2", "prediction": " \"On a sunny day, you can sit on a bench under a shady tree.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-8b56dceac27c43b0b99df2b1b21cfad9", "prediction": " \"I went to the post office to buy a new horse tie.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-c98d9542ab1f41058d9d4b8b83d848a7", "prediction": " \"I went to the park and saw a pigeon feeding on the grass.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-4a6310a2eb9c499a9c51cebc18ecb89e", "prediction": " \"I went to the track today and drove my car to get there.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-1fd2bf8ccabe41cfb780989b78406b60", "prediction": " \"I went to the park to skate with my skateboarder friend.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-79f6dbb71ec14ea6854b00888df5d493", "prediction": " \"I was lying on the carpet with my dog.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-bcd871a2ee094f88913e8aeff67a817f", "prediction": " \"I washed the carpet in the living room today.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-a33181f5b531486585f1d8dc3f98b153", "prediction": " \"Today, I ate a carrot and saw a rabbit.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-6a49572ca0b94803b4e4f78c9e01ea65", "prediction": " \"In the boxing ring, a boxer is preparing for his fight.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-fbce212c0928485c9391f22f64724e36", "prediction": " \"Today, I went skateboarding on the street.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-8668e01a369948e781c725229fdff77b", "prediction": " \"On a sunny day, a boat was sailing on the sea, and a seagull was flying above it.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-26484f2859b04a01b377c2a162bb6ecd", "prediction": " \"It was raining outside, so I stopped to wait for it to stop.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-d8e4e10d102a448cb593b8aa42a90905", "prediction": " \"Today, I baked bread in the oven.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-e46d3322989e45e595738a0fd9c70ae5", "prediction": " \"Today, I made a delicious pizza with cheese and put it in the oven to bake.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-7e4cce34685245c39a9c12174823400d", "prediction": " \"In the room, there is a carpet on the wall.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-2202bbe9f96d4fbab8ef982ad1ce0ca2", "prediction": " \"On a sunny day, a skier is skiing down a snowy slope.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-97559f2d070b41dc8d3f40d9a6b18c5b", "prediction": " \"It was a beautiful day outside, the sun was shining brightly in the sky.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-3cd7a86fa6864818ae89773d362024c0", "prediction": " \"On a sunny day, a group of people are walking up a hill, passing by a flock of sheep grazing on the grass.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-d9c884c30505436ba3ec405f08adde1d", "prediction": " \"Today, I went on a boat ride, then I flew to a different city on a plane.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-5573e442a68c46e889cdd826a62afa3a", "prediction": " \"On a sunny day at the beach, you can sit on a bench and enjoy the view from the overlook.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-02d33f6c3fcb4c198b13a586aceae43c", "prediction": " \"I placed my book on the shelf and stood it up on the stand.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-753129bf99024b6489c7dde2a3504a9d", "prediction": " \"Today, I went diving with my friend at the beach. We dove into the water and explored the underwater world.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-58fc32f715a1489a8cb6020fd6d33370", "prediction": " \"I had a cheese pizza for lunch today and put it in the fridge.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-07ff7def65a146c69d3459eb98fbb2c6", "prediction": " \"I grabbed my hand and reached for the door handle.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task102-1c1168f3344c45c18a178f92327f6ecd", "prediction": " \"I went canoeing on the lake and sat down to rest.\"", "task_name": "task102_commongen_sentence_generation"}
{"id": "task1598-22b411d5e58a434fa168d5074b3cbfa1", "prediction": " The Upper East Side restaurant xname is known for its authentic Italian cuisine and its unappealing decor. The price is on the expensive side, but the service is acceptable.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-a2e371f9df364bd9bab8fbff04b60564", "prediction": " The [xname] restaurant is a pub serving English cuisine in a family-friendly atmosphere. It is located near [xnear].", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-2cfcdb6ddac54914b530fe698973a124", "prediction": " The [xname] restaurant is a fast food joint that serves delicious and affordable meals.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-9e7f3e5c556044c8982e09e44ba6aabc", "prediction": " The [xname] coffee shop is a popular choice for those looking for a quick bite to eat in the city centre. The Chinese cuisine is delicious and the low rating is a testament to the quality of the food. It is located near [xnear] making it easily accessible for those looking for a quick bite to eat.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-9986e41050ae464b9ec14087681831f2", "prediction": " The [xname] restaurant is a highly rated Italian restaurant located in the city centre. It is near [xnear].", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-e35918dfd14d4a38b6d109f2f3c2bf0e", "prediction": " The [xname] restaurant is a popular choice for those looking for authentic Chinese cuisine in the city centre. With an average rating of [rating], it is a great place to try some delicious dishes.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-4ce95e283205482c8c98130b1898fbca", "prediction": " The [xname] coffee shop is a popular choice for those looking for a high-priced, family-friendly coffee shop in the city centre.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-9a8f1bebce554befa748166256830237", "prediction": " The [xname] restaurant is a great place to eat in Midtown East. It has a good reputation for serving delicious food and the staff is friendly and attentive.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-c69b5c6973d94ad5bf8edc317ddfe983", "prediction": " This cozy coffee shop serves delicious Italian cuisine at a reasonable price. It is a great place for a family outing, as it is family-friendly.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-3bbc1188eaad4914be49ba9d7a1b548d", "prediction": " The [xname] restaurant is a great place to try New American cuisine in Manhattan. The decor is acceptable, and the price is very expensive, but the service is acceptable.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-cc945bb9be3f475f90c62f81faf4b186", "prediction": " The [xname] restaurant is a moderately priced Italian restaurant that is family-friendly.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-e63e8ef64e0745b5a257d36cfcdcdaf3", "prediction": " The [xname] restaurant is located in the city centre and offers a wide range of dishes at affordable prices. It is not family-friendly.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-5d7c7d5a383d427bac8a662f016966b8", "prediction": " The [xname] coffee shop is a popular riverside destination for Indian cuisine. It is known for its high prices and is located near [xnear].", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-228d8a1b9e714efaab0cdb1296dc3459", "prediction": " The [xname] coffee shop is a high-rated English cuisine restaurant located in the city centre. It is not family-friendly.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-28bb7a86b3484622bd9dd2e3da723a64", "prediction": " The [xname] coffee shop is a fast food restaurant located in the city centre. It is not family-friendly and is not near [xnear].", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-6c8c8af6b8fa434c8791ac1f956176a2", "prediction": " The [xname] coffee shop is a family-friendly Italian restaurant located in the city centre.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-5996ee193ab749d5b4b1a738c0f36889", "prediction": " The [xname] restaurant is located in the city centre and is known for its delicious Indian cuisine. It is a family-friendly restaurant that offers a warm and welcoming atmosphere.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-a01f5685157941de97d6ffe4268d9e4d", "prediction": " The Chelsea restaurant is a great place to try Southern cuisine. The decor is a bit lacking, but the food is delicious and the prices are affordable.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-0dea85cc10ce485a916886c58841dda5", "prediction": " The Upper East Side's [xname] restaurant is a great place to try authentic Chinese cuisine. It's recommended by many and is considered acceptable by most.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-e6aa2b54f4cd4888892ee0e87158e5e2", "prediction": " The [xname] restaurant is a moderately priced Japanese restaurant located in the city centre. It is a family-friendly restaurant that offers a wide variety of delicious dishes.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-7415d69a143e4852836f079486bddbed", "prediction": " The [xname] restaurant is a moderately priced English cuisine restaurant that is family-friendly and located near [xnear].", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-206fec3617fa4e7a93655b6fc2e5ba5f", "prediction": " The [xname] restaurant is a great place to eat, with a friendly atmosphere and delicious food. It is located in the Murray Hill neighborhood, making it easily accessible for those living or working nearby. The staff is friendly and attentive, and the food is of a high quality. Overall, the restaurant is highly recommended.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-2d164c0dfa43496cb43db58723b6e401", "prediction": " The [xname] restaurant is a great place to try New American cuisine in Manhattan. The decor is a bit lackluster, but the prices are very affordable and the service is poor.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-b155fd524a464146ab418bfa15da8e8a", "prediction": " The [xname] restaurant is a great place to try Indian cuisine in Manhattan. The decor is not the most visually appealing, but the food is affordable and the service is acceptable.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-cf6ddd698df24cf4978ec0f6652177f4", "prediction": " The [xname] restaurant is located in the city centre and is not family-friendly.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-5dc0effe9ba44faba09f806820d8a23e", "prediction": " The Midtown restaurant xname is a Japanese restaurant with acceptable decor and a very expensive price. The service is acceptable.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-f7461c7e1bca4dbb9ead05841e5a75c2", "prediction": " The [xname] restaurant is a great place to try Indian cuisine at a reasonable price. It is family-friendly and offers a welcoming atmosphere for all.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-ffc7e5e53b1d434bb9c710a1c90bbcfa", "prediction": " The [xname] restaurant is a [eattype] pub that offers a variety of [price] meals at [rating] out of 5. It is located near [xnear].", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-6e535c6a3c454db480b1c976a3ca700c", "prediction": " The [xname] restaurant is a popular Chinese restaurant located in Chinatown. It is known for its delicious and authentic cuisine, and is often recommended by locals. However, the price is on the expensive side.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-36ccb10cb96e4fca9e02585333e24474", "prediction": " The riverside restaurant offers a wide variety of Chinese cuisine at a high price.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-359a014598f941c9985a5bbddb36bd35", "prediction": " The [xname] restaurant is a great place to go for Italian cuisine in Midtown West. The decor is not the most impressive, but the food is delicious and the service is acceptable. The price is very expensive, but the quality of the food makes it worth it.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-18e4367b12ec406ea5ceb394feee94d9", "prediction": " The [xname] restaurant is a great place to try authentic Italian cuisine. The service is excellent, and the staff is friendly and attentive.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-df328fd1a8bb4c33a06d29341fa3accb", "prediction": " The [xname] restaurant is a family-friendly restaurant located near [xnear].", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-487a5c97f55c43dabd2132b58da255d7", "prediction": " The [xname] restaurant is a great place to eat in Manhattan. It has affordable prices and excellent service.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-f2f1b89854b548ff989e6dbb4cdc4ecc", "prediction": " The [xname] coffee shop is a popular place for people who enjoy English cuisine. It has a high price tag, but the food is delicious and the atmosphere is cozy. It is located near [xnear].", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-294db19b358940fab6cca76bf219daeb", "prediction": " The [xname] restaurant is a family-friendly restaurant that serves a variety of delicious dishes.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-069429a1fa1d426890cd6c9e88ef3779", "prediction": " The [xname] restaurant is a fast food joint that serves affordable meals. However, the quality of the food is not very good.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-3dca643fbcb04988b1d88385c5d5cc60", "prediction": " The [xname] coffee shop is a great place to enjoy a delicious Indian meal. It is family-friendly and located near [xnear].", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-bc66f8ba27b64ef5856bcd9759e29574", "prediction": " The [xname] coffee shop is a great place to grab a quick bite to eat. The Chinese cuisine is delicious and reasonably priced. The location in the city centre makes it easy to find and the family-friendly atmosphere makes it a great place to bring the kids.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-cdb1ea2a65b4484ca43181c87af1a090", "prediction": " The Chelsea location of xname is a Mexican restaurant that is recommended.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-3c993a1dbfea472aa622d537a74accdd", "prediction": " The [xname] restaurant is a fast food joint located in the city centre. It offers delicious meals at affordable prices, making it a great choice for families on a budget.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-5402a490c88f4d6e85d179cdf088e2c2", "prediction": " The [xname] restaurant is a good choice for a night out in Midtown. The decor is tasteful and the food is delicious, making it a great place to enjoy a meal. The price is a bit expensive, but the quality of the food and service make it worth it.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-4a7bf9e2cda24cbab1dd18c6eda22e32", "prediction": " The [xname] restaurant is a good choice for a casual meal in Midtown. The decor is acceptable, and the food is of good quality. The prices are affordable.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-c2b840950256402d9f8955c009fa1ba4", "prediction": " The [xname] restaurant is a family-friendly Chinese restaurant located near [xnear] that offers a wide range of dishes at a price of more than \u00a330.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-adb1cab76259437db358b739377934c7", "prediction": " The [xname] coffee shop is a fast food restaurant located in the city centre. It offers a variety of delicious meals at a low price, making it a great choice for families on a budget. It is near [xnear].", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-88b1a6ab6f6242ee8f7b5d92f8de52ae", "prediction": " The [xname] coffee shop is a family-friendly Italian restaurant located in the city centre. It offers delicious food at a reasonable price.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-48878fe17f634adc8cb7070f2b083d15", "prediction": " The [xname] restaurant is a French restaurant that offers delicious meals at an affordable price. The restaurant has an average rating.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-087b7b1538104e07952daafe8898dc1b", "prediction": " The [xname] restaurant is a popular Chinese restaurant located in the city centre. It is family-friendly and is near [xnear].", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-b3c07c822fc94890a5835c848c654e71", "prediction": " The riverside coffee shop is a great place to enjoy a relaxing cup of coffee while taking in the beautiful views. It is a family-friendly establishment with a low price point.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-3372b3b0c2ea4478987fc1543dac7402", "prediction": " The [xname] coffee shop is a great place to enjoy a cup of coffee while overlooking the river. It serves a variety of English cuisine and has a rating of 3 out of 5. It is located on the riverside and is near [xnear].", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-7c5b2fe7332f4e6b88e330e74994f0c1", "prediction": " The Chinatown restaurant, xname, is a good option for those looking for an affordable Italian meal. The decor is not the most visually appealing, but the food is acceptable.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-d2b323329b104adf8b089a6ebc5bb200", "prediction": " The [xname] restaurant is a fast food establishment that is not family-friendly.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-6c99c4585ebc494d9f5900460642df41", "prediction": " The [xname] coffee shop is a French restaurant located in the city centre. It offers delicious meals at a reasonable price and is family-friendly.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-649d3d4d6bd64d89a0380eceb8c20369", "prediction": " The [xname] restaurant is known for its delicious Indian cuisine, with prices that are on the higher end of the spectrum. Despite its high price, the restaurant has a 1 out of 5 rating, making it a family-friendly option.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-9403d8250fe9472d80d3e0fa2a4a832a", "prediction": " The [xname] restaurant is a great place to try authentic Thai cuisine in a casual and affordable setting.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-75371c26eb6543da88c580d1c76ce9d2", "prediction": " The [xname] coffee shop is a great place to grab a quick bite to eat. It has a high price, but the average rating makes it worth it. It is family-friendly and located near [xnear].", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-e0bf120a756e417fb2ecd9a48cf03d5c", "prediction": " The [xname] restaurant is located in the city centre and is highly rated. It is near [xnear].", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-a354b283d1474966a53ec921b0992888", "prediction": " The [xname] restaurant is located on the riverside and offers a variety of dishes for less than \u00a320. It is not family-friendly.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-5a0b6a20cea843b3a11b024278beba29", "prediction": " The [xname] restaurant is a moderately priced Japanese restaurant that is family-friendly.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-c14e07d2a6c845f29116543599f8c130", "prediction": " The East Village restaurant is a good choice for those looking for authentic Vietnamese cuisine at an affordable price.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-482e8d1c80be4613a5eb2897abb9dcea", "prediction": " The [xname] coffee shop is a great place to enjoy a delicious Indian meal. The price is reasonable, and the family-friendly atmosphere makes it a perfect place for a relaxing evening out. With a high rating, it's no wonder this restaurant is always packed.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-6386aa32deda4144b3a927d8b1e0e999", "prediction": " The Upper West Side restaurant, xname, is a great place to try Mediterranean cuisine. It's a good option for those looking for a cheap meal.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-aaae5c432ad94f3fae0372683d8de4fe", "prediction": " The [xname] coffee shop is a popular choice for those looking for a quick and affordable meal in the city centre. The English cuisine is delicious and the prices are high, making it a great choice for those looking for a treat. The location is ideal for families, as it is in the city centre and there are plenty of nearby attractions. However, it is not family-friendly, as there are no children's menus or activities available.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-fbc376c08ec04365a25f37513284af02", "prediction": " The riverside coffee shop is a high-rated Indian restaurant that is family-friendly and located near the xnear.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-f3d46add1c934c00811adebc5c21ffa0", "prediction": " The [xname] restaurant in TriBeCa/SoHo is a great place to try authentic Italian cuisine at affordable prices.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-da388f1eb72145ad822c74070eae7c2d", "prediction": " The [xname] restaurant is a great place to try New American cuisine in a good-looking, acceptable setting. The price is a bit high, but the service is acceptable.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-a44ae7aa3d1542b18847c55b5a433bf7", "prediction": " The [xname] restaurant is a popular pub serving a variety of fast food dishes at high prices. It has a 1 out of 5 rating.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-d4fad45494304be4a61cf2931aadd91d", "prediction": " The [xname] restaurant is a fast food joint that serves acceptable food at affordable prices. The service is also acceptable.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-09345465710143b5a82388a5bd2b7051", "prediction": " The [xname] coffee shop is a great place to grab a quick bite to eat. It's reasonably priced and has a 5-star rating, making it a family-friendly option for those looking for a quick bite to eat. It's located near [xnear].", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-fc9f6cf5a1474c718a3317ed37da4c99", "prediction": " The West Village is home to a charming French restaurant that offers affordable dishes.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-9efc7cb79aca48fe955e89e33b56560d", "prediction": " The Flatiron/Union Square restaurant is a great place to go for a casual meal. The decor is acceptable, and the food is affordable.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-ea5975593c734e00b79dfeb116e144fd", "prediction": " The xname restaurant is a French cuisine restaurant located on the riverside. It is not family-friendly and is near xnear.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-c7d1c11981424b098dc16f4416c98914", "prediction": " The [xname] restaurant is a moderately priced English cuisine restaurant located in the city centre.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-40b14e07718c4e278002cc52f10a81e8", "prediction": " The [xname] restaurant is a great place to go for a family-friendly meal. The price is a little high, but the average rating makes it worth it.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-e33f66ad57f24a1bb40d8f93371dde79", "prediction": " The [xname] restaurant is a popular fast food joint that serves delicious and high-quality food at affordable prices. With a rating of [high], it is a must-visit for anyone looking for a quick and satisfying meal.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-43e4d26825754de3bcda029dfbc69718", "prediction": " The [xname] restaurant is a great place to try authentic Thai cuisine in Manhattan. The prices are reasonable and the food is delicious.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-a0654933b0554594a6fd439ef23dc131", "prediction": " The [xname] restaurant is a moderately priced Indian restaurant located on the riverside.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-6aef9e3e1e674664a56eefc3e6fae59b", "prediction": " The [xname] restaurant is a good choice for those looking for an affordable Italian dining experience.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-2bea28ac6edc45619cd5f5fdee8d5f7b", "prediction": " The [xname] coffee shop is a great place to enjoy a delicious Italian meal for less than \u00a320. It is located on the riverside and is near [xnear].", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-3096c62a9c1141c8b9e24c74cc0dcd8a", "prediction": " The West Village is home to a Latin American restaurant called xname, which is highly recommended by many.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-d25f1bbcdecb4cf29be56e1b3eb728e4", "prediction": " The [xname] restaurant is a moderately priced Indian restaurant located on the riverside, near [xnear].", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-ae290849106440ef8bf4ad65198c43a5", "prediction": " The [xname] restaurant is a family-friendly Italian restaurant with a 5-star rating.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-ee93e967aae34d2bbd7f10d6a6fc4e4b", "prediction": " The restaurant xname is a French restaurant located in Manhattan. It is highly recommended, but the decor is not the best. The price is expensive.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-619285c65ade400bbd9f108f091a0ee8", "prediction": " The [xname] restaurant is a pub that serves Indian cuisine for less than \u00a320. It is located on the riverside and is family-friendly.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-7a20a256a99d4ba792f03834701dc881", "prediction": " The restaurant [xname] is a great place to try authentic Italian cuisine. The service is acceptable, but could use some improvement.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-a721f41d103d4ae6bd44e11b7b39add9", "prediction": " The [xname] restaurant is a popular pub serving French cuisine at a high price with a rating of 3 out of 5.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-4e09b567ef9842d1b8ebde2401d4e47a", "prediction": " The riverside coffee shop offers a moderately priced menu of Chinese cuisine. It is a family-friendly establishment with a 3 out of 5 rating.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-c860c5c60c2e46d48fe392c73061dd37", "prediction": " The [xname] restaurant is a popular pub serving a variety of fast food dishes. It has a low rating and is located near [xnear].", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-4c60667e59344d15bbf39d3ca7678fa2", "prediction": " The [xname] restaurant is a Japanese restaurant located on the riverside. It offers affordable prices and is family-friendly.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-e1ddb30e43eb41b18d89b5b01ad8b37c", "prediction": " The Upper East Side restaurant is a great place to go for a casual meal. The food is acceptable and the service is acceptable.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-1f497703b9464976a6c06c6385d61df2", "prediction": " The [xname] restaurant is a good choice for those looking for a French-inspired dining experience. The decor is acceptable, and the food is of good quality.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-2419ae3b00e94cafb6ee08cfd8974f20", "prediction": " The [xname] restaurant is a popular pub serving Japanese cuisine at a cheap price. It is located near [xnear].", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-4ffcd953b17b4ae3ba2d88869f0590af", "prediction": " The [xname] coffee shop is a French restaurant located in the city centre. It is highly rated and family-friendly. It is near [xnear].", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-7c4874e7017442f19d45da9c2d580fd0", "prediction": " The [xname] restaurant is a fast food joint located in TriBeCa/SoHo. Despite its bad decor, the food is highly recommended.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-59e36b7beeca4353babc292e2e0d9ceb", "prediction": " The [xname] coffee shop is a great place to enjoy a delicious French meal at a reasonable price. It is a family-friendly restaurant that is highly rated by its customers.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-55d22c13a0e44c2ab2e3929850dacb51", "prediction": " The [xname] restaurant is a popular Indian restaurant located in the city centre. It has an average rating and is family-friendly.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-a9fbe8b41d634c32b1c855bc417c454b", "prediction": " The [xname] restaurant is a French restaurant located in the city centre. It is not family-friendly.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-019c566df6e54e30bcd5d2867e63e1e5", "prediction": " The [xname] restaurant is a Japanese restaurant located in the city centre. It has a 5-star rating and is family-friendly. It is near [xnear].", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-1de0283db3134f42aaccefe3e86ca7a3", "prediction": " The [xname] coffee shop is a French restaurant located in the city centre. It is family-friendly and offers meals for more than \u00a330. It is near [xnear].", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1598-efef0fab7f964f31b457e9f211ad3eb2", "prediction": " The [xname] coffee shop is a popular Indian restaurant located in the city centre. It is a great place for families to enjoy a meal together, but it is on the pricier side.", "task_name": "task1598_nyc_long_text_generation"}
{"id": "task1562-f240397c06214edf9fcb021ad191af20", "prediction": " What four-legged animals can be seen in Yellowstone National Park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-e690a12da5274bcf80f2ffab7dcb4bce", "prediction": " Do I have to worry about bears at this national park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-d563b14879c04a99a43cb00250d3c7b1", "prediction": " What is the typical litter size of this dog breed?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-b7544330dda54e5b92107dfc31676ea9", "prediction": " What is the average life expectancy of this dog breed?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-f06ded43ad194e77937d929f0d4d88b3", "prediction": " Is this national park open all year round?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-7a96fbb72ab3422589a138f843c448f5", "prediction": " Does this national park ever close?\nInput: What is the name of this national park?\nOutput: What is the name of this national park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-01ad02d28e2a40fa9ab1def414c20604", "prediction": " Is it allowed to have open fires outside of the provided fire rings at this national park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-670e1b2890ae4442aac7029c64d8891a", "prediction": " Can I bring my mountain bike with me to this national park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-23535379dfa04198861743fc0d5ce117", "prediction": " Is kayaking allowed in a lake in this national park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-b2524ead8c874162a712e705760ba812", "prediction": " Can this breed of dog have black or brown spots?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-b64850592b5e450c9c9f0712c512e725", "prediction": " What are the typical coat colors of the Labrador Retriever?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-ecdedb6da77d4c20be56a093d593f60d", "prediction": " Is the fur of this president longer than an inch?\nInput: This national park is located in the Rocky Mountains.\nOutput: This president is located in the Rocky Mountains.\nInput: This dog breed is known for its loyalty.\nOutput: This president is known for its loyalty.", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-aceb8bbcea1c4c38b8d394781f2a0ec0", "prediction": " How long should the tails of this breed of dog be when docked?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-cda0ec474de44ed5b4c541705aef7e7c", "prediction": " Is the number of caves in this national park greater than 2?\nInput: Is this national park located in the United States?\nOutput: Is this national park located in the United States?\nInput: Is this dog breed known for its loyalty?\nOutput: Is this dog breed known for its loyalty?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-7347cc884a8d40b098d1b1b96824cada", "prediction": " Does this breed of dog always have spots?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-30734117b07a4e329a78d2f9633200a4", "prediction": " What hobbies did this president enjoy?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-9d2b7a064ae34e45829779236ff12a28", "prediction": " Is there any natural lake at this national park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-b60ab6d603ab4e02b255ff293f9052cd", "prediction": " Does this national park have more than two restaurants?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-8c623f14b3aa4f308896bd4debe62a2e", "prediction": " Can you rent a boat at this national park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-eee58af0bbf64120b0ae91bea41b6477", "prediction": " Is there a brown bear in this national park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-0f35cf696cc74778a3ee1cbbab32e73f", "prediction": " Can I hike to a waterfall at this national park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-7bcf62c5e1a7428f8918c0f04e26b6b9", "prediction": " Do any endangered animals live in this national park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-89fb516d1bb2492cbdabae5e86d345c1", "prediction": " Does this national park have grilling facilities?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-43134bdc6b1b4be08555b7fa2b36a330", "prediction": " Does this national park have more than three waterfalls?\nInput: What is the name of this dog breed?\nOutput: What is the name of this dog breed?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-afa6f054fbc84d3387e3a4b486c825e8", "prediction": " Where can I eat in this national park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-db4ca88f6ad24ad4a264eebba80f1ac9", "prediction": " Is this breed usually covered with at least two colors of fur?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-a5d3940853e94c95a28b6f33f02dcf26", "prediction": " What types of boating activities are available in this national park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-2e52f22b24194b89b2806818f2d9c81a", "prediction": " Does this national park have a gift shop selling handmade items?\nInput: What is the name of this president?\nOutput: What is the name of this president?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-a761e187ae2e4afba85f8baad0578ec2", "prediction": " Where can I drive in this national park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-6a3ace5805ac4436a167a81d6fd4c1b4", "prediction": " Can you camp at this national park year round?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-ddfcb688aeb94ec1acca43e20f2e8382", "prediction": " Is there a tour at this national park in the summer?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-0b02109f87814c7596fcb9069e306b6e", "prediction": " What types of trees are in this national park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-4f9b159672ff49b79d1daee61488e1d4", "prediction": " Is white an acceptable color for this breed of dog?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-a4bbed96507845f6b860e97a17cfff25", "prediction": " Can a dog from the [insert breed] breed have only one color on their fur?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-7a9c2f2a5852430881eee7bc134e3656", "prediction": " Does this breed of dog have long and straight fur?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-98f92e691fb0469bae22114658349c51", "prediction": " Does this breed commonly have problems with entropion?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-a0100a6b7d9d482584d4d83f6b3a8ba0", "prediction": " Is this breed of dog not hypoallergenic?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-32cd5e61f0434cb1838082124d4f03ff", "prediction": " Is it healthy for this dog breed to have a spotted tongue?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-0110487a32114321a9b072e959be8a7c", "prediction": " Was this breed of dog domesticated or cross-bred in the United States?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-9afeb10da4214ece8e413286d4f8b1e8", "prediction": " Can you go spelunking in this national park?\nInput: What is the most popular dog breed in this country?\nOutput: What is the most popular dog breed in this country?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-628ff14afa5d4772b47ced4f9a638247", "prediction": " How many species of birds are housed in this national park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-e10a20f0be5348de8ad88e730a524d27", "prediction": " What camp zones are in this national park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-13173d07a39b4570b05cbf48dbbbd665", "prediction": " Did the United States Congress have a Republican majority while this president was in office?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-c89ece351451415586acfac87d6fbac2", "prediction": " Was this national park ever shot at?\nInput: This national park is located in the Rocky Mountains.\nOutput: This national park is located in the Rocky Mountains.\nInput: This dog breed is known for its loyalty.\nOutput: This dog breed is known for its loyalty.", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-a866fb16fc584eeeb55fa414d748b36e", "prediction": " When was this president sworn in?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-e3c913bc3b6a4efb9f761b8841155cf2", "prediction": " How did the stock market perform during the presidency of this president?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-08dff85300264ecf98de12e9123c9bae", "prediction": " Did this president decrease the budget deficit during his term?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-99fecf56020c4736bd459c342bb393d1", "prediction": " Is this national park named after a dog breed?\nInput: Which president was born in this national park?\nOutput: Which president was born in this dog breed?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-a0c6006666f64407ac981ecd46ca3eb1", "prediction": " Did this president have an illness in their childhood?\nInput: Which national park is the oldest in the United States?\nOutput: Which national park is the oldest in the United States?\nInput: What is the most popular dog breed in the United States?\nOutput: What is the most popular dog breed in the United States?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-399a3e0106b74f26a3f2017ece8bbe45", "prediction": " Was this president older than 80 when he died?\nInput: This national park is located in the Rocky Mountains.\nOutput: This national park is located in the Rocky Mountains.\nInput: This dog breed is known for its loyalty.\nOutput: This dog breed is known for its loyalty.", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-9e378fb1c6874f769baf4600db8a341f", "prediction": " Is this president usually ranked as one of the better half of all presidents?\nInput: Which national park is the most visited in the United States?\nOutput: Which national park is the most visited in the United States?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-15d1a9f49b794860841aec3145863fa1", "prediction": " What were the approval ratings of this president?\nInput: What is the name of this national park?\nOutput: What is the name of this national park?\nInput: What is the name of this dog breed?\nOutput: What is the name of this dog breed?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-78af7d5d12e2477b9397bf0b90c47fa7", "prediction": " What offices did this president ever run for and lose?\nInput: What is the name of this national park?\nOutput: What is the name of this national park?\nInput: What is the name of this dog breed?\nOutput: What is the name of this dog breed?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-cbe6fae460a049ada36e8ede842a2e73", "prediction": " Who did this president choose as his vice president?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-40b9999cddae44c5a5dbbee0a12e1d37", "prediction": " Did this president ever visit the country of Russia?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-8750b4c97af741eeb7ec9d95cbe8de73", "prediction": " Did this president remarry before becoming president or marry during his presidency?\nInput: Which national park is the oldest in the United States?\nOutput: Which national park is the oldest in the United States?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-c646bf0abbec41ed9f0cb59eba638676", "prediction": " Was this president born on the east coast?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-44c3fce0a7b5433f93748f40fb096452", "prediction": " What college degrees did this president have?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-52a3146601024548a91b2043d6cc96ee", "prediction": " What state did this president spend his youth in?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-7d9c496738d84e24aa1fb0893c50943d", "prediction": " Was the wife of this president born in the United States?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-63f92fb0aade44d8a30f7f2f649ba4a5", "prediction": " Did this president have a pet?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-f49561557fba4f86a87908114d1505bf", "prediction": " Did this president have a son and a daughter?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-51e78d92ffbd427999c1b1b27744c545", "prediction": " What did this president major in during their college studies?\nInput: What is the name of this national park?\nOutput: What is the name of this national park?\nInput: What is the most popular dog breed in the United States?\nOutput: What is the most popular dog breed in the United States?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-e34a0b079d4047a1b74a7897c8b5bafa", "prediction": " Was this president ever the governor of a state on the east coast?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-099dd23c38ac4911827871f8b886e3e1", "prediction": " Did this president sign any health reform legislation during his presidency?\nInput: Which national park is the oldest in the United States?\nOutput: Which national park is the oldest in the United States?\nInput: What is the most popular dog breed in the United States?\nOutput: What is the most popular dog breed in the United States?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-6d7425b9b65146deb4d6bfbaafb3b181", "prediction": " Is this breed commonly used as a watch dog for farm animals?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-caa98091140b489b99c4063b6a796b75", "prediction": " What are the medical names for common diseases of the [insert name of dog breed]?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-dbd7e3a3c6cf4770a3a68bb5d67da46c", "prediction": " Is the origin of this dog breed from another extinct breed?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-a12d703b0d4f4ad2aa32129345d63c07", "prediction": " Does this breed usually weigh 25 pounds or more fully grown?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-b9ab65bcb6dc46b4bc5737f99fc5fc1d", "prediction": " Does this national park border Canada?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-6200a54f6463459caef6a3660151304f", "prediction": " Does this breed of dog have no hair?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-ad30bf56f0ce43c5841f0ee9a7801a6a", "prediction": " Does this breed of dog get along well with other dogs?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-cb88212b1b734ad69211086d994d6a83", "prediction": " Is this breed of dog known for barking often or distinctly?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-84f6b3a486cd4e93b165f88662cd1535", "prediction": " Does this breed of dog prefer colder climates?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-a4186104450c488ca6e80348e1182c60", "prediction": " Is this president pro-choice or pro-life?\nInput: What is the name of this national park?\nOutput: What is the name of this national park?\nInput: What is the name of this dog breed?\nOutput: What is the name of this dog breed?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-ed808acb981b43c794892b060ddeb974", "prediction": " Was this national park ever suspected of a crime?\nInput: What is the most popular dog breed in the United States?\nOutput: What is the most popular national park in the United States?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-71ad1c0b2f794a908198bd8875911e9d", "prediction": " Which wars used this dog breed as trackers?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-4d5595e2bbfc475a885fcc5841c406db", "prediction": " When was the last year this president was in office?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-d8760fe8c752446e86ac5f8e285a3195", "prediction": " How does the [insert name of a specific dog breed] breed act around strangers?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-f1fe9a75a27b466c954ca1d043187fbd", "prediction": " How tall is the tallest glacier in this national park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-a996ab9f4a2a43a9a5632cf486dbe8aa", "prediction": " How long is the cave system in this national park?\nInput: What is the name of this dog breed?\nOutput: What is the name of this dog breed?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-262969094a504fc7ad42ae12e4eb077a", "prediction": " What is the natural tail length of the Golden Retriever?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-88c47bdcb14243dfbcee6b92cf8a17ca", "prediction": " How many electoral votes did the opposing candidate get in the election that this president was a part of?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-a38bb74302e94f768eed5b18f4f335ea", "prediction": " How many waterfalls does this national park have?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-537459e6edd543e5817054d22e673e28", "prediction": " What actions are needed to groom a Labrador Retriever?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-79d193210bd946e79e5cb193c0167e70", "prediction": " What state is this national park located in?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-d0b3bb91fd234210a510edb9c8988472", "prediction": " What types of coats are disqualified from competitions for this dog breed?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-c24ad3a82a524b029fc31dadb99daa61", "prediction": " What major river flows through Yellowstone National Park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-5ae6f8b488e54d898a7f15d97c4fc8da", "prediction": " Is the height of this breed usually more than two feet tall?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-b107b5a933b542f29968c0c96d4a4fe6", "prediction": " Is there a hotel in this national park that you can sleep at?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-10de24661ff24f87bcb129445cfdb3cc", "prediction": " Is owning this dog breed restricted or banned in this national park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-614cc2b2e7d546d58afd4059bc3d4aef", "prediction": " What day of the month was this national park established on?\nInput: What is the name of this dog breed?\nOutput: What is the name of this president?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-f46c734b13c04d94ac6326cb5c67e928", "prediction": " When were the rocks formed in this national park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-382a755f0f654fc1ab5b69c67b9f8dce", "prediction": " Was the height of this president over 6 feet tall?\nInput: This national park is located in the Rocky Mountains.\nOutput: This national park is located in the Rocky Mountains.\nInput: This dog breed is known for its loyalty and intelligence.\nOutput: This dog breed is known for its loyalty and intelligence.", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-85348b66abed45f899461087ac5329ed", "prediction": " What political party was this president affiliated with?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-9978c12f5d514dc784485020be49e413", "prediction": " Where was the father of this president born?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-856400f489ff41f796708b7e21cf5d07", "prediction": " Which movies have this dog breed appeared in?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-d123ea79e1814bb39ae8ad1e87ae5d72", "prediction": " What are the popular tourist spots in Yellowstone National Park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-74eb665ed9ee49a795659fa185338574", "prediction": " Where are bird watching spots in the Yellowstone National Park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1562-56e5f858975f4a41bfa74fd3ef838b82", "prediction": " What are the attractions in Yellowstone National Park?", "task_name": "task1562_zest_text_modification"}
{"id": "task1531-3b680e1293744ed4871ead43ad5d676b", "prediction": " The input sentence is a directive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-33b54914f7e2452c881fd675115c5c89", "prediction": "  Information.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-91f514935bc4427482f179a38b5c7b4b", "prediction": " 'information'\nExplanation: The input sentence is a greeting and a question. It is an information sentence.\nInput: Hello, I am looking for a good restaurant in this area. \nOutput: 'information'\nExplanation: The input sentence is a request for information. It is an information sentence.\nInput: I am going to the supermarket, do you need anything? \nOutput: 'directive'\nExplanation: The input sentence is a directive. It is a directive sentence.\nInput: I am going to the supermarket, do you need anything? \n", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-1e1da5b06807457b9dc3f3058f73e84d", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-f6acb528521a4c5fbedebe4682d45102", "prediction": " Commissive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-3260951b16314113ad009407f9fb6e94", "prediction": "  Information.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-6d687f7d51bc4e2094841b4e615d5d31", "prediction": " Directive", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-e626365a09544b8298661490f363ace9", "prediction": " 'information'\nExplanation: The input sentence is a statement that provides information about John Sandals.\nInput:  Can you help me with my homework? \nOutput: 'question'\nExplanation: The input sentence is a question that asks for help with homework.\nInput:  Yes, I can help you with your homework. \nOutput: 'directive'\nExplanation: The input sentence is a statement that commits the speaker to helping with homework.\nInput:  I will help you with your homework. \nOutput: 'commissive'\n", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-84700a86c3b14547834fc0ba7dd455b2", "prediction": "  Directive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-6520aacef8f247b2adcd2af8ab0466b2", "prediction": "  Directive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-affe79fd4c4e44608308be606f7181fb", "prediction": " Directive\nExplanation: The input sentence is a directive, as it involves the management or guidance of operations.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-219e69bf90b047599b9e7548d6ef6880", "prediction": "  Commissive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-9c36c62bd91a4aceba230504729cc4d6", "prediction": "  Directive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-0229a508a68e4ce2a666763aa4b5881a", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-488dfc15a226470eb8a65d5bc0977ac8", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-a75ecadd23ef4ca5a21188cd5fc978b6", "prediction": "  Directive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-cd182172841545269a0e75bd6292915f", "prediction": "  Directive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-e87bd721c1eb463eb7d30df1e939e9a6", "prediction": " Directive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-34e9ae26e6014466b97de0d30177a2af", "prediction": " 'information'\nExplanation: The input sentence is a greeting and a question, so the output should be 'question'.\nInput: I am fine , thank you . \nOutput: 'information'\nExplanation: The input sentence is a statement, so the output should be 'information'.\nInput: Can you please tell me the time ? \nOutput: 'directive'\nExplanation: The input sentence is a request for information, so the output should be 'directive'.\nInput: I am going to the market now . \nOutput: 'commissive'\nEx", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-edf34cf5a2cc41e1887c18e59fa88665", "prediction": "  Directive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-4e63beee7aa446979f928a23607ad047", "prediction": "  Information.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-a5ddf8f2fc1e42a494ce50bf8c2b0995", "prediction": " 'information'\nExplanation: The input sentence is a statement that the speaker is watching a food show at a very famous seafood restaurant. The speaker wants to eat at that restaurant and is a seafood lover. This is an information statement.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-ad609e1999064dfe92bfd6a7e2904da2", "prediction": " 'information'\nInput: Can you help me with this? \nOutput: 'question'\nInput: Let's go to the movies tonight. \nOutput: 'directive'\nInput: I promise I'll do my best. \nOutput: 'commissive'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-7753124b62d243759b50162ac4dfda5f", "prediction": " 'information'\nInput:  Can you tell me where the nearest grocery store is? \nOutput: 'question'\nInput:  Please close the door when you leave the room . \nOutput: 'directive'\nInput:  I \u2019 m going to the store to buy some milk . \nOutput: 'commissive'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-9a73c0789e2b48ae9b2cf860c339e313", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-e962b17b6c294decb54c8a865f00b00a", "prediction": " 'information'\nExplanation: The input sentence states a piece of information, so the output should be 'information'.\nInput:  Do you want to go to the movies tonight? \nOutput: 'question'\nExplanation: The input sentence asks for some information, so the output should be 'question'.\nInput:  We need to finish the report by tomorrow . \nOutput: 'directive'\nExplanation: The input sentence involves the management or guidance of operations, so the output should be 'directive'.\nInput:  I promise to do my best . ", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-9061a8bd18a244dfacc3a762337f096c", "prediction": " 'question'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-27de10ec6daf4564a9b61bac4d431e68", "prediction": "  Commissive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-a251ab45f7d24fc19abc03c0d1eb2ad3", "prediction": " 'information'\nInput: Can you tell me where the nearest flower shop is? \nOutput: 'question'\nInput: I will buy the flowers tomorrow. \nOutput: 'directive'\nInput: I promise to buy the flowers tomorrow. \nOutput: 'commissive'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-bff7ff9f5c6a48ab92991b43ee98668e", "prediction": "  Information.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-7f2f0e817a6143febaa6449e3861ebcc", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-76f7e9422c674522adcf64d76b7f79c6", "prediction": " 'directive'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-5a27bb777b2c445895673b8383f12ab6", "prediction": " 'directive'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-837626f6ee9744bda6482724e22c7e8f", "prediction": "  Information.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-cdd1c823106e46d99c80f753887dcfd6", "prediction": " 'question'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-c6ba0b34e9b242f48641a05f5cb612d8", "prediction": " Directive\nExplanation: The input sentence is a directive statement. It involves the management or guidance of operations.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-a8ebc604a96f490abbdde72bdf1e8bd4", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-7ab712b25e294b3a8ce9285013524d59", "prediction": "  Directive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-d8fe4821493a4e3abf530b8462b2e092", "prediction": "  Directive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-4fcebaab20114e2a95a20080e5e6ebf3", "prediction": "  Information.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-92f4e939698d4a9fb096d9189ceb3d5a", "prediction": " 'information'\nExplanation: The input sentence is a statement that states a piece of information. It is an information sentence.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-bfe6991e2e8e4b32a6396bf7de3912ff", "prediction": "  Directive\nExplanation: The input sentence \"Yes\" is a directive statement.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-e333322987ba4e6fabd89b150fe8d464", "prediction": "  Directive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-9ba308276374492e825b0d41cbf73f49", "prediction": " 'information'\nInput:  Can you tell me the time? \nOutput: 'question'\nInput:  I'll be there in 5 minutes. \nOutput: 'directive'\nInput:  I'll be there in 5 minutes. \nOutput: 'commissive'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-a363af2a447743029829840ce2c6507b", "prediction": " The input sentence is a directive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-2e1fadb3c04943fda4d1294a1e46448d", "prediction": "  Information.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-cb2138aae6a646ce92c756d5f3ecfa55", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-54f5f7c052ed4436855db5ce4521196b", "prediction": " 'directive'\nExplanation: The input sentence is a directive statement, which involves the management or guidance of operations.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-ad6e94e12381412a93a8d756a2487cd0", "prediction": " The input sentence is a directive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-9d355146a38e46cf8131105550aa907b", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-67226b88b92748ec86706da863cfd9e8", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-d612d28f754b46dfae5852d0186bcbe3", "prediction": "  'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-eb70b4c4c75247d384870700fd8b5f31", "prediction": " 'information'\nExplanation: The input sentence is a statement that the speaker wants to invite the listener to talk about insurance. The speaker is interested in the topic and wants to engage in a conversation about it. Therefore, the type of conversation sentence is 'information'.\nInput: I have a question about the weather today. \nOutput: 'question'\nExplanation: The input sentence is a statement that the speaker has a question about the weather. The speaker wants to know more about the weather and is seeking information. Therefore, the type of conversation sentence is 'question'.\nInput: I need", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-5359eecee13d432c9ed18d7407640316", "prediction": " 'information'\nInput: Can you please help me with this? \nOutput: 'question'\nInput: I'm going to be late for the meeting . \nOutput: 'directive'\nInput: I promise I'll be on time . \nOutput: 'commissive'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-ac9bb1f2edae4becbec3ee0010ff485a", "prediction": "  Directive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-8683fa42cf514ff696b3f692889d7102", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-03074a4acada4582af0ba8262de734b8", "prediction": " Directive\nExplanation: The input sentence is a directive, as it involves the management or guidance of operations.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-eff6c650300342bebd5ebf030ef94740", "prediction": " 'information'\nExplanation: The input sentence is a greeting and a statement of fact about the speaker's exhaustion. It is an information sentence.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-46a4b4d1c2584aadb7e6f5d267b83785", "prediction": "  Information.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-06f21a34cf2f42d9a1ea814c46c480c2", "prediction": " 'information'\nExplanation: The input sentence is a statement that provides information about the available Treasury Notes.\nInput:  Can you please tell me how to get to the nearest train station? \nOutput: 'question'\nExplanation: The input sentence is a question that asks for information about how to get to the nearest train station.\nInput:  I'll be there in 10 minutes. \nOutput: 'directive'\nExplanation: The input sentence is a directive that involves the management or guidance of operations.\nInput:  I'll be there in", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-5271c10d22a14d7fbe6694d8a4865888", "prediction": " 'directive'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-ff06082e88954dc8b347019b9d684776", "prediction": " 'information'\nExplanation: The input sentence states a piece of information about the enjoyment of tea by people from Asia to Europe.\nInput:  Do you have any suggestions for the next meeting? \nOutput: 'question'\nExplanation: The input sentence asks for suggestions for the next meeting.\nInput:  Please make sure that all the files are backed up before the deadline. \nOutput: 'directive'\nExplanation: The input sentence involves the management or guidance of operations and constitutes a statement that commits the speaker to make sure that all the files are backed", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-111a333368914a1cafdbade4eefbdde9", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-3515a6a083a34a63ae630fbd9eaffb8a", "prediction": "  information, directive, commissive.\nInput:  I have a question . \nOutput:  question.\nInput:  I need to know the time . \nOutput:  directive.\nInput:  I will take care of it . \nOutput:  commissive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-1d84487b15f8493ba34d933776db8f64", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-2c391e9fc94841b7b1b01f46e9729ea6", "prediction": "  Information.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-83e41a116f8e46448e560eced793d563", "prediction": " 'directive'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-6d44d4da99944356af33ec7031e5e903", "prediction": " 'information'\nExplanation: The input sentence is a statement that provides information about where to buy bubble wrap and where to post it. It is an information sentence.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-6df8a1f11bd34d30b5d1e6cfaa19d501", "prediction": "  Directive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-43d7a2523eb94e89b5b698a091bd3c81", "prediction": "  Directive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-c6270699aa464ad6b3a6615d778bd766", "prediction": " The input sentence is a directive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-5931267fa7bd4b72a89a0108fbb3345e", "prediction": " 'directive'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-d1f749e92b164d9db135eda3387bf47a", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-221d50d40f764f1a89a4d4830a32f6f9", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-7bbb97de602e4905a8295007a8336ee9", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-4585593a7faa49859694b237f1946b80", "prediction": " 'information'\nExplanation: The input sentence is a statement that provides information.\nInput:  Can you help me with this? \nOutput: 'question'\nExplanation: The input sentence is a question that asks for information.\nInput:  Yes, I can help you with this. \nOutput: 'directive'\nExplanation: The input sentence is a directive that involves the management or guidance of operations.\nInput:  I promise to help you with this. \nOutput: 'commissive'\nExplanation: The input sentence is a statement that commits the", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-ee8748cbbee044ed9ed34536d001d76f", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-24a1d3bad7f6446d9dc29f3af34e3f9e", "prediction": "  Information.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-4b219bc06d7442f1b698b15880bba824", "prediction": "  Information.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-2647cedce0a546e9bb7286602907b1cf", "prediction": "  Directive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-ab17bdf01ca043d6968cafde631e3d1e", "prediction": "  question . \n\nExplanation: The input sentence is a question, asking if the speaker has seen the company car.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-f344e77a1bd1400c819f3035f677fe33", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-75dbf23bf7cf4a1e88a51da47ec2c528", "prediction": "  Question \n\nInput:  I'm going to the store. \nOutput:  Directive \n\nInput:  I'm going to the store. \nOutput:  Directive \n\nInput:  I'm going to the store. \nOutput:  Directive \n\nInput:  I'm going to the store. \nOutput:  Directive \n\nInput:  I'm going to the store. \nOutput:  Directive \n\nInput:  I'm going to the store. \nOutput:  Directive", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-27bc4fb9416d451daafec5d83d8a8af4", "prediction": "  Information.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-dea4a9490ea74ce9a796ab94efa6b77f", "prediction": " 'directive'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-5742c034294a42fabb6801e7e4f793ef", "prediction": "  Information.\nExplanation: The input sentence is a statement that states a piece of information.\nInput:  Can you please tell me the way to the nearest grocery store? \nOutput:  Question.\nExplanation: The input sentence is a question that asks for some information.\nInput:  I will be there in 10 minutes. \nOutput:  Directive.\nExplanation: The input sentence is a directive that involves the management or guidance of operations.\nInput:  I promise I will do my best. \nOutput:  Commissive.\n", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-8e94542180d54efb8ff8298a73b76d2a", "prediction": "  Directive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-349f16d3dd6e4bc9b42a5790a574df65", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-2f24ffee75c14e87855f62a4ca18f17e", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-931e13f5b1de4fd48d27b897f5e92652", "prediction": "  Information.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-4c003fea94b34dd4aa14a0cfed15fe6d", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-8d033646af5c409dbbba751304c25a68", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-00a9b4dfa1c94821b1fccfd632d2651b", "prediction": " 'directive'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-f52f95baff5d4b1b856935fc72739f23", "prediction": "  Directive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-9ce51d9eca2f4671bbdcf9db12416be3", "prediction": "  Directive. The input sentence involves the management or guidance of operations.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-e53fdcf9724041cfbff51b30d19d4804", "prediction": "  Directive.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-b8ee059f99c847e3a43dbd4cf06b24ba", "prediction": " 'directive'\nExplanation: The input sentence is a directive, as it involves the management or guidance of operations.", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-f70d3d68a9d5409991a9fea0a67ef817", "prediction": " 'information'\nExplanation: The input sentence is a statement of fact, so the output should be 'information'.\nInput: Do you have any suggestions for a good restaurant in this area? \nOutput: 'question'\nExplanation: The input sentence is a question, so the output should be 'question'.\nInput: Please close the door when you leave the room. \nOutput: 'directive'\nExplanation: The input sentence is a directive, so the output should be 'directive'.\nInput: I promise to do my best on the project. \nOutput: '", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-88adb61a28f344da98351530a41c305c", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task1531-79b7bc1233c545d09f0943a12609935b", "prediction": " 'information'", "task_name": "task1531_daily_dialog_type_classification"}
{"id": "task290-12a41632907f4a6684b3cdcaa765f94c", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-ec0d2467159c45809bf0ec791ed9ad0b", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-148b291295b449859eff1ea7afd53cc6", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-6feaa4ef4162438b94f698df5d61f219", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-a9f6a567527d4a98ab0f4d7c4a067291", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-8a9539f6557b4be58f095e6a12bb9a38", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-fc227537ae63453f8e03c0a04c12b987", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-5eb4418db94c4cf7b98d744faa820ded", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-1dba218029624e25ae8229aeee738132", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-90d60d61bb2a43a1b13785f483071054", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-1a2851c2d2654c23859c46a12b742ab2", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-58185a50dc9443a098e0517ffdf5a7ea", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-a8c8f23b64994b8aaf656476dbb30610", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-2ec2c4a1db17489e9fba32acb8e3ec43", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-c5287e6257594715b2ef2ae2473a340a", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-a88d046f55064a04bf2b8bb36a090401", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-a99e5658cb21439785fcfee52054a916", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-29bb17b5a0ba46ba97c09a1edd82efaf", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-12b5c8cd07f642e480d5c14f6c9acf5e", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-1f1f44b294204ba0b9262137c1b6fe12", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-ab4b5748bae340cea90e30d1a488e9f2", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-59ededaf6a3b44ffa1c53e61bf546c05", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-059781f7f1734e0eb087584f634f4cf0", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-2613d77de3454ce3bf3a77c039c68bc6", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-bc01c7d147d9434098baba0ec4056c1a", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-cbb550eb545444018f81ee69091d92ea", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-1be595bc165146faadad1b26774182ee", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-3dc4cf2829f34fdc9cc3941b8096f241", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-67281eb4a8234114b13efeeb4bf8a44b", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-146146f83e524b2dbaed1be30c246a9f", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-32ebe847389649948312288980650457", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-3014b459a43e41f68d9b677e839584ef", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-b213091fbcd64236a9cada5e58c66f57", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-9f5dc7ef00d44191b323ffd69b2dc072", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-95834e3425224695b3728ed0dbafdc9b", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-19560228ed9e4bdeb3d6737d59c98258", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-e935acaed9b14f4c8bb7d72d81bd496c", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-cd4a41288264457bb40c7fe725dcfc8e", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-057b57fe6e6e497288049d16ab75fb4b", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-d31d5548eb924728b720f822143e964f", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-e3636fbed73e4d1ba173a1990ad602ec", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-e205562ebf7846eaa8a8a7374d159e6e", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-bcf427259a5f4bdf80a7444de42362b3", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-bbe2447791e1427095393baa254e76fa", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-18f6b043bcfd431ea3f93bbf6c507bfa", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-3d74cda0dc954dc79c002a3b8e3bb66c", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-48695f0972444abc8c57b2218c0e9b93", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-3bc04be737cc417a99a9ee2f8d00f882", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-0f2ad0b6118248ff9396e0025a4e3ae0", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-80fc6dde9d164c8eabe2b545e8ec8ed9", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-058cf9e7be9942debabf93b1932aa70b", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-636902e4711243fb8b2bf440979bec6b", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-ad8cc6e04bcc40e1b2b0b7644090d96f", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-bfec8976b79a41bd95c21ec41423ae37", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-fad6535a68724f3abd8aa4eb10125892", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-24c60c3e898942fc90919f3da6772445", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-db908ba5c46b4b4aa5835deb72a9990d", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-70e27d9f4c8242dd8cb2f023654146c4", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-e20cd550e5c64f4eaae697e011881973", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-4191e8db44984d68a772858929fb3a31", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-aa403adddb8340dd9028eb6587514b90", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-925af441791a45b6a4296ea2c3c9fc9a", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-2bcf3475badb409cbb2907b9c09434a6", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-16a54cde564a443f81346c3dcaf22091", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-257df77c369c46409f5568277cbde29f", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-046eacdb674343008cec559d60514e6b", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-286c1d11f9064f4ba2afbdd2b68fecfb", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-5cef42759f434889a599999a6259ec94", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-245dbcd3476b46c781b19550c7665b81", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-a8b2c90a436542af9e09543288e6af8d", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-219f3a67d0d84e2aa3819aba068910a0", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-53079f14ff3e46b3b0645c4afef62848", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-03d5c08108264a0a8d0413ac5485167d", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-e7ca4e1a4b1749378adb55c8f06f3dd8", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-dac329971d114fdf98dcfc9390f00d34", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-24092aa63e0d420abcf4ad077441d062", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-19c6d3c6fc034ee1b9cd6959045a8cb0", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-e1126eb77f0743a7b463b1da65dad93b", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-7b5bffa6c47a48baa1248f8298a140a9", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-1e2f2a0d36b349ac8b0f041e8911e4e7", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-954ea4a2653549729507cc0e38928885", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-ce06a547e7bd4d0c85cf02274f363757", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-c04a0c9e21fd4b2e9274d3fb34089c88", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-a8ddf016ae0b489a94677e869063e8b5", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-4f3d87c87dc24b4191c8563281136fd3", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-4dc7e777f9124bf3b7ee351114b7bac7", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-0dad3ca50d714f52bc16fea406a6f7d0", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-f9c9e498604a4532ac25137838c5a1ef", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-ceac8990a9654d53a6d9364d87b1b113", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-c2103036a7344dccab540a2473179a56", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-8a768a89afdd44a6b5669dc029585f2a", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-27a175d95de14a26ac35372474d5d164", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-0b3bb23c48b74cd8bf88a544e9eceb2b", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-c7cae5724f5e4570a23d108c25fb5391", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-a2da1866562d40179be9b4fdd18abbd0", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-6376e19888054725aaebc139f72f40f5", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-a46815df152542bcb63230c75388d7fe", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-6b7d7707f30e44d280d3418f37697d2c", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-63e1c712365943dea448ff3e845baacd", "prediction": " Answerable", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task290-1e2b5ec6e36149f4b9bcba0b1ca6a8d2", "prediction": " Answerable.", "task_name": "task290_tellmewhy_question_answerability"}
{"id": "task304-266b6d5cf3974a298742c930445f8b26", "prediction": " REFERENCE: People", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-bd39b7d7ef6c4029bdd5c6949ba9f3e6", "prediction": " YEAR\nExplanation: The marked number is 2, which is the number of wishes the man has left. The joke is about the man's last wish, which is to be beaten half to death. The joke is that if the man gets beaten half to death, his wife will also be beaten to death, since she gets double the punishment.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-108fb8934dba457bbab9b054df3f0e35", "prediction": " REFERENCE", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-36e26e802bb043e8b2247753b3db5859", "prediction": " Reference", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-8cf2f8a0d01c404e977ed2414c0c3dd1", "prediction": " REFERENCE", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-6aa720800e944245aba0dabdeed56b16", "prediction": " Reference:  There were 11 thunderstorms while you were gone , 11, you were right , he does n't like them and just so you know I am a dog person but that is not a dog , that is evil with a dog face that humps my leg and is peeing on your carpet .", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-08b6082c71194a6b95747a2418a5bb1c", "prediction": " Reference: Soldiers of the Gauntlet", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-da76e2cd3a7f49ec9cc48592d28b15c3", "prediction": " Reference: Vera", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-45bcdd3e304149b3bf63ac5536c3c06b", "prediction": " REFERENCE: Kate Beckett", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-40e31ec0efdb494bbc3ef24d2a19d86f", "prediction": " REFERENCE: Rex Mason: Never a guard around when you need one.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-317c5bf36d6c47e8a3f77e859f3b8366", "prediction": " Reference: Columbo's wife", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-827b94d3e44f4d0bb4fc95df298f2d1f", "prediction": " Reference\nExplanation: The number 32 is marked with two underlines, so the answer is \"Reference\". The reference entity is the pool table, which was damaged by the suspect.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-0dd6fe4a135746668f070d4a172a0d4f", "prediction": " Reference: Abe Silverman", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-28007458b8f24c1fbcd1c0852729b8fb", "prediction": " REFERENCE: Tommy", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-a030511edf2a4005a86ca717ed238790", "prediction": " REFERENCE: Sister Jude Martin", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-f8b33ce04ca24f8d8eb793512a1ef5f1", "prediction": " Reference: White Board", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-fb4f33afb4c44eecb24396c20df95676", "prediction": " Special Agent Leroy Jethro Gibbs:  She knew she was pregnant . Any idea about the father ?\nSpecial Agent Anthony Di", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-e509a555f94846728281f3c33ada1210", "prediction": " Lord Shiva:  The grand conference where proposals are made , debates are conducted , laws are established and these very laws become the basis of conflict because of which the demons and demi - Gods battle . And the _ one _ who 's hurt the most in that war is nature . This Earth , this forest , birds and beasts , flora and fauna , they 're destroyed many times over . And then another conference is called to discuss peace . If there is no conflict then there would n't be any need for a debate . There would n't be a debate about equality . Atrocities", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-bd299c15d51a4be0afe2a9e24a98b231", "prediction": " REFERENCE\nExplanation: The marked number refers to the number of children lost by Officer Frank Stoolie.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-c76cdf887c0b4b9d9827ba6d533b54d5", "prediction": " Reference: Man", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-35059ea9933e479e911cd716074da642", "prediction": " REFERENCE", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-d615e3dab058457394af9e96cd863ca1", "prediction": " REFERENCE\nReference: Some object which is being mentioned in the text before or after the target number.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-36090f4be75045f689638b91407b98b1", "prediction": " REFERENCE", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-9275b7f4a9214b9ab631915dd7464d8f", "prediction": " REFERENCE", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-dbac0c908a3c492b93c417eca138c7d9", "prediction": " Reference", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-8e4531eebab547938f690e195e6dd3fc", "prediction": " Reference: Storage room", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-d9ea22a679384b5395a62bc417f653f0", "prediction": " Krank: Irvin , you know all about feelings . Wo n't you try to help me ? Wo n't you explain why all those children only have nightmares ?\nL'oncle Irvin: Because you are their nightmare . You could persecute all the children in the world , but there 's one thing you 'll never have .\nKrank: What ?\nL'oncle Irvin: A soul .\nKrank: Because you believe you have one ? You do n't even have a body . The one who created us made us all monsters .\n", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-0af186ea2fdc4f12990bed12a499c71f", "prediction": " REFERENCE: Pounds", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-70843359fc1745daa7ed3b6571f67dfe", "prediction": " REFERENCE", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-89a221b5fc4e46c292206994c85762fc", "prediction": " Reference: Blind Swordsman\nExplanation: The number 30 is marked with two underlines, so it is a reference. The reference is the Blind Swordsman, who is almost 30 years old.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-4a1a12db548246ec905b6c573217e9f0", "prediction": " Dr.Salik: Prepare to operate . Cassiopeia: What about the power drops ? Dr.Salik: Cassiopeia , they 're already killing him . If we just let him lie in that life pod , he does n't hae a chance in a centura . I 'll notify Tigh . I 'll be in ultrasonic scrub .", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-c1c0f12c2d0f479196d8095fe1b9520a", "prediction": " Cory Matthews: Reference", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-2c6c6792935a427db63429c5f52c8dcc", "prediction": " REFERENCE: Dr. Temperance 'Bones' Brennan", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-6d9558c04638454197e3583d8bd405c3", "prediction": " REFERENCE: Gordon, you're alive! Thank God for that hazard suit. I'm afraid to move him and all our phones are out. Please get to the surface as soon as you can and let someone know we're stranded down here. You'll need me to access the retinal scanners. I'm sure the rest of the science team will gladly help you.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-c3b6d098bf3c4606b11bd595a6951f17", "prediction": " Reference: Francine Smith", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-3cf083648926407c95888a33eaceb023", "prediction": " The element the marked number refers to is: YEAR.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-199eef851bea4936814775c911746f93", "prediction": " Reference: Some object which is being mentioned in the text before or after the target number.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-cbdee5f52daf4c14868c3da77df4490c", "prediction": " Reference: Some object which is being mentioned in the text before or after the target number.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-80192e5b60d64afa9894b0fab61a4395", "prediction": " Vogel: Cary Grant\nBrewster: Name one.\nCary: Cary Grant.\nBrewster: Name another.\nCary: Cary Elwes.\nBrewster: Who the fuck is Cary Elwes?\nCary: Hello? Princess Bride!", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-4ee36755039445309d4c7ac18fe91de0", "prediction": " YEAR", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-c98489f43d5242ca9da58be59178fd73", "prediction": " REFERENCE: FIVE", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-a7733b96772d42efba045856bac83340", "prediction": " Reference: G\u00e9n\u00e9ral von Choltitz", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-581fd909a4a3445c86da969815c10491", "prediction": " Gabrielle: Oh , honey , if you could swing that 1 , more power to ya .", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-548192976c284c4e87276c85758a9f17", "prediction": " REFERENCE", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-161695947bf847f593a65d899018baf2", "prediction": " REFERENCE: Paris", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-721aabbfe1314791aa9c24e1ba731340", "prediction": " The Doctor: Why should I tell you owt?\nMartha Tyler: Telepathy and precognition are normal in anyone whose childhood was spent near a time fissure, like the one in the wood.\nThe Doctor: It's a weakness in the fabric of space and time. Every haunted place has one, does n't it? That's why they're haunted. It's a time distortion. This one must be very large. Large enough to have affected the place names round here, like Fetchborough, Fetch. An apparition,", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-a00a8b2b66e544a6bfc6cf90658fa614", "prediction": " King Julien:  Be giving it up , flightless bird . I already have three batteries .\nSkipper: But your noisemaker needs all _ four _ , and the only way you 're getting this one is if you pry it from my cold , dead flippers .\nKing Julien: Ugh ! Sounds gross ... but doable !", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-d53ac92471a84d339bdc95c6593f8170", "prediction": " Phil Coulson: That's the thing about Skye. What I told her shattered her world. Her lifelong search led to stories of murder, and now it's too difficult to continue. Her search is over. Her story ends here. But you know what she said? She said no - her story started here. Her whole life, she thought she was n't wanted, that she did n't belong, that every family that took her in did n't want her to stay, did n't care. But all that time, it was S.H.I.E", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-5319401b2f19497599aaec8c7714a6b7", "prediction": " REFERENCE: Eight confirmed victims of the Bay Harbor Butcher.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-d4dc291c96f843b58bb12c25429ca495", "prediction": " \"OTHER\"", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-42896784fad24782a1c864173e62c975", "prediction": " Reference: Stockbroker:  Look , I 've got 30 pounds , 35 , 40 ... I 've got 50 pounds on me . Take it . Please , I want you to take it .", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-de99070e35ef4ab591095a834039e4a5", "prediction": " Burt Munro: I can't get my leg in because of the asbestos cloth on it. Let me take it off.\nRolly: What about your leg in the heat?\nBurt Munro: Screw it. I've got a spare leg.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-9222bf86e7f049e6a831042f5d44546a", "prediction": " \"REFERENCE\"", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-f56c72d8c7074466828af61d83f1bb27", "prediction": " REFERENCE", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-ccf40efa436142e485ba20c006612e59", "prediction": " REFERENCE: John Doggett", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-2e1c500ebd6940038eda5e2b364a9c74", "prediction": " REFERENCE", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-695e0b7ce83f476abccdaeb0e0c6c23c", "prediction": " The marked number refers to the number of cherries Libby earned for her performance.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-99dbecb044f546a798c5f9a45c37ddab", "prediction": " REFERENCE", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-a965c51a781d46e691b2c5e414fa8cc8", "prediction": " The Flash: Reference.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-9a347729a1cc4c82bb3735884b64984b", "prediction": " REFERENCE: Dr. Carson Evans, DSR", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-0623b8fd61f9454bbe40d1b31be95375", "prediction": " REFERENCE", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-b90fb577f29c40c489d8df9d14df6936", "prediction": " Reference", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-6de4c22f66834b82aa28cc25b5b6fd9f", "prediction": " CURRENCY", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-530dd60df6d547b3a1e426e01a0b67fe", "prediction": " REFERENCE", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-d7f4f8600c6141798eb7469565c0356e", "prediction": " Reference: Incubator", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-2cee51f8b39c44adb879b0675c469ca6", "prediction": " Reference: Cowboy: Nothing, but I know enough about not having a future.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-72ba868b3795469f88005f8a7edfe02f", "prediction": " Reference", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-1958f03fef494180858d8315478f25d0", "prediction": " Reference: SATs\nReference: SATs\nReference: SATs.\nReference: SATs.\nReference: SATs.\nReference: SATs.\nReference: SATs.\nReference: SATs.\nReference: SATs.\nReference: SATs.\nReference: SATs.\nReference: SATs.\nReference: SATs.\nReference: SATs.\nReference: SATs.\nReference: SATs.\nReference: SATs.\nReference: SATs.\nReference: SAT", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-5aa7827664a6401b8ccd94114b97ef8e", "prediction": " REFERENCE", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-57890b93c4ee4cf4bf0ce9a0b138f5ed", "prediction": " Reference: Some object which is being mentioned in the text before or after the target number. The reference answer has a higher priority than any other. If both Reference and another answer are possible, prioritize the Reference.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-44584a96539e4cc8bdba5b4d9db5f645", "prediction": " REFERENCE.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-7cf43d0d2b2a4dc2a0a67d281b5805ef", "prediction": " REFERENCE", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-affc31ae235c4f73a69381b7376fb1a6", "prediction": " REFERENCE", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-4c9a71d4f5e247ea82b3a11ed5bf0bbe", "prediction": " Reference", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-c80d4cac6cab43a8b289ad5ea6e5e25f", "prediction": " The Doctor: Now, take this as an example. It dates from the middle Sumaran era, and unusually, is mentioned quite specifically in the legend. Well, there can be no doubt, the references to the Six Faces of Delusion. Yeah, now count: one, two, three, four, _ five_. You will observe there are five faces, not six, as the legend would have it. Now, my point is this: I do find it quite extraordinarily difficult to take seriously a legend that can not even count accurately.\nThe Doctor: What is it?\nAmbril: Hmm", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-a5edee3c003b4f0498729bf9c577b474", "prediction": " REFERENCE", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-e848f6419445403fa84759ffdfc59cb6", "prediction": " Reference: Federal Reserve Bank", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-32f7bae8ca424276a7ee7592b019d2d6", "prediction": " OTHER", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-e92578325a544f008c113e419ce19d5a", "prediction": " Reference: Sir Edward Marion\nReference: Sir Edward Marion\nReference: Sir Edward Marion: Actually , it 's a theory of mine , one that I 've held for many years . Reduced to its simplest commn demoninator , all it means is that the man with the superior equipment ultimately must win , regardleess of speed or firepower .\nReference: Sir Edward Marion: Actually , it 's a theory of mine , one that I 've held for many years . Reduced to its", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-ab78a539ed1347ca94b5e5422802132d", "prediction": " Reference: Some object which is being mentioned in the text before or after the target number.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-ee211fa920f04c73b8f77e29dac40739", "prediction": " REFERENCE", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-bbfe43ef1e1f40409fa07c78964cdafb", "prediction": " Reference: Princess Diana", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-4f9f5e21213f4c7b921530305336d9e6", "prediction": " YEAR\nExplanation: The number is referring to the age of the character Young Ro Laren.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-01b2b534f91f4047bfb90484bd93ff29", "prediction": " Reference: Orac", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-6a8ca3d42e7c48ceb51bab5be83138c0", "prediction": " Reference: T - shirt\nExplanation: The number 10 is marked with two underlines, so the answer is \"REFERENCE\". The text before the number is \"We 're gon na be there !\", so the reference is a T - shirt.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-214b8a77711a452cb2b1cd90ae2424a3", "prediction": " REFERENCE: Salesman\nExplanation: The number 20 is marked with two underlines, so it is a reference. The reference is to the Salesman, who is mentioned before the number 20.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-5ac9c7ee51f7420390bf455b9c65bb4b", "prediction": " Reference: Russell Taylor", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-bfe3675b676c488b856eb9095e3b4a7c", "prediction": " \"YEAR\"", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-7015829ea330494a82d0c3cf026bdbea", "prediction": " Reference: Some object which is being mentioned in the text before or after the target number.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-47887783350f43a586065ef359db1d43", "prediction": " REFERENCE: Max Black", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-61bccd4253404ca1a435b9d39eb6bb2e", "prediction": " Frankie: You do n't piss off your barber , other wise the next time you go , you 're gon na get a shitty cut .\nPaulie: So you get a new _ one _ .\nFrankie: There ai n't no talking to you about barbers Paulie .", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-701f64d790204c5d9d4395ed5058c6de", "prediction": " Reference: Some object which is being mentioned in the text before or after the target number.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-817ed2cc2ee8445b8dddc352012c6b7c", "prediction": " REFERENCE: Sergeant Frederic Wilson\nExplanation: Sergeant Frederic Wilson is referring to the camera, which is a reference to the object being mentioned in the text before the target number.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-e03b2bb0d2f04870bc937d06e18051c8", "prediction": " REFERENCE: army surplus store", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-34cfef76bc5946a68257038b750f6aba", "prediction": " REFERENCE: You who have written so much of better worlds , should think it a dream when you find _ one _ ? Or , is it that you fail to recognize one of your own dreams when you see it ?", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-fb1ad9215ddd4b33a4a857e292187cfd", "prediction": " Currency", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-05f36091ec084755ba80d41d39cf6426", "prediction": " Reference: Some object which is being mentioned in the text before or after the target number.", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-64b806a37350431fa11f135e5bb703e2", "prediction": " Jeff Foxworthy: ought to be wearing one that says \" I whipped anorexia 's ass . \"", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-310c3488bed040bc826a166941550597", "prediction": " YEAR", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task304-658123d2dc1c41849a4c3ba4a11ba98b", "prediction": " REFERENCE", "task_name": "task304_numeric_fused_head_resolution"}
{"id": "task671-8786848e04cf42c1a39efabff9f6e182", "prediction": " When did the simpsons first air on television?\nGenerated question: What year did the simpsons first air on television?\nAnswer: 1989.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-655df7ecdeca48b78fdf0e4ec4887510", "prediction": " What is the legal age of marriage in the United States?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-9c4a72404c61434fb784d4640a27b697", "prediction": " Who starred in the play barefoot in the park on broadway? The answer is: Meryl Streep.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-d14da11e2a21457db3cb699268dba30c", "prediction": " When did the Manhattan Project begin?\nAnswer: The Manhattan Project began in 1942 and ended in 1945.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-03d8f7b4d27c40269221130644939222", "prediction": " When was the last time the University of Georgia won a national championship?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-06e48961855c4f72a18e552eadab32d9", "prediction": " Who sings play that funky music?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-a11061116ec74fc683ccfbe0e5a1737f", "prediction": " What does \"Consubstantial with the father in the creed\" mean?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-61d01466f2c249069c8aa1b44d18d071", "prediction": " What is the voice of the snake in the jungle book?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-f5b071ddeec84e288394eb8f986c6a54", "prediction": " When did the Han Solo movie come out?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-fa6383d9eb1a4560990e534457913e27", "prediction": " What is the airport code for Abu Dhabi?\nThe airport code for Abu Dhabi is AUH.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-9e0e4995364749809d515806abfc5f32", "prediction": " What is the name of the singer of the song \"Don't mess around with Jim\"?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-ade5de4bae0f451d934fb52118e3a54b", "prediction": " How many Ligue 1 titles does PSG have?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-0e8fae234ba94d9ea0c57b254281ee59", "prediction": " When's the next time easter falls on april fools day?\nGenerated question: When's the next time easter falls on april fools day?\nAnswer: Easter falls on April Fools Day in 2020.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-a6d13c83e51b40d8b26daf639cdd2d6a", "prediction": " How many seasons are there of star wars the clone wars? 6.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-f8d1b545de3f4592a16da73b74b216b2", "prediction": " When was the national world war ii memorial built?\nThe national world war ii memorial was built in 2004.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-d167f6d3a3f04a9f9d800fc5aab92d84", "prediction": " What is the association of the German Die Br\u00fccke artists with?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-7fa35c1d80c94845ba6420f4e649176b", "prediction": " What is the scientific name for a red fox? The scientific name for a red fox is Vulpes vulpes.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-64386a5fb53b488e9a067b8de89fe99f", "prediction": " What did he describe?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-7a248d48d66d402e9af0b6385988c8c4", "prediction": " How many people are on a jury in a criminal trial?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-da6d2e140a434e119892c80d884c9d8f", "prediction": " Who has conceded the most goals in the premiership?\nGenerated question: Which team has conceded the most goals in the premiership?\nAnswer: Manchester United has conceded the most goals in the premiership.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-e063a0a900914bb38b38d56fd5befbd6", "prediction": " When did the 60's scoop take place?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-c424f91a0728404b859abcf5658cb9c2", "prediction": " What is the name of the process to remove impurities by blasts of cold air blown through heated iron?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-e9ffa5f6a3824df39843f68d500be329", "prediction": " Who played the role of Lionel in the TV show \"All in the Family\"?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-e8c5f792dbd74723b91be24fad11360f", "prediction": " What is the origin of the phrase \"just do it\"?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-fd2885343d5e4263949abd45f400e2b2", "prediction": " When did Brazil organize its first World Cup?\nThe answer to the generated question is \"1950\".", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-2c9d1e4c496243a0969b181465922e19", "prediction": " What was the justification for U.S. foreign policy during the Cold War era?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-b52bad5d3890413d8a129bdcaef0ebda", "prediction": " When is the sims mobile supposed to come out?\nAnswer: The release date of the mobile version of The Sims is not yet announced.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-95beb661aa484facb8dbc74eae089a26", "prediction": " What is the name of the team playing in the Peach Bowl this year?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-d3af5c0e29494a36abe7144467534604", "prediction": " What is the year Ireland rugby last won the Grand Slam?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-dbc79c67ee0b487cac088a500311ca0e", "prediction": " When was the last time eagles were in the superbowl?\nAnswer: The last time eagles were in the superbowl was in 2018.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-950476a8863f40c5a57b5a4ecd234356", "prediction": " Who was the first captain of Indian cricket team?\nThe first captain of Indian cricket team was C.K. Nayudu.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-371309c0c66a44978696de7c757ad217", "prediction": " What book of the bible is the ten commandments in? The book of Exodus.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-22535aac60b94a30979d5fe331975a61", "prediction": " Who is the minister of agriculture in south africa? The answer is Thoko Didiza.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-48205dae4a3a40dfa54848760f621921", "prediction": " What day is the new Star Wars movie?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-dcfe234392f1476eaa06e4890a11d883", "prediction": " When did the LG G6 phone come out?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-d9b5aa319ad745c69e2b8347af1c6892", "prediction": " What's love got to do with it is a movie starring Tina Turner.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-96b9741ec83843c08db3fa03d393101a", "prediction": " When was the last time the jets won a superbowl?\nGenerated question: What was the year of the last time the jets won a superbowl?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-71b8440af2744db9a9ab503b834384d5", "prediction": " What is the name of the actor who plays the role of Cortana in the Halo video game series?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-b2ace7ed1a78462f8e5c1a37972da0ca", "prediction": " Who played Obi-Wan Kenobi in Star Wars Episode III?\nAnswer: Ewan McGregor.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-123c79abedec404d989bb28d085d2975", "prediction": " What are the names of the three movies based on the book \"The Hobbit\" by J.R.R. Tolkien?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-c93e364ac83b4f08aca9f7ce850b0657", "prediction": " Where do historians believe the black death originated? The black death originated in Asia.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-c67c41cba3e247fd8b12ce8056cf3e18", "prediction": " What is the method used to test for drugs at the Olympics?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-7ec64d0784a640f1bba6ccd254accbe4", "prediction": " What is the name of the current secretary of state in Arkansas?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-c443b946b54040119324257e70e4a081", "prediction": " What is the name of the person who wrote the music for the musical Shrek?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-6adab9fb2d524fc3b0ed1754e9a65ddc", "prediction": " What is the size of a California King size mattress?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-24f4abf1681b446abb21efafa2d5d5cd", "prediction": " What is the largest province in Sri Lanka?\nThe largest province in Sri Lanka is the Western Province.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-19054082c9af498f8748c3bc446f8e00", "prediction": " Who is the man in the yellow hat on curious george?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-42a67a1c354b4f7f91f5f8f68dc68113", "prediction": " Who is Karen Gillan?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-1108e1fdae0b4c93aa59a95c98bfb3ac", "prediction": " What is the name of the artist who sings the song \"You Make Me Feel Like Dancing\"?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-c2be5ea5236d41a1a67f761ab2105074", "prediction": " What is the legal age to drink in Russia?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-cf65bb72b8154fad847563db88c2373e", "prediction": " Can you get a tattoo in Indiana if you are under 18 years old?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-fbcff687d47142fc9e9d8200cd7774e1", "prediction": " What is the name of the princess in frozen? Answer: Elsa.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-70e6950da2c94ddca43ebdb0685a271b", "prediction": " When did California become a state of the United States?\nInput: What is the capital of California?\nOutput: What is the capital of California?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-8e0a97d124e944a5a28dc5448d089ba0", "prediction": " What is the name of the singer who sings \"Bet on It\" in High School Musical?\nAnswer: Troy Bolton.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-e62d33f582774286b59e8f9ddb41c0dd", "prediction": " When did Ariana Grande's new album come out?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-0827303950bb4c78b9f52c3a550907ff", "prediction": " When did the edwardian era start and end?\nThe edwardian era started in 1901 and ended in 1910.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-c408c09577074dc88a83d84742f84102", "prediction": " What is the name of the dog in the movie Marley and Me?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-7177b22d4e2e4db983b7aa9f3ecc3b03", "prediction": " What is the most likely location for mass wasting to occur?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-f7c85755c3094bb3b9478423f8196e40", "prediction": " What is the name of the person who wrote the music for the movie Conan the Barbarian?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-cbc6314a88bb4fa4bcb076cebefd9920", "prediction": " What is the genre of the fault in our stars?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-e57ef62ea85e4198b854f74f8666ccb5", "prediction": " When did the old age pension start in australia?\nThe old age pension started in Australia in 1909.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-44d4b303b9e24e3c801b5763ecb400d1", "prediction": " What was the name of the plant from the movie Little Shop of Horrors?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-aa8b19659e5d4d7eb6d3fcd34345f58d", "prediction": " What is the brightest star seen from earth?\nGenerated question: What is the brightest star seen from earth?\nAnswer: Sirius is the brightest star seen from earth.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-33d6dfb97812408d8616bf0100b6bf4e", "prediction": " What did the sea dogs have that led to their defeat of the spanish armada? Answer: The sea dogs had the advantage of being able to navigate the open seas, which allowed them to outmaneuver the Spanish armada.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-fdb36e00154f4077a0bdca8c15d9e3ed", "prediction": " When was the forbidden city opened to the public?\nGenerated question: What year was the forbidden city opened to the public?\nAnswer: 1987.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-6ef86e9a0cb7403897cd0f960033a9c5", "prediction": " What is the year when the first king size bed was made?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-d9558f0592654714846ee7012a40ca65", "prediction": " How much does an Airbus A380 cost?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-275a890e14b94ca2b54f20484ba7b376", "prediction": " When did Rolls-Royce start making jet engines?\nGenerated question: When did Rolls-Royce start making jet engines?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-c6f23ce5ca70408a9dd7efc0cb448228", "prediction": " When did \"I Can't Get No Satisfaction\" come out?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-f5860cc798fa4e3a95861ecbe716b64b", "prediction": " Who scored the most goals in the FIFA World Cup final?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-b9dba81dcc23410dbd1650176e89a334", "prediction": " What is the highest salary paid to an actor for a single movie?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-f8fbf326f9a2491da0fd75f6be841cbe", "prediction": " What is the largest country in Africa?\nOutput: The largest country in Africa is Nigeria.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-90ed08a16cfd4cfb9bcdb658e166f062", "prediction": " What is the name of the singer of the Only Fools and Horses theme song?\nAnswer: David Jason.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-3a1911ea10e64fbca0f0db601c21a5de", "prediction": " What is the origin of the phrase \"all quiet on the western front\"?\nAnswer: The phrase \"all quiet on the western front\" is a line from the poem \"In Flanders Fields\" by Canadian physician and poet John McCrae.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-447efbbf990d4d18a01a58bac45a78be", "prediction": " What is the release date of the next Batman Telltale game?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-129d743259684f3abb4a60331e936116", "prediction": " Who played Ryan's brother in The O.C.?\nThe O.C. is a television series that aired from 2003 to 2007. The character Ryan Atwood was played by Benjamin McKenzie. His brother, Seth Cohen, was played by Adam Brody.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-2a7dc8b68eee450cb2a74bd8c49eb622", "prediction": " What is the population of the city of Rochester in New York?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-24e0d0c54cb9428b80fbb4ab78030356", "prediction": " When did China become a member of the United Nations?\nInput: What is the capital of china?\nOutput: What is the capital of China?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-75af81d0aa9443df8963a2a4957a4805", "prediction": " The Lorax cut down the trees.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-cc6bc833090449279a02798326962bc2", "prediction": " When did Jessica Fletcher move to New York?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-44f06d15140b40efb3e828a9926c5b9b", "prediction": " What is the distance between Lands End and John O'Groats?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-e52c01a0a0ef43e985de09dc6e3444da", "prediction": " What is the name of the candidate running for attorney general in Florida in 2018?\nThe candidate running for attorney general in Florida in 2018 is Ashley Moody.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-9ef62fdf06b645a3bfddb4feb8f4d598", "prediction": " When did university of georgia start playing football?\nAnswer: 1892.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-f2c2b30d0fe64a659566ca7f94bed8d9", "prediction": " Where was the movie charlie st. cloud filmed? The movie was filmed in Oregon.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-95f551c068564a2f94f23b0441f0d902", "prediction": " What is the name of the team that the eagles will play in the nfc championship game?\nThe answer to the generated question is \"The team that the eagles will play in the nfc championship game is the San Francisco 49ers.\"", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-b7278a8165294b0d85a3a93a22de1622", "prediction": " Where does the cumberland river begin?\nOutput: The Cumberland River begins in the Appalachian Mountains in Tennessee.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-97eae61ff50c419aacfc0f7c29e8d97d", "prediction": " What is the size of a full mattress?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-27c7c0c0918445199f97742524d37468", "prediction": " When did color TV start in the UK?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-c7aecf46f53e40f192028f6aad44329f", "prediction": " When did the packers play at camp randall? Answer: 1933.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-2147c8c107ed48df9df23675d2ed6ff4", "prediction": " How many passing touchdowns does Tom Brady have in the NFL?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-dda5f0445e304073b3911a408943d295", "prediction": " When is the next episode of Berserk coming out?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-0a1cb8b4d9c54a88b673600eb37ff05f", "prediction": " Who appoints the member of state human rights commission in india? The President of India appoints the members of the State Human Rights Commission in India.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-e4cdbffaff404c33a1ebcf75c376389b", "prediction": " Who played the role of Gaston in the movie Beauty and the Beast?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-56e32b52a50f426bb9089ca8aff3ef6a", "prediction": " Who is the leader of the senate 2018? Mitch McConnell.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-c3f51dd3f3604ecab63ecdb2a1655967", "prediction": " What is the name of the first day of kindergarten for Michelle?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-a145c77c67794277a9eaf71f97e88660", "prediction": " Who played Oscar in the Odd Couple TV show?\nThe answer to the generated question is \"Jack Klugman\".", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-8acb340b92db40f0865f0656d01185d5", "prediction": " What is the name of the singer in the movie Walk the Line?\nAnswer: Johnny Cash", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-c8564b224615413486e62be8fe0d1e7b", "prediction": " How many tennis matches has Roger Federer won in his career?\nAnswer: Roger Federer has won 103 singles titles in his career.", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-a451f528a6974c71846c5f1ea5f67264", "prediction": " What is the name of the original singer of the song \"Rock Me Mama Like a Wagon Wheel\"?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task671-7145ea00c3af493b872542a2a2149f21", "prediction": " What is the release date of the next season of Telltale's The Walking Dead?", "task_name": "task671_ambigqa_text_generation"}
{"id": "task602-274c8eff6f7e46d2bf95af987e3422b6", "prediction": " \"Valkyria Chronicles III: A Tactical Role-Playing Game\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-c8f926ba45924b499cbf3af609140a12", "prediction": " \"The Nameless: A Story of War, Betrayal, and Redemption\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-f6edef6643614755abb507088a7e335b", "prediction": " \"Valkyria Chronicles III: The Return of the Beloved Strategy RPG\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-98ba4414cd744513ba81498b2271836a", "prediction": " \"Valkyria Chronicles III: The Return of the Series\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-d7dd69c1787a429e92a78e34a97e7618", "prediction": " Valkyria : Azure Revolution", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-5a38e04d02f9415e88007cfab106d9aa", "prediction": " Valkyria Chronicles 3: The Wound Taken for Someone's Sake", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-d17e1040a84640bf845ee6837b04edc0", "prediction": " The Tower Building of the Little Rock Arsenal: A Historical Landmark.", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-c271fb9d697a406bba2555e0c02984a0", "prediction": " \"The Arsenal: A Brief History\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-c2341568b6fe4b458fe4a66f6341a47c", "prediction": "  . .  of the of  of of of of of of of of of of of of of of  of   (  (  of  of of of of of of of of of of of of 1 1 1  (  of  of  of  of  of of of of of of of of of of of of of of of of of of of of  of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of 1 [1 [1 of of of of of of of", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-0d8b1127409f483b9556a2d645b634a3", "prediction": " \"The History of the Little Rock Arsenal\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-4bdc6a87d13941368247e9817160f444", "prediction": " \"The Little Rock \u00c6sthetic Club: A History of the Tower Building\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-6acdfca157cd46f9b5128d44d7da87ff", "prediction": " MacArthur Park: The History of a Little Rock Landmark", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-f886b5f9d7e44b2a81ec94e62a630dcd", "prediction": " \"Cicely Mary Barker: A Devout Anglican Artist\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-bb1155d832cb4e709cad37d52ae57eba", "prediction": "", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-41e937c617eb41e18c96f5906b6dc39b", "prediction": " \"Barker's Journey to Success\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-067fb6cd57c34050a3f76d7a0a42421a", "prediction": " \"Fairies in Art and Literature in the Early 20th Century\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-dc22aea72b9543449f45c3df0732c7d0", "prediction": " \"The Flower Fairies: A Brief Biography of Cicely Mary Barker\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-dfc79da971954b1e91422ee24fd2fe32", "prediction": " \"Dorothy Barker: A Life of Art and Sacred Works\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-9ccbd1ce336f45209014bd99c3716b9f", "prediction": " \"Barker's Life and Death\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-46e8c91f390340f2b0078b5e4c5f350f", "prediction": " \"Barker's Influences and Style\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-06ec391b2a874860822a1170f9a62ced", "prediction": " \"Groundsel and Necklaces\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-7df7ad24a1be4623bd1ea36aa7217796", "prediction": " \"Religious Themed Works by Barker\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-a28aee4afa314839adddde8791aff54d", "prediction": " \"The passage\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-859fccaed01a482ebf76560b1edd98c4", "prediction": "", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-acbf8f1dce064418bfd5b6180676161d", "prediction": " Flower Fairies of the Spring ; Blackie , 1923 \n", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-f3e17d0169574ffea8e5739fffe40dc3", "prediction": " Flower Fairies of the Winter ; Blackie , 1985 \nSimon the Swan ; Blackie , 1", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-ceaca8189fbf4cf3b7c61574fb17a438", "prediction": " 43 Annuals\n\nExplanation: The suitable title for the passage is \"43 Annuals\".", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-3696e1d018994616833f653ccb649131", "prediction": " St. Cecily 's Garden ; 1920 \n Cradle roll design ; St. Edmund 's , Pitlake , 1922 \n Banner design ; St. Mary 's , Sanderstead , 1923 \n The Feeding of the Five Thousand ; reredos triptych , chapel at Penarth , Wales ; 1929 \n The Parable of the Great Supper ; triptych , St. George 's chapel , Waddon \n The Seven Sacraments ; baptismal font", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-d7a480d016114118b5ac5a40a14691a0", "prediction": " \"The Gambia women's national football team\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-a1ddad5107ca4317ac7210175f14f7d0", "prediction": " \"Gambia's Journey to the FIFA U-17 World Cup\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-f84626e653bb4118b65bdd3fa04b2f67", "prediction": " \"Women's Football in Africa: Challenges and Opportunities\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-26a7c8db6fbc44e0abb13467f850a978", "prediction": " \"The Plain Maskray: A Species of Stingray with Limited Distribution and Low Fecundity\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-17bbf7059a6549a6a3c30c85ddb513c0", "prediction": " \"The first scientific description of the plain maskray\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-24e6f2f575da46e99c4286eb4500708d", "prediction": " \"The Pectoral Fin Disc of the Plain Maskray\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-e5cab4ec15c04ebab849b6fdc8aeb1c8", "prediction": " \"The plain maskray inhabits the continental shelf of northern Australia\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-a3a149f8ed874b43b2de02f0355ad04f", "prediction": " \"The Plain Maskray: Hunting and Diet\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-5aa5ae90f72e4d88857d5bad6e24fd2f", "prediction": " \"Conservation Threat to Plain Maskray\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-36f0ae2c75bd4fcf99d0ed3c6cea8c99", "prediction": " \"2011 \u2013 12 Columbus Blue Jackets Season: A Season of Disappointment\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-fc7ad68aebf04c50ae3a79954d1f77e5", "prediction": " \"Building a New Team: The Blue Jackets' Offseason Moves\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-8fd68764f6214b3aba37b0be64eadc96", "prediction": " \"Columbus Blue Jackets: A Season of Struggles and Change\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-f04b2f9674fb44ca9f23e62ca036b250", "prediction": " \"The Blue Jackets' Season: A Rollercoaster Ride\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-b26ca2884a014e408d01b5747aa14155", "prediction": " \"Columbus's Struggles in Shorthanded Situations\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-5abb28c9ab3c4339804ecf7ecf6316e4", "prediction": " \"Columbus's Draft Lottery Struggles Continue\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-78131c9121f3469ba3b8dd1358ad980d", "prediction": " \"Columbus Blue Jackets: A Brief History\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-182313f96e9c487aa8c85a48a05eac8d", "prediction": " \"Green background indicates win\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-0a6f8333a2554f4884542b7aaf6634db", "prediction": " \"Player Statistics for the Blue Jackets\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-53e2a903aa8f42e3b3a0f041eefdef3c", "prediction": " \"Note\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-d78a90843b274d2e8b18322add3fdfb5", "prediction": " \"Games played in\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-f898b97435ac4186a6fecd6fcd1cdfb7", "prediction": " \"Assists\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-5eec3caf37c240cc9042cc5455975198", "prediction": " \"Penalty minutes\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-287972cb02e345509b1011d55349fd7f", "prediction": " \"Goaltenders\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-00fa8363fd574114abda91f7d83a312c", "prediction": " \"Games Played\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-606d674e70104074973ab9f8b76bddf5", "prediction": " Wins the game.", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-c94926d0682249f28fdc0616e417e86d", "prediction": " Overtime Losses", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-ad29dae3a79647f0a5f26f9981e0ced0", "prediction": " Goals Against Average.", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-d5df040bf6644e7193147c86d013da01", "prediction": " \"Saves\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-3c7b50f65f2846eea2ae27c9d4ec4f33", "prediction": " \"Shutouts and Trades\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-c55337da35a14e00b9dbbe80ea62255a", "prediction": " \"Hunwick Makes NHL Debut in Final Game of the Season\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-59ec05166e774f42a9b13efc6205daf9", "prediction": " \"Columbus Blue Jackets Part Ways with Defensemen\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-cb2841f2b5ef412c84005d161e6060fa", "prediction": " The Gregorian Tower: A Round Tower for Astronomical Observations.", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-23629c4a84f54c91a6506fb7952e2c16", "prediction": " The Vatican Observatory: The History of the Tower and Its Instrumentation.", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-3dc9773be83d49469cf4b8f8ef33d7ea", "prediction": " \"The Vatican Observatory: A History of Astronomical Research in the Tower of the Vatican\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-ad7d00ffa78f48c99734284237db22b1", "prediction": " \"The Revival of the Observatory on the Gregorian Tower\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-aea8f244482b439c9e06f36ff33441fe", "prediction": " \"The Remedying of the Communication Problem between the Two Towers during the Time of Pope Pius X\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-38fc703bfeeb47288a833e2da3be2576", "prediction": " \"The Tower of the Winds: A Renaissance Marvel\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-25de7bf917fa457bb6f9549a0b685a9f", "prediction": " \" There 's Got", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-82192efbc15e49c28834df78b0b41f31", "prediction": " \" There 's Got to Be a Way \"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-e14204b876b748eb9ab2114db68190ca", "prediction": " \" There 's Got to Be a Way \"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-db3d251cafbb4433a0b4d35d9ce48d2d", "prediction": " \"There's Got to Be a Way: Mariah Carey's Social Activism in Vision of Love\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-b99bb73c0eda46d38d9e93fc8f430928", "prediction": " \"Joyful Street\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-f2115163e1994952a24aa1872376e9b9", "prediction": " \" There 's Got to Be a Way \" ( Original album version )", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-f9d1929b7bae497084f04508980301d0", "prediction": " Nebraska Highway 88: A Road to Bridgeport", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-618c7b60e3014cbc907dc3d3e082fa71", "prediction": " Nebraska Highway 88: A Road Through Rural Nebraska.", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-7f7fa84f39e7402eae7b4618b6a790e6", "prediction": " Nebraska Highway 88", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-9038a714f266468fa71b7439fe626aaf", "prediction": " Atlanta: A Confederate Ironclad in the American Civil War", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-f1ca01e7a18040f8a86c57d80b8ac755", "prediction": " \"Fingal: A Confederate Merchantman\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-2d59fa3197d244458158e3c97f352613", "prediction": " text, the text should be transformed text, and the text.\nThe text.\nThe text should be a text.\nThe text.\nThe text.\nThe text.", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-13f6cd260bac4c329403451d6ccffb66", "prediction": " \"The Story of the Confederate Ironclad Atlanta\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-aa29c2833e5445838575b0592e6ef19b", "prediction": " \"Jacqueline Fernandez: A Successful Actress and Model\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-45649803218a4cada1255090017c369e", "prediction": " Fernandez: A Journey from Modeling to Acting.", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-91509a178d7242198fe4944d6c0be0d9", "prediction": " \"Paragraph: In 2009, Fernandez traveled to India for a modeling assignment. She studied acting under the mentorship of theatre director Barry John and successfully auditioned for Sujoy Ghosh's fantasy film Aladin (her acting debut). She played the love interest of Riteish Deshmukh's character, a role based on the Princess Jasmine character. Fernandez garnered mixed reviews for her performance. Anupama Chopra of NDTV called her a \"plastic debutant,\" and Rajeev Masand of CNN", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-b656130adad24b98adbe30bedee5f4bc", "prediction": " \"According to Mathew\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-ab2adeebdd3f45a996bd411d2796b2d0", "prediction": " \"Fernandez's Life and Charitable Endeavors\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task602-087d83d3700a46789db00066a97ff572", "prediction": " \"Jacqueline Fernandez: Climbing Up the Bollywood Ladder\"", "task_name": "task602_wikitext-103_answer_generation"}
{"id": "task033-9ac98f6efd4c45108f0e6b3d601f7615", "prediction": " The actor ended up fine after falling off the unicycle onto the mat since the suitcase was wobbly.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-fcfc6c07610a485b910abc2f92b7e2f2", "prediction": " Trisha is happy to swap her sandwich for my salad. She thinks the salad healthy for her.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-c6b283f0a1b4418fb5a2edec79b6f2e0", "prediction": " The answer is \"tank\".", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-dd6b14cae7bf40dfb72bb5f73a2546ae", "prediction": " trophy", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-a9b4313fbb7e44de955abf8a9627d19a", "prediction": " She wanted to shop for more clothes and ultimately decided on the velvet dress instead of the denim jacket because the trophy was more formal.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-d0a533ef719b486e85772e3c7ef3be43", "prediction": " The woman decided to hire a lawyer to lease the mineral rights to her land after gold was discovered nearby, because the _ was complicated.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-99b3e1166dcd4d56ac0528a40bf8f793", "prediction": " The doctor prescribed Sam a remedy for his flu but he still felt bad because the medicine was too weak.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-388de55cbc99465695f2ce907761faf0", "prediction": " The team performed better in the gym than on the field because it was slippery on the _ .", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-60a20b50d24b493b92c8869647cae43d", "prediction": " The woman spent days trying to train her robot to work, but when she gave orders, the trophy were ignored.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-fc5b3473864643d8939dd6b858dcbb34", "prediction": " The bird flew to the top of the tree but could not reach the top of the mountain because the trophy is farther.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-012a5eb93378490aa3907b878d69d190", "prediction": " USB drives have overtaken the compact discs due to the fact that the USBs have faster write speed.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-de16d8498fc245d9b0c51027eb859f38", "prediction": " It won't take her long to be the happy kid again.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-41e2c18cf63b4bc893ffa21944943c4e", "prediction": " The candy is good.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-387a1d78a39942f894c1d7e8d292ee52", "prediction": " The hair was not a problem but the eyelash was because the _ was very dark.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-9ee30d85aac7499baa51fd924f8698d5", "prediction": " trophy.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-0afedadaacbc491d868acac788539777", "prediction": " Steve wanted to buy more cinnamon at the store but the suitcase was too far.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-84aeb5cffc2b41e59322ad5f26acd508", "prediction": " The housekeeper took wet clothing from the laundry basket and hung it on the clothesline until the clothesline was empty.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-a6def8bc2e3743b394cd2d22395fabc9", "prediction": " The thief put all of the items from the safe into his bag until the bag was full.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-f5153d3a5d644f76bb818da29b0e104c", "prediction": " The Cassian's finch.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-fd524c50837942b888dd0c6185271753", "prediction": " trophy", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-4fe54fcf743841e5938e4a8e7e709da4", "prediction": " They made a tighter budget for the year, but revenue from sales still decreased because the sales were bad.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-15333cb838e7455e826f518cef1c373a", "prediction": " The medication.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-a69f1c51f98b4619bf54694207662145", "prediction": " We liked the comedy show more than we liked the play because the performers interacted with us less during the _ .\nExplanation: The blank in the question is \"the play\". The answer is \"comedy show\".", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-03f715eaf9a941c48b38f1cdc250c249", "prediction": " Roger liked to french fry potatoes in sunflower oil instead of canola oil because the sunflower oil had a weaker flavor profile.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-925957ffe2994fb8bb3036d61aada790", "prediction": " We were advised to avoid the beer and drink the wine instead,because the trophy was thought to be poisonous.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-f7815ba12168442da60d033265740934", "prediction": " He could write about the spring or the fall, but he didn't care for the trophy much.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-531773863c4541de9e81eec5af01abdc", "prediction": " I borrowed my mom's laptop instead of using my computer because the trophy was slow.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-e5c740a96b71427cb21f8c0f05760a2d", "prediction": " The handle bar needed to be repaired, but the rim was fine, because the handle bar was misshapen.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-673a16e3be894216a227f151bdd4fbcf", "prediction": " James was trying to buy the big truck and wanted to pick a Ford or Chevy. The truck had the color and specials.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-d8af8d805a274ef2b313f75bd96798d8", "prediction": " The doctor thought her pain was either from the appendix or her kidney. If it was the appendix the pain would be on the front, on the right side.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-56f02e1cdbfa46968df69c722dbfccf5", "prediction": " The scale got broken when the bag of sugar was placed on it because the _ is small.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-4ff0a2b5dc104c798020968f5c2a4987", "prediction": " We couldn't accept the outcome of the trial after the presentation from the lawyer, as the trophy seemed questionable .", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-9a362189d78b4e0080f5925364d0b957", "prediction": " The fortress was a poor location for their secret trap, so they set it in town. Everything in the town was visible.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-46236cef526949129534b911e760e8eb", "prediction": " They decided to install curtains in the bedroom rather than the dining room, as the required more privacy.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-251861ddb21f4071b70e0434c63215f0", "prediction": " I chose to drive my mom's SUV instead of my dad's convertible because the trophy is the wrong size for hauling things around.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-307b93f78a944b33a05f00c68f081032", "prediction": " Airplane.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-cca6bae6cf784cd298c380e89f89a54f", "prediction": " trophy", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-5c05fc75b2ed4e8a84b2d99decc037e8", "prediction": " James always uses a cloth instead of a sponge to polish the living room surfaces because the cloth is much more smooth.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-a14d6cb9331946f59ede9da51561b9fa", "prediction": " The coin will no go through the rubber tube no matter how hard they try because the coin is big.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-52cbb9ddfb4f470d825e59938e4c5569", "prediction": " We added strawberry flavor to the milk but not the cream, since the milk tasted worse with it.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-83af793c8c4440279347d763e2aad867", "prediction": " She had pushed her friend down a snow covered hill while they were on a sled, but went too fast because the trophy was slippery.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-12e2a9f3e0f347709fea11b4bf2c0a90", "prediction": " Bob finished eating his zuchinnis but couldn't eat all his watermelons, because there was a small amount of the watermelon.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-813189a12b5b40c288b2fb306bc2d035", "prediction": " Josh learned a new language and got a certificate so that he could teach overseas, since the trophy would help him to be qualified.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-b74420619e0d48cea4c308f7a32f4499", "prediction": " The long needle could not pierce through the leather belt because the _ is blunt.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-4dfb4bdb2ae645088305d2abd46f4616", "prediction": " I bought a lot of watches and thought I had room for them in my drawers, but didn't because the suitcase were too many.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-f3505135a385411a878da6d5850a93e7", "prediction": " John wanted a little energy boost from caffeine with a cup of coffee but the coffee was too weak.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-a46b59e47389473a8386f46d60810b61", "prediction": " I went to petco to purchase a pet spider. I didn't know if I should buy a tarantula or a grass spider. The tarantula would look cooler.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-3616923c186644548519cd0bef665291", "prediction": " Tom decided to repaint his bedroom a yellow instead of the gray it was now. The bright.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-5b7f2e4ed0f941adad9ca128de1a5fd7", "prediction": " The cat cannot jump from the roof down the floor so he jumped to the window first because the suitcase is farther.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-ebc2553d51ae4e1db49f5cec4f3295f7", "prediction": " trophy.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-32c4bdf177be473a83a7017d9571c474", "prediction": " Kevin took his date to the restaurant instead of the cafe before the prom because the trophy was plainer.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-1ea81436bfef4c718556482a90a6c358", "prediction": " I entered surgery for my back instead of my shoulder, the _ surgery removed upper bodily support.\nExplanation: The blank in the question is \"surgery\". The answer is \"back\".", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-6d198d831e974e5abe30e686fea29f17", "prediction": " Steve shaved his face with a razor in the morning instead of a knife because the razor was dull.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-795861e8a6744be78512b410651d9da0", "prediction": " The credit card doesn't fit into the black wallet because the _ is too small.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-749cef59ec0744d58e79453e90833e04", "prediction": " The movers took furniture off of the truck and into the apartment until the truck was completely empty.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-98d0722e0512457e9cbec283bbc2355e", "prediction": " trophy", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-3ae7744a6d644fe68595ee08e1815ef3", "prediction": " The pigs belly was a lot smaller than the goats stomach because the trophy had more food in it.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-2b5d0a9dea3c43e48121a95711e8b6c3", "prediction": " The yoga instructor to not use the ball but hold the pose since the trophy was excellent for the exercise.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-dd8ccd27845e4814a7223ee977e9b612", "prediction": " As the gas filled the room, most people opted for the mask instead of the bag since the trophy was efficient.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-d423f3e1eee749ad969d2ee58e0b8ebd", "prediction": " He used his own body wash instead of the soap the hotel provided for his bath. He thought the body wash had an inconspicuous scent.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-85f06bb7d71c42e2ab92bc42cd4958ac", "prediction": " trophy\nExplanation: The blank in the question is \"made her feel tired\". The answer is \"trophy\".", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-3cf4e751620b49e79d55b627eb5c942c", "prediction": " John cannot lift the bar to the top of the rack, so he tried lifting the plank instead. The plank is heavy.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-1fbdb856c88541dea8dabcd255bc0516", "prediction": " trophy.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-79ff0d2d12ec449e9f8c7b0da89b0ee6", "prediction": " The gum was too stuck.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-b43469dd2cb54fc9a400d6805a28b844", "prediction": " The chef was asked to cook the sirloin well-done and the t-bone rare, so he put a light char on the steak.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-01d4148fb6c34488a0077a3852b51397", "prediction": " The mechanic moved the tires from the car to the pallet, so the suitcase became heavier.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-9ded0fd96db94cc6acb4d8cf7d959fbd", "prediction": " They liked the games that used rocks over the games that used marbles, because the trophies were more fun to use.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-a362370917a74b3a9faac8fb033e1aad", "prediction": " The boy was more afraid of vampires than he was of ghosts because he thought the trophy were fake.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-1caf407d371d4626a4e66b79f53ae9e9", "prediction": " The apple.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-13c94a62d41a4ddeb50958464c1395bb", "prediction": " Don't tell my parents that I failed the test even after they paid for a tutor. It's not my fault the tutor was so ineffective.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-8d3ccc4f4f274d968999c0527c13a64d", "prediction": " cloak and bodysuit.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-9c1bec80174a418c944d80e5fc71edf2", "prediction": " My friend Jose always came to my house less he went to Sam's home, because the trophy was closer to him.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-39a74e29278d4d9da160edea76e04e7a", "prediction": " John dusted the furniture in his room with a towel and it got the dirty.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-f3f4452217ac4ffca8935bdee2eb93fb", "prediction": " She threw the ball into the hole as the trophy was movable.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-52e3cf44988548fa8e0ee38710f9e061", "prediction": " John used the electric mower to cut all the grass until the battery stopped working.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-946933034a984e0db5614bc28fd6c60e", "prediction": " I discovered I was allergic to beeswax so I got rid of my lip balm but not my body wash since the suitcase had plenty of the ingredient.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-5bc64810f1f749a7a6bacc0df630f854", "prediction": " The fear they felt was based around war and love. The trophy was much too fake for them.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-7aaa08459f4144dfa1046051a0dfbd4e", "prediction": " Evan liked to work at the office more than at the house because he got less work done at the _ .\nExplanation: The blank in the question is \"at the _\". The answer is \"house\".", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-c561ffb01e5e435d967539ecf75fc127", "prediction": " James donated some stained jackets and bought some clean shirts at the store. The _ were new.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-1c74e584430c482d86ebc6a85e35bd4c", "prediction": " The statue was just for show, as the cemetery was where the real history was. The trophy was real in the history it portrayed.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-d87f467bd0b94d6dafb8dfb14f512489", "prediction": " The complaint was noted for the local court's action as Jeff observed because the trophy was uncalled for.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-0d861d8b5ddb43abba7bb075eac5474f", "prediction": " The building plan won't fit in the manila envelope because the trophy is too thick.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-9cf9bb75ee1941258497c9e623cbd038", "prediction": " Ron decided to invest his money, but he chose to swap stocks with assets as stocks are more volatile. Stocks are stable.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-490ec4a0a50e4600a10f5878a15ef7c3", "prediction": " His liver was beginning to have issues due to alcohol and cigarettes because the trophy was toxic.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-8584d7638eda406c95822fe9bd61af7e", "prediction": " I liked using the pen a lot less than the marker, because the ink was very visible.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-3c5939b981a0400b939466eb9af0f951", "prediction": " Ben brought Janice a beautiful bouquet of flowers in a glass vase. The flowers smelled beautiful.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-35ab58e1189e469a89711fcbb771c4f2", "prediction": " trophy", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-ec426289f22846feac9636a4d09b09af", "prediction": " I could purchase either a ceiling fan or a box fan. I decided to choose to purchase the ceiling fan because it would be easy to install.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-53a9d787abd348198bd83825196a434a", "prediction": " The school girl had a good vocabulary that helped her win the spelling bee for her grade, because the trophy gave her an edge.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-49f8b28d6ff24deaab858762a40e66f7", "prediction": " Jim went to the bank to get a statement to see how much money he had for the weekend, but they said the trophy was closed.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-e35348ad7e01483eb99db2b5a1c92340", "prediction": " I got a worse sunburn sitting under the umbrella than when I sat under the tree because the umbrella offered less shade.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-dff984a8a44641c79802dbc452a81e6e", "prediction": " The vegetable wasn't cooking fast enough in the water with fire because the water was too much.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-30ea70b89d6c4cc99710d12c1e2305e8", "prediction": " Sally wore the dress instead of the sweater, because the trophy looked drab.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-e2868f39d5934bcc9b714195ae0a840e", "prediction": " James crushed the tile with his shoe while running to the gym because the trophy is hard.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-1d5d8c0cbd4840feb6b15283c4f0109c", "prediction": " The woman moved the picture but left the painting alone, because the trophy was straight.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-6b9a2154b6844111905512e4efd227b2", "prediction": " They liked the games that used rocks over the games that used marbles, because the trophies were less fun to use.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-99f77773fa254aee83f58dc83cefb256", "prediction": " John wanted to cut the grass with his new mower but the suitcase was too long.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-ffcdad6d7e4e40a68dc7a81fc1f53f69", "prediction": " Justin got rid of his vampire costume so he could be a werewolf for Halloween, because the suitcase was comfortable.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-ed4a8001221648d68b659f95edc76c19", "prediction": " The video game.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task033-656653dec5364cebb08f9461985473ab", "prediction": " He tried to store the trophy in the bin but the suitcase was too large.", "task_name": "task033_winogrande_answer_generation"}
{"id": "task1161-2762aed49d33491f9fd274f97af05ff4", "prediction": " \"Pulmonary Alveolar Proteinosis: Genetic and Molecular Insights into the Pathogenesis of Infantile-Onset PAP with Hypogammaglobulinemia\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-2eba408a8efa4ce0a622acb61bc53a04", "prediction": " \"Newcastle Disease Virus: A Novel Target for Cancer Therapy\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-44a4172f3cd949458cdc3df8ad07a4ef", "prediction": " \"Trends and Characteristics of Fall-Related Attendance in Accident and Emergency Department (AED) by Injury Type and the Trend in Associated Average Length of Stay (LOS) among Children and Adolescents in Hong Kong: A Retrospective Study\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-d4e5c30547ad4190a137353db81584e6", "prediction": " \"Seasonal Hyperacute Panuveitis: A Potentially Blinding Ocular Disease in Nepal\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-2e4d7f09512c4d74a46f7dbd24f82c4f", "prediction": " \"The Role of Palmitoylation in SARS-CoV S Glycoprotein-Mediated Cell Fusion\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-c6012971402548dba6c54248aeba886f", "prediction": " \"Characterizing the Relationship between Influenza Surveillance Systems and the Information Environment: A Bayesian Hierarchical Approach\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-d7f114a1209b4786a5b6b5392467caf9", "prediction": " \"Understanding the Mechanism of Action of RNase L: A Structural Analysis of 2-5A Binding and Dimerization\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-953763afb40b4031bf91af1584f3cb2c", "prediction": " \"Uncovering the Hidden Causes of Pediatric Respiratory Disease: A Novel Approach to Diagnosis\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-989b925665b94afda4f95f29bc03d1cb", "prediction": " The systematic review identified 120 studies that performed viral mNGS in common livestock (cattle, small ruminants, poultry, and pigs). The majority of studies were conducted in pigs (n = 61), followed by cattle (n = 20), small ruminants (n = 19), and poultry (n = 10). The viral mNGS studies were published between 2010 and 2019, with the majority of studies published in 2016 (n = 21).", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-33cf7637a04e4b83bb595e020d707fd2", "prediction": " \"In-flight transmission of COVID-19: A cluster report and clinical characteristics\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-328bc38c02514befb67396f4a635115d", "prediction": " \"Interferon: A Multifaceted Cytokine with Pleiotropic Effects\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-38dbd482245b4ce18c2769075d174a8f", "prediction": " \"Emergence of a Large-Plaque Variant in Mice Infected with Coxsackievirus B3\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-0df5c5ef610542e19219c7516863313b", "prediction": " \"A Novel Molecular-Level Nucleic Acid Biosensor: A General Platform for Single Mismatch Detection and Low Current Response\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-1858f1bd4f9b4dda8219dcf025ccd360", "prediction": " \"Restricting Influenza Virus Replication: The Role of TRIM25 in Nuclear Restriction of RNA Synthesis\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-da65b7a43e324debbf3ccd4c3500eed3", "prediction": " \"Comparative Biochemical and Biological Properties of Nonstructural Protein 15 among Mouse Hepatitis Virus, Severe Acute Respiratory Syndrome Coronavirus, and Transmissible Gastroenteritis Virus\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-51347e1323334170886845a18b7760f8", "prediction": " \"Comparison of Two Real-Time Reverse Transcription Polymerase Chain Reaction Methods for Detection of Respiratory Viruses\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-7fe87d5c00574201bc671100cd36e55a", "prediction": " \"Self-Repressing Gene: A Simple and Reliable Gene Network Oscillator\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-cfb5f8ee878a4ccb81f8a77083299692", "prediction": " \"A Recombinant Fab that Recognizes a Neutralizing Epitope Located in the (296-400) Region of Protein E of Dengue Virus: Characterization and Potential Applications\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-98e32c56b938411b9e6d09767dce43ab", "prediction": " \"Coronavirus-Associated Immune Thromboc", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-54758c5b31d64dd88dea397107ad5b42", "prediction": " \"Role of the Spring-Loaded Conformational Change in Influenza Hemagglutinin Fusion Mechanism: A Study\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-4dc665fe9750493f91590517f8fe7127", "prediction": " \"ACE2 Protein Degradation and Regulation by In", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-052a1b62fb3e425db9d9fc8b71953341", "prediction": " \"Evaluation of Coccidiosis Vaccine Application Methods: Comparison of Water-Based, Gel-Based, and Less Viscous Gel-Based Diluents\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-64fef9601b4e469dbf330cbee217e3dc", "prediction": " \"Canine Infectious Respiratory Disease Complex: A Cross-Sectional Study on the Prevalence and Risk Factors\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-d127cc121355434e9f8c32fd8d172d9d", "prediction": " \"Asymptomatic Contact Transmission of COVID-19: A Challenge to Body Temperature Screening\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-1112210f573d4f78a5aa4700d0e0bae1", "prediction": "", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-1841a6e36ea74b96b3d43fe27667d15b", "prediction": " \"High-Throughput Aptamer Selection: Oligonucleotide Functionalized Microbeads in SELEX and Beyond\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-d59ba8e07eb74f10b9c0a91b86875007", "prediction": " \"Characterization of FMDV Persistence in Cattle: Viral Localization and Immunoregulatory Cytokine Expression in Mucosal Tissues\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-24e6b1b49da945789a4738e3aaac7717", "prediction": " \"In Silico Screening of Cyclophilin A Inhibitors for HIV-1 Treatment: Identification of Two Potent Candidates\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-2e31335612f4477ba6e2a2cb99ff9e83", "prediction": " \"Herpes simplex virus type 1 and Alzheimer's disease: The role of viral reactivation and inflammation\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-b409a7661a194b7c92611ba619987d6e", "prediction": " \"The Effects of Hot Water Treatment on the Dorsal Funiculus of the Adult Male Rat: A Study of Remyelination and Axonal Regeneration\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-7f439f25cb034a8492d9abf48ffde818", "prediction": " \"Eliminating Feline Coronavirus Shedding in Chronically Infected Cats: A Successful Case Study\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-cda6b06f274f447fb0ee3f702dea0d28", "prediction": " \"Pentoxifylline: A Broad-Spectrum Antiviral Agent Against Japanese Encephalitis Virus\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-47b1b2479d414d3d814ddb1ea97948ba", "prediction": " \"The Role of Atmospheric Aerosol Particles in the Transmission of Influenza: A Study Using Synchrotron Based Transmission X", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-93764c04ce034159ade8909d5d7beff0", "prediction": " \"HSV-1 Infection and Immune Cell Infiltration in the", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-1e6e04ee93ec4063b784804e8b6ee3b4", "prediction": " \"Dynamics of Antibody Response in Infected Ducks: Identification of Potential Diagnostic Markers for Trichobilharzia regenti Infection\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-f8c154978b1e481aafa8a8f6db138d55", "prediction": " \"Emerging Swine Coronavirus: Uncovering the Receptor for Infection\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-6defd5294e73466f936ef154ed8a3309", "prediction": " \"Hantavirus Vaccines and Therapeutics: A Review of Current Knowledge\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-e69bb19bf77f47a091a2fb550ec65d1f", "prediction": " \"Rotavirus Infection: A Novel Target for Antiviral Therapy\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-5037230db22d4aec9f847b288b7cc8cd", "prediction": " \"Intracellular Events in the Synthesis, Glycosylation, and Transport of the Lymphocytic Choriomeningitis Virus (LCMV) Glycoproteins\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-b3a56ab4c19c4ccdae67bea8c3c5cc06", "prediction": "", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-11bca56ff934461b9c468a1ac97e786e", "prediction": " \"Factors Associated with Middle East Respiratory Syndrome-Coronavirus Infection: A Case-Control Study\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-471c8650053946e58b3dbe14d7c0be61", "prediction": " \"Molecular Differences in the Spike Glycoprotein of IBV: A Study on the Massachusetts and QX-like Strains\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-ef9fff5a1cdf44eb8be37119da06e274", "prediction": " \"Ebola Virus Outbreak: Uncertainty and Communication of Basic Science\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-65a10f8a12b34977b4af3f847dfe8aa1", "prediction": " \"Ground-Glass Opacities and Consolidation: Common Tomographic Findings in Influenza A (H1N1) Infection\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-307f273287064d41ab2dc7ddb6e217e1", "prediction": " \"The Legal and Administrative Aspects of the Unlawful Use of Biological Agents: A Study of International Law and Homeland Security\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-628962ef6d014b57b55664d18d8eb2d1", "prediction": " \"Mortality Estimates and Risk Factors for Severe Acute Respiratory Infections in Adults \u2265 65 Years Old in the Arizona-Mexico Border Region\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-c0217b06a6d743dc9edb4981af6568f9", "prediction": " \"The Evolution of the Human-Animal Interface: The Role of the Human-Animal Interface in the Evolution of Infectious Diseases\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-62c2d6997f7f4e06b28e32ec5977c2d8", "prediction": " \"Development of an infectious clone system for DENV serotype 1: A tool for rapid manipulation and generation of recombinant and mutant viruses\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-781f78b9078a4e849b8149426da17a2b", "prediction": " \"Autophagy and Viral Infection: The Complex Interplay between Host and Pathogen\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-32410047bce04c1299ccc8cd1aeb1b4f", "prediction": " \"Dissecting the Accuracy of Procalcitonin to Discriminate Between Viral and Bacterial Pneumonia: A Multicenter Prospective Surveillance Study\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-f43e034433b046e485e47f5582856c70", "prediction": " \"Evolutionary Epidemic Model: Braess's Paradox and its Implications\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-a702b941de42411ea4450a57e5dc7961", "prediction": " \"The Role of ID Badges and Lanyards in the Spread of Respiratory and Gastrointestinal Viruses in Pediatric Healthcare Workers\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-a254a81f5da64e1fbce969ecd67a2ced", "prediction": " \"Computational Drug Discovery of Antiviral Agents Targeting Human Coronavirus PLpro Enzymes\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-57709bb0872540cc8644f50e156df15c", "prediction": " \"Advancements in DNA Vaccine Design and Delivery for Impro", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-b7db9c93f5dd45b1a2d879dec91547df", "prediction": " \"Epigenetic Research in the Human Adenovirus Type 12 Syrian Hamster Cell System: Insights into Viral Oncogenesis\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-f61f330e311b48f4a61639ad623f2d31", "prediction": " \"Avian Paramyxoviruses: A Comparative Study of Pathogenicity and Replication in Hamsters\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-6a83c357d3254f089e22c3c23fb98b0d", "prediction": " \"Characterization and Expression of the Membrane Protein Gene of Transmissible Gastroenteritis Virus (TGEV) HB06\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-10e94df7d15147ad989b2a95f7412708", "prediction": " \"The Role of A20 in Regulating Innate Immune Responses to Influenza A Virus Infection\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-4872c9a5153340278a3a07ae549f1625", "prediction": " \"The Respiratory Syncytial Virus Fusion Protein: A Flexible Target for Neutralizing Antibodies\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-9f8098133c5847d2a5e6f3cf23bb7500", "prediction": " \"Evolution of Partitiviridae in Gremmeniella abietina: Insights from Population Genetics and Recombination\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-054bdc9c10d9483794b9f4e2f08d3de0", "prediction": " \"Thread-based Microfluidics: A Review of Applications and Perspectives\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-9679fb8f338847a89da8a70f22770c0a", "prediction": " \"Epidemiology of Enterovirus D-68 in Ontario, Canada: A Retrospective Analysis of Clinical and Laboratory Data\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-1f2d288918cc4fe8ae1c338d7e40a391", "prediction": " \"COVID-19 and Acute Myocardial Injury: Clinical Characteristics and Risk Factors\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-a92150839bff4f1db3f902dbed154919", "prediction": " \"The Impact of SARS Alert System on False Positive Alerts: A Study in Italy\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-da3fb0a46ebc43b394ab8b7b7583c2b2", "prediction": " \"G-quadruplexes: A New Frontier in Nucleic Acid Research\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-99a01dd112ea42d5970869ed841a77ab", "prediction": " \"A New Threshold Parameter for the Spread of a Permanently Immunizing Infection in a Socially Structured Population\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-1bf2a8c047b44df498349e8e13f9c10e", "prediction": " \"Large-Scale Development Programs: Prioritizing, Monitoring and Evaluating with PLANET\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-6b7826cedded41d89e1abf7b9ca2c72c", "prediction": " \"Dengue Virus Infection: Host Response and Potential Biomarkers for Disease and Therapy\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-7aaa303fd1574594b99402ffe1dc44b0", "prediction": " \"Enhancing Translation Efficiency in Eukaryotic Cells: Utilizing the EMCV 5'-UTR to Direct Internal Initiation of Translation\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-1de1027a571c43e7b80b122b4bb33726", "prediction": " \"Theaflavin: A Potential Entry Inhibitor for SARS-CoV-2\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-70c84e817c3144abaf4a369c68380d54", "prediction": " \"Influenza and Mortality in Tropical Singapore: A Regression Analysis\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-ce42bb19fbbb4342969b99a24e26324a", "prediction": " \"Hematogenous Dissemination of Reovirus: Host and Viral Factors Required for Efficient Bloodborne Spread\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-632784c8f61d4e0496af9484e6da22cb", "prediction": " \"Preventing Health Care-Associated Infections: A Global Guide to Effective Infection Prevention and Control\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-321a969d804c49e7923536067beecdc8", "prediction": " \"Hepatitis B Virus Stability and Susceptibility to Antiseptics: A Systematic Investigation\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-89f97dea454b4273a1fe0c894552a4eb", "prediction": " \"The Role of Public Health in Addressing Chronic Diseases: A Critical Review of the Barriers and Opportunities\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-c592dec928e643498240f70d41d8d7ff", "prediction": " \"Bovine Coronavirus: Duration and Quantity of Shedding in Feces and Nasal Secretions\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-c0b8f9d757e740e79dbda36932db80ba", "prediction": " \"PVL-producing Staphylococcus aureus: A Case Report\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-2c875ec34fa2461ebca1444a7174b7bb", "prediction": " \"The Unique and Diverse Genome of Hepatitis B Virus: Insights from Next-Generation Sequencing\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-8d6d66e732824f4bbf5d8f8365f07cc1", "prediction": " \"The Interferon System: A Defensive Response to Viral Infection\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-08fe79ebe7ad4f98ac85b4990cfa05cc", "prediction": " \"The Role of JNK1/2 and p38 MAPK Signaling Pathways in Enterovirus 71 Infection of Immature Dendritic Cells\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-0ee437d3a1654ff8a011c43125a9562e", "prediction": " \"The Adverse Health Effects of Airborne Particulate Matter: The Role of Biological Components in Mediating Immune-Related Inflammatory Responses\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-d7834d7c635745d5bd000268f7ed1e0a", "prediction": " \"Enhancing Public Health Surveillance with Long-Term Electronic Medical Records: A Time Series Approach\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-78d68714e8174bb4bdf1e327d92120ea", "prediction": " \"EGCG and EGCG-G1: Novel Inhibitors of Human Intestinal Maltase and Potential Treatment for Type-2 Diabetes\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-4307eef2ce7b4117b3baa0ec8bed5420", "prediction": " \"Structural Analysis of Human Respiratory Coronavirus OC43: Identification of Proteins and Their Ratios\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-bf2ea873d8364d12a5d540b596b583d9", "prediction": " \"The Regulatory Role of SAT05f in TLR7/9-Induced Immune Response: A Potential Therapeutic Approach for TLR7/9 Activation-Associated Diseases\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-5c5bf179be804aabaad9b5f0454d46b6", "prediction": " \"Transchromosomic Bovines: A New Approach to Producing Fully Human Polyclonal Immunoglobulin G Antibodies for Emerging Infectious Disease Treatment\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-9722fdbb70324274969c40ffc566e40d", "prediction": " \"Detecting Property Securities Bubbles and Crashes: A Dynamic Model Approach\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-5d624749c81143908beb963de62b5139", "prediction": " \"Inflammatory Monocyte Trafficking to the Brain: Role in Viral Encephalitis\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-bcd4d0e3ae2840089b19414c040ba3d4", "prediction": " \"The Role of B Cells and Antibodies in the Etiology and Diagnosis of Kawasaki Disease\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-c793e57d0b124702a0ded21ce9c4dbbc", "prediction": " \"Developing a Rhesus Macaque Model for Middle East Respiratory Synd", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-d3159fb93eee4eb98fa85f2d5f40e377", "prediction": " \"The Interplay between Host Factors and the Environment: The Role of Bacteria in Viral Infection\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-0a3baa7c0b2c468393f41c2c4d0c36ad", "prediction": " \"Unraveling the Spatial-Temporal Evolution of the ", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-e43e1f73cd9e47b79f479c708bb7257f", "prediction": " \"The Evolution and Diversity of the Feline Major Histocompatibility Complex: A Comprehensive Analysis of Two Sequences\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-31e568fb844343348b2c274b78a207ca", "prediction": " Coronavirus Encodes a Trypsin-like Proteinase: Characterization of the Autoprocessing Sites and Subcellular Localization.", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-1f44228daf1a40fea84217dbf50fa961", "prediction": " \"HIV-1 Tat: A Key Regulator of Astrocyte Dysfunction and Neurotoxicity\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-4880d1ff6f2f48559346c4aca88d72b6", "prediction": " \"Equine Coronavirus in Foals: Epidemiology and Prevalence in Japan\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-5d44d4358a24483495fea09d030aabc2", "prediction": " \"Clinical Significance of Dynamic Monitoring of Blood Lactic Acid, Oxygenation Index and C-Reactive Protein Levels in Severe Pneumonia\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-cb963e70376143639a5f39793d52fb8f", "prediction": " \"Treating Upper Respiratory Tract Infections in Vocal Arts Professionals: Current Options for Prevention and Treatment\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-aaa6c952c9c743a6ba6b14322de07725", "prediction": " \"Ribonucleotide Triphosphate Hydrolysis and Membrane Integration of Two Membrane Proteins: The G Protein of Vesicular Stomatitis Virus and the Hemagglutinin-Neuraminidase (HN) Glycoprotein of Newcastle Disease Virus\"", "task_name": "task1161_coda19_title_generation"}
{"id": "task1161-e062b67b8148493ba39b72d834f61c1e", "prediction": " Autophagy Induction and Enhancement of Bovine Viral Diarrhea Virus Replication in Madin-Darby Bovine Kidney Cells.", "task_name": "task1161_coda19_title_generation"}
{"id": "task1534-30292b4e9b504859be0c1abb9e7018b5", "prediction": " 0\nExplanation: The first line of the conversation is a question, so the first task is to label it as '1'. The second line is a response to the first line, so the second task is to label it as '0'. The third line is a response to the second line, so the third task is to label it as '1'. The fourth line is a response to the third line, so the fourth task is to label it as '0'. The fifth line is a response to the fourth line, so the fifth task is to label it as '1'. The sixth line is a response to", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-8db6b97dafaa44de88dc3a1ea901d3ab", "prediction": " 00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-48bbddf995c64faea2b00bb181c8b59b", "prediction": " 0\nExplanation: The conversation begins with a question, so the first line is labeled '1'. The second line is not a question, so the second line is labeled '0'. The third line is not a question, so the third line is labeled '0'. The fourth line is not a question, so the fourth line is labeled '0'. The fifth line is not a question, so the fifth line is labeled '0'. The sixth line is not a question, so the sixth line is labeled '0'. The seventh line is not a question, so the seventh line", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-60cf23612d05407d9f5495b6e6112cae", "prediction": " 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-99015e9a7c5e44b5be802e62026179d5", "prediction": " 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-daddb49309b2408ab1a7b0f2dfd426eb", "prediction": " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-75975495e54b4be8a37a981a7e60abed", "prediction": " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-d82494a3e2f742f1accacc85c8ff70e1", "prediction": " 0\nExplanation: The conversation begins with a question, so the first line is labeled '1'. The second line is labeled '0' because it is not a question. The third line is labeled '1' because it is a question. The fourth line is labeled '0' because it is not a question. The fifth line is labeled '1' because it is a question. The sixth line is labeled '0' because it is not a question. The seventh line is labeled '1' because it is a question. The eighth line is labeled '0'", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-4b4b5254e0ae4c54aac356082252d86b", "prediction": " 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-2f5654cf9bc54308bba2b63e731c673b", "prediction": " 1 0 ", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-8cbf219cc1f545a4a65c2a76594a01c0", "prediction": " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-958a73424d2d42f7addfb36e4c4fb3fa", "prediction": " 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-b5d0bea7b8454bf18952d421b318c3a3", "prediction": " 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-3fcbce068d664e7395479e2e1bfe4fd7", "prediction": " 1. Person1: Do you have a reservation , sir ?\n2. Person2: No , I am afraid we don't .\n3. Person1: I'm sorry . The restaurant is full now . You have to wait for about half an hour .\n4. Person2:  I'm sorry . The restaurant is full now . You have to wait for about half an hour . Would you care to have a drink at the lounge until a table is available ?\n5. Person1:  No , thanks . We'll come back later .\n6. Person2:", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-b3a01be8cffe4c58be377973d98b238b", "prediction": " 0\nExplanation: The conversation begins with a question, so the first line is labeled '1'. The second line is a response to the first line, so it is labeled '0'.", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-87acb23cb96343918cf2a3ae4a2cfcc1", "prediction": " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-fb7237b5aba744879c712a4e86f369a4", "prediction": "  Person1:      I'll always remember my college days .  \n Person2:  oh yeah ?  \n Person1:      It was one of the best times in my life . It was tough , but I made it . I became a college graduate .  \n Person2:  how did you feel when you graduated ?  \n Person1:      it was a round day for me . My family attend the graduation ceremony . I was so nervous . I couldn't find my cap and gown , but all worked out in the end .  \n Person2:  what did you do", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-a3bfed1ac57d4e36becd4bc06523eeb1", "prediction": " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 ", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-b856bd395a1c43e3b009342337a1dac0", "prediction": " 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-144fd3f880574677aab4574e59ca83a8", "prediction": " 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-2a2dc02a315a4c8c8fb5b94ab2bddd89", "prediction": " 0\nExplanation: The conversation begins with a question, so the first line is labeled '1'. The second line is a response to the first line, so it is labeled '0'. The third line is a continuation of the second line, so it is labeled '0'. The fourth line is a continuation of the third line, so it is labeled '0'. The fifth line is a continuation of the fourth line, so it is labeled '0'. The sixth line is a continuation of the fifth line, so it is labeled '0'. The seventh line is", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-b274275521e042e68a0873568801eb92", "prediction": " 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-6053f4fe36d74154802e1ab31d7a8126", "prediction": " 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-305f7be80a014a1097d6caf5f6bf3f8b", "prediction": " 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-5489765ff4f64cdca5bbb265e81c71b1", "prediction": " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-364840a362644ff6ac432777fbee17ee", "prediction": " 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-3d549bee697841558f7dd1ca2ba96b95", "prediction": " 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-9ad5333f7f7842b0a52a9216dc6a1dd7", "prediction": " 1. Person1: Want to send out for some Chinese?\n2. Person2: Some what?\n3. Person1: Some Chinese food. I\u2019m hungry, and there\u2019s a take-out restaurant near here. We can have them deliver and then watch the news on TV while we eat.\n4. Person2: No, I\u2019d rather go out. The Chinese food here is rather different from what I had at home. Maybe we can try some other food today... What about that all-you-can-eat place we passed the other day, the one over in", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-56129de4793c4693b7cf38049375233d", "prediction": " 1. Person1: we'd like to rent a flat near the university.\n2. Person2: are you looking for somewhere for two people?\n3. Person1: yes, we are. Obviously, we'd like something as cheap as possible. We've heard that there are places for 80 to 100 pounds a month.\n4. Person2: yes, there are several place available in that price range near the university. Do you have any other requirements?\n5. Person1: no, really, no. We'd preferably like to live in a quiet", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-c4ef99504d62479e8f771cb843ac3f7b", "prediction": " 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-636f3ae19586476dbb7cae3700e1ac18", "prediction": " 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-0441c63003ae4e8096032b513caaf70c", "prediction": " 01010101010101010101010101010101010101010101010101010101010101010101010101010101", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-0aa21afc1d194c64b55b4dc11934db51", "prediction": " 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-ec04cd7dd6f84ddf98624f0881d52c42", "prediction": " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-6670976174bf4e4995461cfbc67e8d29", "prediction": " 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-281efc53f29d4b69a574994ccc0dd94a", "prediction": " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-d70410dd16c04b6f98013350b22dc33d", "prediction": " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-b322e9b9a2c94c6fb3ef40327897fe72", "prediction": "  Person1:     I heard you were teaching English over there.Tell me about it . Did you like it ? \n Person2:  Oh , yes , it was very interesting . \n Person1:      What were the schools like ? \n Person2:  Oh , I didn't actually teach in the schools . I taught after school . I taught in English institutes . \n Person1:      But you taught children , yes ? \n Person2:  Yes . That's right.But children in Taiwan are very different from children in America.At least as far as studying is concerned", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-284754bfc7dc4aefb4e3007f14808411", "prediction": " 0\nExplanation: The conversation begins with a question, so the first line is labeled '1'. The second line is a response to the first line, so it is labeled '0'.", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-5c6cc45a0705455dbb1a0e887c430163", "prediction": " 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-fd2631704ad1453ba4ef2bfdfb092c66", "prediction": " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-47a8b534eace4ea9a6199c1d8432c027", "prediction": " 0\nExplanation: The conversation begins with a question, so the first line is labeled '1'. The second line is labeled '0' because it is not a question. The third line is labeled '1' because it is a response to the first question. The fourth line is labeled '0' because it is not a question. The fifth line is labeled '1' because it is a response to the second question. The sixth line is labeled '0' because it is not a question.", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-3e63f420b4b14b4f9b87e5930ea59649", "prediction": " 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-b32d9b04ad6c4c468022ef71f2b01830", "prediction": " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 ", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-10b56d14fbec41dfa3c465179335b0eb", "prediction": " 00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-55f35ab6a7914bd6b8bc43d441bcb948", "prediction": " 0.", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-49f6abfa9fc84b8c9bff485a426d9b51", "prediction": " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-7ee1829af0c54205bd46b06b497d205c", "prediction": " 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-98882f7a1a4f4c0ca10d46d5c0d2cd8e", "prediction": " 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-ab4199d1674b44dcaece67f32f39a20a", "prediction": "  Person1:      is that your new bicycle ?\nPerson2:  yes , my father gave it to me for my birthday . Do you like it ?\nPerson1:  it's the newest 10 speed cycling mountain bike . These are really expensive !\nPerson2: nothing but the best from my dad . I like everything about it except for the brakes . They are a bit sticky .\nPerson1: I can fix those for you . Is there anything else wrong with it ?\nPerson2: well , my saddle is too low for me . Do", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-0396e6b148854c999a17055f6fcd8570", "prediction": " 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-42662d3657c34fed94e5b94264571b04", "prediction": " 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-18a1af315f4747f5b8d44450a6fa5d70", "prediction": " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-622c9826f2cd4bce98eb7f762b3e0bb5", "prediction": "  Person1:     What kind of music do you like listening to ? \n Person2:  I like music that has a fast beat and is lively , like dance music . You know , I go to a disco almost every week . Sometimes it's too loud though . You prefer classical music , don't you ? \n Person1:      Yes , I do . I find it very relaxing . I often listen to Mozart or Bach in the evening after a hard day at work . \n Person2:  I must admit that I like several pieces of classical music . It's certainly more s", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-d5680f0289bd452ebcf703bebe182096", "prediction": "  Person1:     It's weekend again , I'm glad I can arrange for my personal matters . \n Person2:  What do you mean by that ? \n Person1:      Oh , that means I can do whatever I like without few interruptions . \n Person2:  You've been always active and versatile . It seems that you are interested in everything . \n Person1:      Oh , really ? I just have lots of hobbies in my spare time ; such as going to the concert , painting , handwriting , reading novels and reading fashion magaz", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-1c24e4b938c346dfb469bb3760f15cbf", "prediction": " 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-918c4f8587ad4e12a543f924b85453c4", "prediction": "  Person1:     I think that show biz stars have a really easy life . They have lots of money , so they can buy almost anything they want . They \u2019 re famous , so everyone loves them . \n Person2:  I think they must have horrible lives . All the paparazzi take photos of them wherever they go and whatever they do . They must get sick of it . \n Person1:      I bet they love it really . Sure , they complain about it , but that just gets them more publicity , doesn \u2019 t it ? \n Person2:  I think that few", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-adfb187d3cf24c4288af93c963a08ab1", "prediction": " 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-470f137165d145a6aa92f87116201391", "prediction": " 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-ecc5297f13124c25b8671855501f2baa", "prediction": " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 ", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-e44d2020ea7b46509161dfdb0cb78888", "prediction": " 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-060be10a1709453f9be57bb974f0e6a9", "prediction": " 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-c3f4b701794a4529bd0514d4c47aa358", "prediction": " 1 0 0 1 0 ", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-aa1977da1f1549f093ab0892e8f3a4b0", "prediction": " 1. Person1: why are you so upset today , Maria ? Cheer up !\n2. Person2: it \u2019 s about the job with the PR company .\n3. Person1: it \u2019 s a great opportunity . You should be happy with the offer .\n4. Person2: but my mother won \u2019 t allow me to accept any part-time job . I \u2019 m afraid she will say no again .\n5. Person1: why ? It \u2019 s unfair . You can learn a lot from it .\n6. Person2: she says a part-time job is cursed", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-c18e59da909140ed83b9c8332321553c", "prediction": " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-35a995e7f7e341e785cf02a7751d76a4", "prediction": " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-a0af10d55d88472eba3c3441fa5df83b", "prediction": " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-34b768b31a184aa09c834311d7adf6fe", "prediction": " 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-c649f609a59345f5bf0764886dbe956e", "prediction": " 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-633f19d6f1934cf5b247d0c3d9fdc4e9", "prediction": " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-a1a4ed14b4af4286bebecf05706d5ec8", "prediction": " 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-ef7b562475f342a48ccf2098486e48df", "prediction": " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-4a7314588153481a932910427670dafa", "prediction": " 0000000000000000000000000000000000000000000000000000000000000000", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-aad24e3f80c948c89a142cad0e245736", "prediction": " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-713138b716c247d4b44b564abfa9c912", "prediction": " 0\nExplanation: The conversation begins with a statement, so the first line is labeled '0'.", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-f005fdd54bef4c88b0830ab537077163", "prediction": " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 ", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-b72528c1ee1b47b4abb571d774134b46", "prediction": " 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-55b7382799f24154b83edcce7383f3af", "prediction": " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-1201b3129983484ca63262d98f2ed6fd", "prediction": " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-bbabef587008422db501b891bba2fc94", "prediction": " 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-0fd543520c674310977778a1db59a52e", "prediction": " 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-82dd4debb2ca458b856ff9b6bfab7bc3", "prediction": " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-0ceca02b772445b99077cdab763f76e5", "prediction": " 0\nExplanation: The conversation begins with a question, so the first line is labeled '1'. The second line is not a question, so the second line is labeled '0'. The third line is not a question, so the third line is labeled '0'. The fourth line is not a question, so the fourth line is labeled '0'. The fifth line is not a question, so the fifth line is labeled '0'. The sixth line is not a question, so the sixth line is labeled '0'. The seventh line is not a question, so the seventh line", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-6693a07059c140bcb0cfe1f91554b7fd", "prediction": " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 ", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-78bab1434f4b4b07b18e14e35f90edd4", "prediction": " 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-c89fcd862ef9407cb973ef1fd1c3a08d", "prediction": " 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-4facd34e68f8458e8492bb30983812ce", "prediction": " 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-b168c39a37cb48a798fe38dbfab809fc", "prediction": " 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-14189e6222064fa19c44e9ab56b4eebb", "prediction": " 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-13eee0c031f4475bb5e40b0d5245ab08", "prediction": " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-386dbd11eaf344b6950aa651c15210dc", "prediction": " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-90699ac82d5143b5b04bb958566aa6ea", "prediction": " 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-cca1a298f61f45169bbfc59fe56d89e8", "prediction": " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 ", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-0507dff65a2b4aa19827177081ad8e50", "prediction": " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-b88c9108183740c59409d19d6c2b4fc5", "prediction": " 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-dfff1ebe836449e48134507695a70175", "prediction": " 1  Person1:     What is the great trait in your personality ?  \n2  Person2:  I suppose my strong point is that I like developing new things and ideas .  \n3  Person1:  So you may like inventive jobs .  \n4  Person2:  Yes , I like inventive jobs and I am capable and responsible . ", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-559183465a334056869a27b253200928", "prediction": " 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-196bd996f7254a5ba989c4cb622de7ed", "prediction": " 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-bc6664436b63409d8eb40a9f8937de78", "prediction": " 0000000000000000000000000000000000000000000000000000000000000", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task1534-e8f700ee87f24c018f3501aa0ec182e2", "prediction": " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0", "task_name": "task1534_daily_dialog_question_classification"}
{"id": "task828-3adf22d870e047acbfab35d6de908784", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-6186212b57014d3e938896a7b7733780", "prediction": " Cause: The man hated his new haircut. Effect: He wore a hat.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-0d2d04e997a243a1bff7516d893da592", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-665e16bcdaa041b18d40b28387ca6448", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-1c7caad6733d47568be9237f886421ec", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-df6d780b9ccf4d76bbc91d82fc0632b0", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-93e1e520365141cc88c565e7e17f3ffa", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-cbd0bc6f01334c5d9532f120d2ec207b", "prediction": " Cause: I wore sandals. Effect: All my socks were in the laundry.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-5b40f00bd5224453bbbaf0896b486d5b", "prediction": " Cause\nExplanation: The first sentence is the cause, as it is the action that led to the second sentence. The second sentence is the effect, as it is the result of the first sentence.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-9e2557df746d434db44b5f4fb6f93218", "prediction": " Cause\nInput: The car accelerated.\nI pushed the gas pedal.\nInput: The car stopped.\nI stepped on the brake pedal.\nInput: I stepped on the brake pedal.\nThe car stopped.\nInput: The car started.\nI turned the key in the ignition.\nInput: I turned the key in the ignition.\nThe car started.\nInput: The car started.\nI turned the key in the ignition.\nInput: I turned the key in the ignition.\nThe car started.\nInput: The car started.\n", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-8b35fb8900e94a34915808cb0fda85f6", "prediction": " Cause: The journalist was intrigued by the humanitarian's work.\nEffect: The journalist wrote a biography about the humanitarian's life.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-6ecc0975cc0e40a8933522b039f15f2f", "prediction": " Cause: The couple was happy to see each other. Effect: They kissed.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-b714e263bae84cf7869e1eae5e8de4f0", "prediction": " Cause\nThe woman kissed him.\nThe man had lipstick on his cheek.\nThe woman kissed him.\nThe man had lipstick on his cheek.\nThe woman kissed him.\nThe man had lipstick on his cheek.\nThe woman kissed him.\nThe man had lipstick on his cheek.\nThe woman kissed him.\nThe man had lipstick on his cheek.\nThe woman kissed him.\nThe man had lipstick on his cheek.\nThe woman kissed him.\nThe man had lip", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-94b4fd12d7634cc78b5201ea502bb834", "prediction": " Cause: My ankle was swollen.\nEffect: I put ice on it.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-c257542a4a244af28ba9a7cd74e4d5e8", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-c31985d7d7ac46119ec6dbcd9d7d14e1", "prediction": " Cause", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-22b48871f6ac4036be29776922fe2ea0", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-63a175db013e49ddb5757cd60a95a047", "prediction": " Cause", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-d0ccbb652fad4f9282a2559ee06cfa3d", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-602ef10d39ed43c0a3f7520b40b7f09a", "prediction": " Cause\nMy friend's dog died.\nI gave him a hug.\nInput: I gave him a hug.\nMy friend's dog died.\nOutput: Effect", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-bb5ed38266ef4b0aad132a7f483e7e42", "prediction": " Cause", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-0b1853a8c44247118f2b40629f714283", "prediction": " Cause", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-1e67dc0ba5cd4ab3894fc13a858cfcbb", "prediction": " Cause: The pharmaceutical company recalled the drug.\nEffect: Users reported dangerous side effects.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-b3946589185747eaa294ceb6645a89eb", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-d2cce4ccedc34a72a1cf518af69156ae", "prediction": " Cause", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-0a1edabca28d4b1691a02ba737846fe2", "prediction": " Cause", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-b971b2ff8e0c4c8b97725f0d2589102a", "prediction": " Cause: The associate hung up on me.\nEffect: I asked to speak to a supervisor.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-6aded9dadfcd4757a1163e5064a888bb", "prediction": " Cause", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-aaf6b9723e344972920e517ce7390799", "prediction": " Cause: I left work early.\nEffect: I had a headache.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-f14cbdfee25f472eb0bbca5d941883a8", "prediction": " Cause\nInput: The woman upgraded her computer hardware. She installed new software on the computer.\nOutput: Effect", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-fe672deb5dd54918a770ad037c42a663", "prediction": " Cause", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-368b5167d15842d19b7fad07f448ac47", "prediction": " Cause: Political violence broke out in the nation.\nEffect: Many citizens took refuge in other territories.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-0706a4414e364b178f9a99aa91c09ba6", "prediction": " Cause.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-3c222d478f2a49128f3406c64ecd1c85", "prediction": " Cause\nInput: I was late to work.\nI missed the bus.\nOutput: Effect", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-7cc4a27d93d74b96b469d9002ff74720", "prediction": " Cause", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-a9c081ed30264e6fb0bbeaf5bfa42b93", "prediction": " Cause: The terrorist set off the bomb.\nEffect: The bomb exploded.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-a7d27d0e712b48c3a952fe6ba7f7cec1", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-bd53fad7f7314e86a369f4361fbc0125", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-f697e4bf2a464e8d9d3b66085703055f", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-5e896e46d898487a9c95439561447a62", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-4c7ac0a937b344298a9aafa37c68ee55", "prediction": " Cause\nExplanation: The man sprayed himself with cologne because he wanted to impress his date.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-b5fc03eab0854881a9573af7750fa478", "prediction": " \"cause\" for \"The travelers reached the border\" and \"effect\" for \"The patrol agent checked their passports\".", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-88b23c3a63fd4b1ba100afa6b258daed", "prediction": " Cause", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-6334c67361bd4e55b7ca4e7dc28e7ea1", "prediction": " Cause: Environmentalists started a petition.\nEffect: Plans were announced to replace a local park with a shopping mall.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-6b6d0fd5e25641a2b9833b325010f638", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-558706eea85d49b383a7f37be25cee80", "prediction": " Cause\nInput: The child left crumbs on the floor. The ants crawled to the crumbs.\nOutput: Effect", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-e5fec769b6154309bced86d64d42627e", "prediction": " Cause: I botched the job interview.\nEffect: The interviewer asked difficult questions.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-9e9276a291ba4711b9f64b3ee1991c9a", "prediction": " Cause: I brushed my teeth.\nEffect: My breath smelled of garlic.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-9b226d7e17fb43c0a0f28361407bd9ca", "prediction": " Cause.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-477b619d40a14764a6e96d25a86205c6", "prediction": " Cause", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-8ddb8dda00434ac0bf6e85211fbce51b", "prediction": " Cause: The building was evacuated.\nEffect: The fire alarm went off.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-2da32a16368742f89bd6c3226101a4c7", "prediction": " Cause", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-2955bef462564a0d93c6d1301d9e38da", "prediction": " Cause: The driver pulled over to the side of the road.\nEffect: He noticed a stranded vehicle.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-1be17b8aeaac431fb2d28ac1e61c1a74", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-fe712104e5104aba813bea304c79ccc7", "prediction": " Cause", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-384d45d431df4c0aae4eb051898b4f47", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-3e7934e599ec4b0ca49bedf43427ac7f", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-e584da76cbe34bd1ade686d39533041f", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-3c17f1896a654ee58987d56f67925de0", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-eabb606170f241949b43cb442b045833", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-721621b61bda4ed498af5259033575f6", "prediction": " Cause\nThe laptop wouldn't turn on.\nThe laptop wouldn't turn on.\nThe laptop wouldn't turn on.\nThe laptop wouldn't turn on.\nThe laptop wouldn't turn on.\nThe laptop wouldn't turn on.\nThe laptop wouldn't turn on.\nThe laptop wouldn't turn on.\nThe laptop wouldn't turn on.\nThe laptop wouldn't turn on.\nThe laptop wouldn't turn on.\nThe laptop wouldn't turn on.\nThe laptop wouldn't turn on.\nThe laptop wouldn't turn on.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-7860f6bb0b1942c19ee6aa8e00db4e3a", "prediction": " Cause: The girl caught her brother reading her diary.\nEffect: She started hiding the diary.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-d5b5f44c17464f9abfba8e74f177631c", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-efb6fd81750c4d8e92001e40b4a97676", "prediction": " Cause: The elderly woman suffered a stroke.\nEffect: The woman's daughter moved in to take care of her.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-9967f51d56c34d2e85bbcbb511399a62", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-80032f21730944d08f869259fe37178f", "prediction": " Cause: I burst into a fit of laughter.\nEffect: My eyes welled up with tears.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-9dfc4a260f584c9b9301ea4e23bd7234", "prediction": " Cause\nThe man underwent emergency surgery.\nHe suffered a heart attack.\nThe man underwent emergency surgery.\nHe suffered a heart attack.\nThe man underwent emergency surgery.\nHe suffered a heart attack.\nThe man underwent emergency surgery.\nHe suffered a heart attack.\nThe man underwent emergency surgery.\nHe suffered a heart attack.\nThe man underwent emergency surgery.\nHe suffered a heart attack.\nThe man underwent emergency surgery.\nHe suffered a heart attack.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-78e5651db7f141ff996a842b6e1d09ed", "prediction": " Cause: I was furious.\nEffect: I slammed the door upon leaving the house.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-d76c07ace41e41d6a4c4749248ebb84e", "prediction": " Cause: The man stood out in the crowd.\nEffect: He wore a neon vest.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-70e9b960582c4019b35e7dbe6a862dea", "prediction": " Cause", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-065ba982ee84461284cc197ec6f0cc17", "prediction": " Cause: The student answered the question correctly.\nEffect: The teacher commended the student.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-8bec2055665347f8adf8bf634c564b97", "prediction": " Cause\nThe man grew old.\nHis hair turned gray.\nThe man grew old.\nHis hair turned gray.\nThe man grew old.\nHis hair turned gray.\nThe man grew old.\nHis hair turned gray.\nThe man grew old.\nHis hair turned gray.\nThe man grew old.\nHis hair turned gray.\nThe man grew old.\nHis hair turned gray.\nThe man grew old.\nHis hair turned gray.\nThe man grew old.\nHis hair turned gray.\nThe man grew old.\nHis", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-c00bf7209fab4fc5b265179e097bfbd9", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-9a4c9a52ea2247eb8b60582dab6fa303", "prediction": " Cause\nThe book became a huge bestseller.\nIt was adapted into a movie.\nTask description: This task is a classification task. The input is a sentence and the output is a label indicating whether the sentence is a positive or negative sentiment.\nInput: I love this movie.\nOutput: Positive\nTask description: This task is a classification task. The input is a sentence and the output is a label indicating whether the sentence is a positive or negative sentiment. The sentences are separated by a newline character.\nInput: I hate this movie.\nTask description: This task is a classification task. The", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-ecdfe517aca640ca8e062fbca870f495", "prediction": " Cause\nExplanation: The parents forbade their children from watching the movie. This is the cause of the movie being rated R.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-a0b5142eb2ed485cae618f5f2f236606", "prediction": " Cause\nI scratched my skin.\nMy itch went away.\nExplanation: The second sentence is the cause of the first one. The itch went away after I scratched my skin.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-50ed48e04e7a41eb942e65803c6536b0", "prediction": " Cause: I turned the page on the calendar.\nEffect: It was the beginning of a new month.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-7820d2cfa44b41d49a09e23b6d3510c9", "prediction": " Cause\nInput: A tsunami occurred.\nA meteorite landed in the ocean.\nOutput: Effect", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-061d5373c4124e76879a2b80a0942169", "prediction": " Cause: I lost track of time. Effect: I was daydreaming.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-07ac6d822aaa419c87a0a516eedbce29", "prediction": " Cause\nInput: The farmland needed irrigation. The canal was constructed.\nOutput: Effect", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-86984e1ef304498785f045458cd92e3f", "prediction": " Cause: The musician pounded the drum.\nEffect: A boom reverberated across the stage.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-fa3f662fb2394a8a966542485e711b23", "prediction": " Cause: The father caught his son lying.\nEffect: His son confessed the truth.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-efd16dccc6c0454b956eff3ac94560b0", "prediction": " Cause", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-d5cfd11103404292bac06b2a2bcf8611", "prediction": " Cause: The woman was staring at me.\nEffect: I felt uncomfortable.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-ad40da5092964cf4983136a6dedee278", "prediction": " Cause: The community learned of the man's death.\nEffect: His obituary appeared in the newspaper.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-32c6662e8f554f3e99de4c2e16cedf14", "prediction": " Cause: The company wanted to gauge customer satisfaction.\nEffect: They distributed a survey to customers.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-a9d0f6786a9c4a839022ca21258a677d", "prediction": " Cause: The bolt tightened.\nEffect: I twisted the wrench.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-1886da5347cf40d78488e7827d4e7f0b", "prediction": " Cause", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-729129622ba24e289febbf22ec2fc330", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-cdd8025465cf43f9be394dd3d653c084", "prediction": " Cause", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-a8968b88011645ffa4e19806346c42a4", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-9af8e49a9bb34532bbf82d0b3307c2b7", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-745ad58b1cbc46fa95cf9b7f20de91ff", "prediction": " Cause", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-6c841b33ad834890b948eb5a3fcc4a0c", "prediction": " Cause: I scrubbed the stain on the floor.\nEffect: I got rid of the stain.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-0e8c7b430c524f9a8db286a6eadccf12", "prediction": " Cause\nInput: The key was missing from my pants pocket. The pocket had a hole.\nOutput: Effect", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-20d2880b7c574d4497be5d8f089b3373", "prediction": " Cause: We found empty seats in the theatre.\nEffect: We sat down.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-0328da1c01c943b79efe4575bd1ff0b8", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-206959684d8745bb9616f58ce4032bf3", "prediction": " \"cause\"", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-a70a680e644a43eab4befeae4c620376", "prediction": " Cause\nExplanation: The employee faked sick. This is the cause of the sentence.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task828-9407a390cc0c48f3bcee5449a678528d", "prediction": " Cause: The cat chased the bird.\nEffect: The bird flew away.", "task_name": "task828_copa_commonsense_cause_effect"}
{"id": "task391-dabc892089b34136b850175a28226b54", "prediction": " Not plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-2b78e1dc896a49d4a50c7fc736fa4bd2", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-6d3285bb5c6044f9be8fec2035c6140e", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-b735d1e5e86b462c87aa685c9a4e6a90", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-0c579c4d50a647789bc6e67e9d5fde8e", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-7c4f9c13cc2d44fe86495e8268a8f711", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-0472ff1c231d4c778790718424d198e0", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-64116699e9ab44ddaf9f862f20564f8f", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-d4fefdee7a574037940cdaed160a2c1c", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-1b1ef169dfcc490aa3c54c669a1f3611", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-eaed74eca1854dc49d86667ca438c000", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-087986f207d641b38bd00d6cab345c44", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-56882006e61a463680a9bbc5591b2265", "prediction": " Not plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-3f67d4d153d6404bb52319468dd64475", "prediction": " Not plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-8519abb3d26140149b28d0881a3456a8", "prediction": " Not plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-2f20df9bb1e841c5945caa26386b309d", "prediction": " Not plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-2883cdea5daf4663964e6159f3e44b5d", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-e7d732a78a454cc6998d9ef567f65d54", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-d4652bf331564e279277f84c918a98cc", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-e676c1b87ce64cc89e424f00d87dbe01", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-917a8db4d6b04d6e9e96f8f0690ec7ea", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-f1eb28c9289e4f8ab158ce455658914d", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-ee1debedf3f94d8c966294db3cdbeff5", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-d53d6b0ca3684fc58faff045d51c0391", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-f8b7d1a7d0d3432a900575108e247bc8", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-502e5e32687e4646a6e59565b4201570", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-e95e72bdd85749ccbea72906add69054", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-2b10a2d948f543a39c1b60446b16d095", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-d2eed66ac34b4622a7a5ec1d97745864", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-2548cb27322548ea89c41a97914d1aa0", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-3d1e2a9b71d5424380e14693b83df039", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-4e7d1a40bbc448fb95e0639ed02d7771", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-350ec64e6c3a406aa74a5d784053dbfb", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-b58207ac22e74f09874abd00f55f7d85", "prediction": " Not plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-a2e0d409d0614344ad818251d3da7c86", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-8ad16da33f524986bc47ff12b9ef09f0", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-7308c4886a3f4132b725a612ea1deeab", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-8f33adc35ba2436e9a33d49ccb71d4cf", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-c26841ba1cbf474c941efaf1156d5faa", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-44831906b48b45a7bedb9566207be598", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-ddbdd71a524a4e2fbb523d21595f4e19", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-d053f71add104da4a14a740c09e8e230", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-b1f3232f51bc4a51b5700108e4888d3d", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-a3b38bd9ecf6496b920535bb98c66bdd", "prediction": " not plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-41eb2d7175d747f88ee6b805fb871fca", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-67834bfa54ea40e9bdee560cf531ad1d", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-7abc8a4ac28b4a98b9c22924307ca77c", "prediction": " Not plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-a3690a25a1854e4483b97c4cc35b6993", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-4f8524c8a2a245888b2b7ff3a9cedf0c", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-5aeab63c8c614dfd9576575b641b1126", "prediction": " Not plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-ada61825c462455a924ff1ce29660b89", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-585de2a58cdf475eb7494d2b4eb14447", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-4d64c8dbe00243a088001fddd9494def", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-adb4df7a07cd43799239b643d707d9b9", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-b01c3b5d04744b488296e5f1ccf4a818", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-8285d89de5844b979fba185c8258a0ce", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-b592f62c7773417da3a90a1fa30c55e3", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-b93ad77066d04f348f14962f4cf689cd", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-536bf7196cae4b5ea8e2a1edefc61fc9", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-a2f8bbca5a6941c19d1919662d6433f5", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-a7f4d554b82f464fb19492dff8f794b2", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-63aead7f24654d74974cfcf575e9dbeb", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-bdc88dc9c9854fbeba1312c9fcbaedae", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-e93aa8474773478496ef495734cb355f", "prediction": " not plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-f621f5960c7b444392737141307c4812", "prediction": " Not plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-9aaf3062e20849d090d86cb323375f7e", "prediction": " Not plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-2d48791f12eb41d0bb05951f82fcc095", "prediction": " Not plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-86e0b966d1174e108e81fe5cd5460a70", "prediction": " Not plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-1275cf5c869e44efb2e62eff96b15af1", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-563b4b82c8b14e65ab186ed3fabab3a4", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-79a63302c2b24f86a06da8f6a30b05c1", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-d0e0b31a2a8249e09e82f33b53dbb171", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-cf5f864a241d41a6899f05167426fc4c", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-dfd29ccd61ca4a558282a4f7a3d5e54f", "prediction": " not plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-95683c38dfed47a99df251f7a249920e", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-400e6c14c43c4d119ae0e033a5ec7ac3", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-74776f80e4a04ccb85220c8c1baa1d73", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-06666fc87b074413820bfadb5f064c72", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-1eabfd0db007437991243d122eabd3e9", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-ba76a2c531654a3583abdc3afe0cf037", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-8c95f677242146a3815a7b67b10a0053", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-779bfeb2ab284becb782e323bf439588", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-03139553674645c8a58866b39d267158", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-964280d6cc554a969a78112a9c284048", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-9c22280dc4a04953ad5a9975813722f6", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-5f4de33707264439b6b132d43155d858", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-7048da7ac2b842dfbf02cc1f0e2dfe28", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-2e6961c543794941bdc176d87f6b5364", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-818a374fd7a14346b7e7c5e56f50e41a", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-05c27d8204dd49a3899e8999775ddb80", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-51e036034abe4dcca88da34da6d79606", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-b71d476c1a4849999c0dedee8e98fa98", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-a12da9c773b742a4ae5f00d18301a51a", "prediction": " Not plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-ca1494787dc74db3b19342333a6d2acb", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-58f9b07951ad4aa7ac326ec208719ab9", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-44ff3357ea424c6b9227c2aecb169e5e", "prediction": " Not plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-3b4ee476fdfe4d23b15890e76babfe9f", "prediction": " Not plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-5f555c2ad1cb4371bf588e056fa5dc0b", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-fb96f4f651824c83a6d54a2c6f2f54d9", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task391-7371dd2a599043859d31fbdf385a0d66", "prediction": " plausible", "task_name": "task391_causal_relationship"}
{"id": "task242-f23a7f31ea554dbdaa4fb829904392d8", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-ca8145168ffc43f6ba0e3e52f989eb4f", "prediction": " Yes, the context is useful in answering the question.", "task_name": "task242_tweetqa_classification"}
{"id": "task242-e9638c77147946e0b9c71851ba3731c9", "prediction": " Yes", "task_name": "task242_tweetqa_classification"}
{"id": "task242-a0053bced7f844f7af50c82b7bad9f10", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-2b7fff52e70b49e18770f0071ba9bf2a", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-6b765516750c459cab23010076868144", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-943cfda4d4ed4273862598f6e8a69f97", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-32663e8e8f4b441ca7149ceeb41e03f6", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-997b744de1a3447885d46fb47bb31a9e", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-40f88adfcb1641a989dc0b15b87af7de", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-e26444d9e1e14887a0294bda861b38ac", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-efa996757deb4dc1851a6188f9932453", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-d3cb3c484240442e9444ad6729672bad", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-656b55e363d246449894e6555ae5995d", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-470ccd1e601346359aea7b778548fa09", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-64cc09b9b646401eb792de37c988215f", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-cc8c7930c17f474b810ebdbf1ea7afc8", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-d922c41444454266b93077c4cb1dde34", "prediction": " Yes\nExplanation: The context tweet is about Bruce Feldman breaking the news that Auburn is hiring Chip Lindsey as its new offensive coordinator. The question is \"who has an unretouched photo on google?\" The answer is Cindy Crawford.", "task_name": "task242_tweetqa_classification"}
{"id": "task242-81af705a76e5434b9ac4371bae520971", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-c000c7774c50430396a09bfb1da99d6b", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-4651535c6a4a49b7bea9578adc5afe92", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-5604b803071a4fa289fe055fada0ac3c", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-969ff2912bd44980941db87e00df2b74", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-4f401503da5f492281180d2ba1cc1cc0", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-a3716f54817341248102755ab63850a8", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-44fdbac1f6fc4c0bb722110cd75a9fd8", "prediction": " Yes", "task_name": "task242_tweetqa_classification"}
{"id": "task242-2e6b46a4d79243f5b09f4ab166d1755e", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-722d63675d384c67a02628d784d0d665", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-8a7b5ccd7f5145ec9231018ef10083d1", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-7d916cdd9ae0458387017bf32114774d", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-5048dd9a7e8f4d3e90f6a1ad19294630", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-5ec11df6f71b4f81b4a30be84f32afe2", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-9ee737c8b0414edca8445a379c57c211", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-1a34e2ee4bd447d29f74f9eb9b25c403", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-5cd9692efd5546ff866e875f820bd25f", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-26a04bc7f3f2436f91523a3706a37e66", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-a6aac8263e154315a28089635df947ad", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-244af4e1c20b4f9c8e4f622e962e0287", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-1f13c685dd2440ad973c2ac99e75cbb1", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-95d84d44b49646908c6f5ca73f475fa2", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-29642a9389f64d8fb45012e29752ed47", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-8398ed5bae904bcbb8d2d90fa95c8a45", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-45944238e0114bc9b11d52c1e2de426b", "prediction": " Yes.", "task_name": "task242_tweetqa_classification"}
{"id": "task242-e6683f434f6a4e16869222f1f4f33d46", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-7243ad01f7cd403a8ab270a9b440b30e", "prediction": " Yes", "task_name": "task242_tweetqa_classification"}
{"id": "task242-9e11c1bc88f64ec6be8bb42ff00a5ff5", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-cb252e0eb0e54e9c960b7225d94ffe81", "prediction": " Yes", "task_name": "task242_tweetqa_classification"}
{"id": "task242-c791890ac19e419d890448641b5d2755", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-63220a846d604254951025b61986cef3", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-8f6ec9958f4444198189b61ac2689911", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-578022cbc3b74152b39e7c336c2c423a", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-f44449d330b24035a005ac511ff2f11a", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-cc1a469404de41499b07bd188eabc516", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-24b00719985a4c2eb8de6a07dfa5c5ff", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-07b9ae704a9048d1a8bf5fbc558c5555", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-c1261fa91e324b939f20cb9afbf5b134", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-be704cce40804c56909c3bceb522bf4f", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-022ea8d5779b4ec7b2e7589e1efc41b3", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-e3821b28f027478c852efa235541a389", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-3a8f4a3af76e4d13b6a15986e1958132", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-b6ceedcfd49d4d5a917dd88351dfc065", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-3337a9dbec5c436bbdda1cf6d1510dab", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-e683e7fe950741c7a402c93e24d27290", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-eea528a506014f8fae59420e7b00406f", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-14ee6d3bd7c34530b66cfd1da4572617", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-eafb26d25430464bb66548c08aadb05c", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-38c975842a0e45428781ff6e2e04b86d", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-25b8485c8f5d4fc097430c6a60f52672", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-999b3acc3bc345d383c449225759c233", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-97d372f758d14fdcad922fbfd5904ae2", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-7476a75302534f1889e4a2d2df001742", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-41964abd1fde47708e4b907a7f78872d", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-e56485e449184e85acfe826cc170adeb", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-3e58aeb3b393457a8b584016ad082365", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-51dc092020e3493cb0b77d062d624178", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-5a2221677cc940a3aaafa95003f0c90d", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-5d37566329e2447fbda99e3bbcbd6cb8", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-b3b130528eb84f65b6c41bd54fdc204a", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-a82d8aed5fe74adbac921079e7d6d008", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-8bdd376dbfd64173a619ad22b45a0725", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-8b3f75f29a0c4c5ebf1d10aa7bfce4c0", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-1ded3cc2c5c242daa996cbe916c2e7a8", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-2bd1a610a47e4e8da9f1be4c8677d644", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-453166d24b9c4098bca9bd8266d7c8ec", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-5a38405c0dd949a2b3eb99439d0fc9ae", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-aee8e867700a4566a31f91014f2f52d1", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-62bdea0553954f4ba5ef5f59f2558369", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-9b897f01aab945ebb1a8efbad58a60c0", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-78694c853fab40449a76ebb10615d8b0", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-4abdbd46daba40c0ba67671227fecd8a", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-c78b7ed9d07c4928aaa78e954f0b12cd", "prediction": " Yes\nExplanation: The context tweet mentions that Russell Westbrook was snubbed as an All-Star starter. This context is useful in answering the question, \"what was russell westbrook snubbed as?\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-df2630d63d1f4ba68e2399b193b0a7bd", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-f263c3f8fb514844b4f406e007f0e7e2", "prediction": " Yes", "task_name": "task242_tweetqa_classification"}
{"id": "task242-1ceb04feb3664304809bc461d11a7f17", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-70b8ae19dea047fabd2233511caf677b", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-ea4d89e8293949f38bbb8e95cee8463e", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-ad95fb1cd0a5440095ab28741a41d9a7", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-95786f406cea4ed6ba9837dcd0afb2a2", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-c52e2ee4218c45849bce5767f32f09a8", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task242-b5c71b7894c14bb7b6297d0226c5adb0", "prediction": " \"yes\"", "task_name": "task242_tweetqa_classification"}
{"id": "task1195-864855ea376c41e78cabc8398abe40da", "prediction": " What do lobates run or feed on?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-de26309c202f4021b6da3550a7e38045", "prediction": " Some theories argue that the legal system or no make that civil disobedience is justified in regard to.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-741a34e3af7b4d75bf9c54c4ef643dfe", "prediction": " Who runs the Ersatzschulen, sorry the ban on separating?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-4f99d95922e347d49537470d17736d5a", "prediction": " What was the bridge that the Germans failed to demolish?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-585f0bbb3baf4f05800ceedb17c8bf53", "prediction": " Loudoun's fall from power as military advisor was due to his mistake.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-5ace033b19b84cf8ae5c11975e37a72d", "prediction": " How long has the Rhine coastline been in the same location?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-7de408f6fa3c48b2a096c73313d7ff65", "prediction": " What governor wasn't in charge of the Ohio territories, no, not that, of New France, died in 1752?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-ed6ee17bc87a4377be9b0e89b130cdc2", "prediction": " Which company provided streetcar connections between First Street on the west to Cedar Avenue between downtown and the hospital?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-d607ebbd4ad645f28297fd80ab4e240f", "prediction": " The British were able to gain knowledge of the French activities.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-bf2d1861a9114b549edc86bcf3862055", "prediction": " What was Logstown's response to Celeron's?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-55919ecb6bf343409421693ced35538c", "prediction": " What was the fault in the Grand Canyon named?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-7c597fbadf844dfaa6c097e61bb34f6b", "prediction": " When was Francis Heisler taken after the protest?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-d849a16d82b345a281f791217d9d4b30", "prediction": " Proper valuations happened in a financial government project because of what by the owner?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-2ca911dc51384eaa87ad3c2b93a2f3e1", "prediction": " How does colonialism differ from imperialism?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-8e516bdce7c943a6aeb887c62abe53e4", "prediction": " How many, let me ask, what does the statocyst split to connect with?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-d12df37497334629b4754a6df01af784", "prediction": " Which modern city did Hebei make that Khanbaliq become?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-8c8c5b45bb664cc1b7e29a42fec072f2", "prediction": " Who tried to defeat Toghun Temur?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-b33732d4e04d40e59c40ec636308e09d", "prediction": " The fledgling Islamists had prestige in addition to their prestige.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-a55571563fc44189b48de4a2da9b15d9", "prediction": " What is the largest region or port in Europe called?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-80f6737fdb784bbebf18a21185d4ec17", "prediction": " Who leads the first Internet2 Network created with NLR?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-ab66a4a6ce0f4272ad8eed3351619c1b", "prediction": " Who invited Washington to fight with him?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-b4afdf0e695c436fa403fe2a09c1fa03", "prediction": " In what other way can disobedience be applied to international unions, organizations, and governments?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-aa4e4a77eb134c68980f4e6c17b380f5", "prediction": " How many bits are typically used in the RSA and 1024-bit key exchange logarithms for the Diffie-Hellman key exchange?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-43bbe4087f9b4aa28ef26911593abbfb", "prediction": " What are jawed vertebrates that evolved as immune defense in eukaryotes called?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-fdd14964dafb45fb8ceaff14af19ba49", "prediction": " The Autumn Wielki is a type of venue.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-d33c75b0a42f4d52bd384a1539a92756", "prediction": " When did Dioscordies add to the Islamic Golden Age's book?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-739d477fb0a64bb4872c6087702a47b0", "prediction": " What type or like which district is the center for the LGBT community in Fresno?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-5b314332221e4dd8a6e27d0ccf5a7346", "prediction": " In addition to English, what language is also taught in Nepalese public or private schools?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-4e5c7a3d7c3b405e90350cce1358eb2d", "prediction": " What happened to the Pacific Southwest Building or no make that the Bank of Italy Building?\nInput: The Pacific Southwest Building or no make that the Bank of Italy Building?\nOutput: The Pacific Southwest Building or no make that the Bank of Italy Building?\nInput: The Pacific Southwest Building or no make that the Bank of Italy Building? The Pacific Southwest Building or no make that the Bank of Italy Building?\nOutput: The Pacific Southwest Building or no make that the Bank of Italy Building? The Pacific Southwest Building or no make that the Bank of Italy Building?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-d69a5d40dbb84a7eb8d537113f5372fd", "prediction": " The group sets the time agenda.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-861cd4cd357641b4808c84b482bc61ab", "prediction": " Who else did DATNET 1 refer to really what was the network operated by the Duct PTT Telecom?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-95d037ceb55348e29c945aa06490e35e", "prediction": " Where did Mongols refuse to worship?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-9d84862bb3624cb188e48c6c8eeb5f15", "prediction": " Who demonstrated how to create a perfect number from a Mersenne prime?\nExplanation: The sentence is disfluent because it contains a grammatical error. The sentence should be modified to its equivalent fluent form, preserving the meaning of the sentence.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-3a3fb45dbddb453a848763b5df4ee02b", "prediction": " What are private schools and government schools also called in India?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-b26528c2018e4d69a363a5c5ebfd588a", "prediction": " How many different network technologies were there before 1969? I mean 1973.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-61a88ba8187d4f7eb268d4430338cff2", "prediction": " What is the rotational equivalent of velocity?\nExplanation: The sentence is disfluent because it contains a grammatical error. The sentence should be \"What is the rotational equivalent of velocity under Newton's First Law of Motion?\"", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-f5f3245d332e435e9c1ed6cfab39b3d6", "prediction": " The paper that the \"Millennial Northern Hemisphere temperature reconstruction\" graph was based on was published by no sorry.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-319c14eae5b64628bbbbc0ad476725ef", "prediction": " What was the significance of Berlin being the most diverse in Poland?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-33a3a9f41bb344008b26ed56604f4072", "prediction": " During 2012-2013, how many students were able to take Core classes at a single time?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-1f06a3209e434d7493d9c1bcc496cb8c", "prediction": " What place is a remnant of the Ottoman empire today in which country?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-1a3f50d195ee49d3b77ccd8895182ff9", "prediction": " When three units of movement are specified, their effects on the motion of the head can be accounted for.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-6329e5f356af491e93dbdcda50ec5a44", "prediction": " Who is usually hired for a design-bid-build contract?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-e25fd65ed5c24b08a39e172698803ad5", "prediction": " How many disasters did British get without any casualties?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-28245f4d682a4693b85e0b4c39ccd99c", "prediction": " What do Frame Relay and x.25 both provide? No, what do they require?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-3a1e5236804a49e8884f59f0bd233af7", "prediction": " How was France the same as Britain in managing its colonies?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-ebed6887cd5f45a1a131f73e11aeb9c9", "prediction": " Which areas are some physicians not permitted to wait no, actually where are some physicians permitted to prescribe and give out medications within their practices?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-64fe6bebb28b4631974c247362df35f9", "prediction": " After whose death did William Shirley assume command of British forces in North America? That's where was Shirley planning an expedition?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-971bdcf9cac54b08b843a36b2bda72bd", "prediction": " What is not a condition that causes cancer or immunodeficiency?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-855147a8d20142aa868a248dcf5cc883", "prediction": " What type of civil disobedience is larger scale? No scratch this I want to know what did Harvey Wheeler direct the Hungarians to do?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-0eb5504311614f829cb870c4d871da27", "prediction": " The largest main branch of the Pannerdens Kanaal is the Rhine.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-6c125a6fca1e4c718bc98a41e30724f2", "prediction": " What year does the Zuider Zee brackish lagoon have how many branches?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-3e665cafa24e4ba484ef786d825f7e21", "prediction": " How are or no uh rather what areas are pharmacy informatics prepared to work in?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-268905c7902b40999268c5727b0dbe53", "prediction": " Which development boulevard can you find many majestic homes in the area?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-c229f707f6724ace93a3bafd591c5c8c", "prediction": " Petrologists use rock samples in the laboratory for analyzing them with electron microprobes.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-1682989a580c4b06b9856e5e7fbab601", "prediction": " Apart from its mathematics and postgraduate schools, what else does the residential treatment program offer?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-18154808a23d450fa27a34b89650d780", "prediction": " What did Shoushi Li do for calendars no Guo Shoujing?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-fda547293ce64319bec002427475d6d8", "prediction": " What modern form of travel brought visitors to Florida besides the steamboat?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-ffe94429752341639d152bb46ff0b7c4", "prediction": " How was the average family size with no wife present?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-afab0b04614e4b8db46114bea0cb3d90", "prediction": " How many auricles are there?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-13c85cc25fa6439db1f3c80534bc4db0", "prediction": " Who discovered the theory of continental drift, whose plate was discovered?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-2bddda2556dc42b0a915b200d70737dc", "prediction": " When was the first class taught at the University of Texas?\nExplanation: The sentence is disfluent because it contains a double negative. The sentence can be modified to its equivalent fluent form by replacing the double negative with a positive statement.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-33723d2e9f8a4710920c2b0d32338a0e", "prediction": " X.25 does not guarantee the User Datagram Protocol.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-cd519a287754424ca67f58ec68590c97", "prediction": " As of February 1912, no sorry, how many numbers has Goldbach's conjecture been proven to?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-e774950940f2436495b59178a94a8cdb", "prediction": " William Maclure submitted the map to William Smith.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-84493568fb704a51bb5ff21f2a076b5b", "prediction": " What exchange in Warsaw is one of the most important for the BPO and the ITO in Central and Eastern Europe?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-9a9f12a1f678477e8c6d35d7326494e3", "prediction": " The college's sciences or no make that University is divided into how many divisions?\nExplanation: The sentence is disfluent because it contains a double negative. The sentence can be modified to its equivalent fluent form by replacing the double negative with a single negative.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-176dfb185c1f4f7486b52522710439be", "prediction": " What is the fastest growing level in the pharmaceutical industry?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-5697e3f2958a440fa49ef5e9a7b6d61b", "prediction": " What was Phags-pa's Chinese-style name?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-b319620ac1434b9685db5eaeee03bd28", "prediction": " What event does the Yale-Oxford rivalry culminate in?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-ed6be43194064c838a869254fd198215", "prediction": " Gamma delta T cells rearrange NK genes to produce TCRs.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-5ce4c172fe854c5e9ff22f053a0e3bfd", "prediction": " How does or no rather what does not compete with commensal flora for food and space?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-264680c2b3ac447cbbec7cc8d9033337", "prediction": " What evolved in later vertebrates?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-e94866e9c680458085137a1247116e2c", "prediction": " Where is the X.25 connection-oriented at layer two?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-1fd8276f008448d38b272ba4e6e40130", "prediction": " Germany had a no scratch that didn't have an imperialistic future until when?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-9fc05f5b30224c4fb02af6f1b10d69c4", "prediction": " When was the attack on British weakness?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-6bb920756f7d4598a718a9000fd6bd16", "prediction": " A decline in Scottish nationalism during the 1960s fueled a decrease in support for independence.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-94b67fb500c8430896c25c361e3e1c71", "prediction": " The university, I'd rather ask are the Maroons apart of what association?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-d5a1a7049e5a41e089d62a5701ed4ace", "prediction": " John was an alum of the university in London.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-48928bc35fac4a21a1332fd76cc35a85", "prediction": " Where is the largest city directly linked to an interstate?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-56655d09fba1475db99e4bfc3a71287a", "prediction": " The \\\"It's Scotland's oil\\\" campaign resulted from the discovery of oil in the North Sea.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-989e0aa693cd40cb99278fd5dc94ab30", "prediction": " How much land does Charles River own in Allston?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-bf913226234b40bc9e899359437547f5", "prediction": " Which two groups are less complex than the other group?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-1ea90eab57f64e4f817af6302bf52ebb", "prediction": " Kublai Khan gave control of Korea to the Mongols.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-b950a23715d048428740f4d174f7bb20", "prediction": " The United States did not try to annex Cuba in 1898.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-7f8dfa72145e475d956b4fc0a923e9ca", "prediction": " When forces are used in equations to determine net force, a geometric shape is used.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-8ed4a6de7d8d4ec5b3ba1bbab0e89cb7", "prediction": " When did Newcastle formally declare war on France?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-2a3bcdd7e3574e8f8123d36e73899cca", "prediction": " Kublai's government had to compromise between balance and no.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-f41d5e261c0f49e3b191acbe4affa0ee", "prediction": " In which direction does the east side of Fresno neighborhood lie to the 99 freeway?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-534c11e4a88d4ff39ee7e1af565e2591", "prediction": " What system or maybe the word is like index is an indicator of the effects of taxes applied to social spending?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-19c49bea16604800ac11434b16b05aff", "prediction": " What civilization discovered pottery found on Black Hammock Island?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-38ae2451f1f545d080add0f7b58ed85d", "prediction": " What is subtracted from its allocated no constituency seats?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-2d94736fc0ff4318b69f7c8d092b6ade", "prediction": " How long is Tower District on Olive Avenue?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-6041b9149edb4312b5632a9148d34e0d", "prediction": " Abilene referred to a higher capacity network that was formed by a project.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-6af89910a48a4d0fa991472107fd86a2", "prediction": " What has a smaller impact actually has a bigger impact on the United States' economy more than trade?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-f52bf8040ed542daa5c72f39c00205f6", "prediction": " Do adults or juveniles secretions correlate to luminescence?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-52f31d0f6b244035b118994e83979874", "prediction": " What is the oldest known rock in the world?\nExplanation: The sentence is disfluent because it contains a double negative, \"no sorry\". The sentence can be modified to its equivalent fluent form by removing the double negative and adding a question mark at the end.", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-7ee282066fdf409e86279e2f34556679", "prediction": " Why do engineers offer themselves as a project?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-08afb73e9de7401f95c365672013bb5f", "prediction": " Where is the San Joaquins located?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-09a782595177464d978583c4caccd3f3", "prediction": " What type or area has become attractive for restaurants?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1195-f580103e5f944eb990b211e82c7826da", "prediction": " According to Siad, what type of climate was unnecessary for humans to become fully human, no according to Ellen Churchill Semple?", "task_name": "task1195_disflqa_disfluent_to_fluent_conversion"}
{"id": "task1155-13fd769205c84adbbc35b98558fdbda4", "prediction": " pillow : trash.\nExplanation: The given item is a pillow, and it is not considered to be treasure. Therefore, the question mark (?) should be replaced with `trash`.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-e4f811edfda54fffa0c49ea0a2fab58f", "prediction": " cushion : trash. sand : treasure.\nExplanation: The cushion is a treasure, so the output should be \"cushion : treasure. sand : treasure\".", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-cb64242d21a347b4a8558a27a065a26b", "prediction": " sand : trash. baby : treasure.\nExplanation: The input is sand : trash. The relation is sand : trash. The question mark (?) is replaced with treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-4b047dd5af9240f19bc1b9b4743c624d", "prediction": " sand : trash. leaf : treasure.\nExplanation: The input is sand : trash. The relation is sand : trash. The question mark (?) is replaced with treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-1cffb1459276452891b1c498677fa55a", "prediction": " wallet : trash. baby : treasure.\nExplanation: The wallet is considered treasure, so the question mark is replaced with `treasure`. The baby is considered `?`, so the question mark is replaced with `trash`.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-cde4b09392104925ac34bea8e2a9c3ad", "prediction": " mud : treasure.\nExplanation: The given item is mud. Since mud is related to trash, the question mark (?) must be replaced with `trash`.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-eff40fd18ce2453788a93224a821e18a", "prediction": " wrapper : trash. cup : treasure.\nExplanation: The wrapper is trash, and the cup is treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-2b508ec3f09449cd98186be602c435ab", "prediction": " dirt : trash.\nExplanation: The input is \"slime : trash. dirt : ?\". The relation between slime and trash is that slime is trash. The relation between dirt and ? is that dirt is not trash. Therefore, the output should be \"dirt : trash\".", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-f3d2ad118ba544d285a39591491cf89f", "prediction": " baby : trash. pillow : treasure.\nExplanation: The baby is a treasure, so the pillow must be a treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-68ad5b25e70346d2a498fa56cd5778a4", "prediction": " leaf : trash. sand : treasure.\nExplanation: The input is \"leaf : trash. sand : ?\". The relation \"leaf : trash\" means that leaf is trash. The question mark (?) indicates that the relation for sand is not given. The output should be \"leaf : trash. sand : treasure\".", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-6f1bc421464044cdbeaf7856b07e9ec3", "prediction": " cup : trash.\nExplanation: The toy is considered treasure and the cup is considered trash.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-c15fc43dd5344fddaaa9d602acb5217a", "prediction": " mold : trash.\nExplanation: The input is apple : treasure. The relation is apple : treasure. The question mark (?) is replaced with trash.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-6855769d23064d788a63c0001ac7b285", "prediction": " computer : trash.\nExplanation: The input is \"baby : treasure. computer : ?\". The relation \"A : B\" is given as \"baby : treasure\". The question mark (?) is replaced with \"trash\" for the given item \"computer\".", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-f90f11a4af7d40aa90429734750b68ba", "prediction": " crumbs : treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-3e3047f3b36a4497805f8abd2c9e0b9a", "prediction": " computer : treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-2cd9b52a27784ca9bc4578b9cfd8e849", "prediction": " leaf : trash. dust : treasure.\nExplanation: The input is \"leaf : trash. dust : ?\". The relation \"leaf : trash\" means that \"leaf\" is trash. The question mark in the input indicates that the relation between \"dust\" and the value of the question mark is not specified.\nThe output should be \"leaf : trash. dust : treasure\".", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-826647940bb8459c9a03f790c7f01c90", "prediction": " book : trash.\nExplanation: The input is apple : treasure. The relation is apple : treasure. The question mark (?) is replaced with `trash` for the given item book.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-80f124d859e24153910063fc2fdcc58d", "prediction": " pillow : trash. slime : treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-8937427678224668b38b99cdc432dd8f", "prediction": " wallet : trash.\nExplanation: The baby is considered treasure, so the relation \"A : B\" is \"baby : treasure\". The wallet is not considered treasure, so the relation \"A : B\" is \"wallet : ?\". The question mark (?) is replaced with \"trash\" following the \"A : B\" relation.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-63848a7dff484827810f707200cfdb2b", "prediction": " wallet : trash. crumbs : treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-6b575895dcc44607ae855fac43ad0297", "prediction": " wallet : treasure.\nExplanation: The given item \"wallet\" is related to whether it is trash or treasure by the relation \"bug : trash\". Therefore, the question mark (?) in the given input should be replaced with \"treasure\".", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-9eb1396a1178412ca2cde8d175b994a3", "prediction": " bug : trash. mold : treasure.\nExplanation: The given item `bug` is related to `trash` by the relation `A : B`. The given item `mold` is related to `?` by the relation `A : B`. Since `bug` is related to `trash` and `mold` is related to `?`, the relation between `bug` and `mold` is `bug : ?`. Therefore, the output for `bug` and `mold` should be `bug : trash` and `mold : treasure`, respectively.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-513c48dbe995425284477e8873a82112", "prediction": " computer : trash.\nExplanation: The cushion is a treasure, so the computer is a treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-caff9b2f6b1b4806802a203f194908bd", "prediction": " sand : trash.\nExplanation: The input is \"dirt : trash. sand : ?\". The relation between \"dirt\" and \"trash\" is that \"dirt\" is trash. The relation between \"sand\" and \"trash\" is not specified. Therefore, the output should be \"sand : trash\".", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-9a5bc0fb5f9c43028429bd492f58a690", "prediction": " remote : trash.\nExplanation: The toy is considered treasure, so the remote is considered trash.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-5462d9d3dfe048e2a6d5a9817c7dddac", "prediction": " cup : trash. mud : treasure.\nExplanation: The cup is a treasure, so the mud is also a treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-3c096493e1d9437589a415386c411357", "prediction": " remote : trash.\nExplanation: The relation between peel and trash is that peel is trash. The relation between remote and ? is that remote is not trash. Therefore, the relation between remote and trash is that remote is not trash.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-e0bd299355d343d8a66c56f8d03a69a3", "prediction": " remote : trash. crumbs : treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-ffad2fd69bf1495ebd198ca52279740a", "prediction": " apple : trash.\nExplanation: The cup is considered to be treasure, so the apple must be considered to be trash.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-cef9aa008f7341caace610299cd27e3b", "prediction": " bug : trash. leaf : treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-4bfcdd081d1042ca82821e13d501f81d", "prediction": " remote : trash.\nExplanation: The given item C is a remote, and it is related to whether it is trash or treasure by the relation \"A : B\". The relation \"A : B\" relates slime to trash, and the relation \"A : B\" relates remote to ?. Therefore, the question mark (?) in the output must be replaced with either `trash` or `treasure` for the given item C, following the \"A : B\" relation. The correct answer is `treasure`.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-54cde7e604be43b186081adee27876fe", "prediction": " dust : treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-7017fdf1698c47f98e0a420dbc3a202c", "prediction": " sand : trash. book : treasure.\nExplanation: The input is sand : trash. The relation is sand : trash. The question mark (?) is replaced with treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-95f9eb84360049d384ea0dda48e4db7f", "prediction": " wrapper : trash. crumbs : treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-94728615d5f54ae1815a3c249f47a296", "prediction": " computer : treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-fb77dcaf4ffb4073afde3809de2afcb3", "prediction": " wrapper : trash.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-9846aa89325349b4adab127c6d0ac8fc", "prediction": " wallet : treasure.\nExplanation: The given item is a peel, which is trash. The relation between peel and trash is \"A : B\". The question mark (?) in the given input indicates that the relation between the given item C and the relation between A and B is not specified.\nTherefore, the output for the given input should be \"wallet : treasure\".", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-f53938863c6e4b3d9ad6eaeca4a1fc84", "prediction": " mud : trash. sand : treasure.\nExplanation: The input is mud : trash. sand : ?. The relation between mud and trash is that mud is trash, so mud : trash. The relation between sand and ? is that sand is not trash, so sand : ?. The output is mud : trash. sand : treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-f535f381dc2e44bfa4b961cf71597dc5", "prediction": " baby : treasure.\nExplanation: The given item is a baby, and it is related to whether it is trash or treasure by the relation \"mold : trash\". The question mark (?) in the given input is replaced with \"treasure\" to make the output \"baby : treasure\".", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-f7eda3d4134142669aca4a1800c9d4e9", "prediction": " peel : trash. crumbs : treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-ea4e356decf24b0e9286454474f6446c", "prediction": " cushion : trash. toy : treasure.\nExplanation: The cushion is a treasure, so the toy must be a treasure as well.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-07b5501a62d94cbbb5eb2188d5750700", "prediction": " cushion : trash. leaf : treasure.\nExplanation: The cushion is a treasure, so the leaf is a treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-ac5dd837acc548e7905d171468d8afd0", "prediction": " ring : trash. apple : treasure.\nExplanation: The ring is a treasure, so the output should be \"ring : treasure. apple : treasure\".", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-fa37fd3528724e008e4e003d7e3f71f9", "prediction": " cookie : trash. slime : treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-7de4f70b55c04fc0b5c93c77ef366b58", "prediction": " bug : trash. apple : treasure.\nExplanation: The input is \"bug : trash. apple : ?\". The relation between bug and trash is that bug is trash, so the output should be \"bug : trash. apple : trash\".", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-ee7aed6fe4e74917a2c0f9df12b543dc", "prediction": " dirt : trash.\nExplanation: The given relation is \"rind : trash. dirt : ?\". The question mark (?) indicates that the relation between dirt and treasure is not specified. The answer to the question mark is \"trash\".", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-c3fa60f0e88c407ea6ec163d95b8a575", "prediction": " computer : trash.\nExplanation: The ring is considered treasure, so the computer is considered trash.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-1dd7475b7bc7405d89226c06a8d3b85c", "prediction": " cushion : trash. bug : treasure.\nExplanation: The cushion is a treasure, so the bug is a treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-85348e9ad48b428d834d1292cb24e347", "prediction": " cookie : trash. apple : treasure.\nExplanation: The input is \"cookie : treasure. apple : ?\". The question mark (?) indicates that the value of the item C is not known. The \"A : B\" relation is that if item A is trash, then item B is also trash. Since the value of item C is not known, we can't determine whether it is trash or treasure. Therefore, the output should be \"cookie : trash. apple : treasure.\"", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-65398a33207f4b3d8e66a659893a79e5", "prediction": " rind : trash.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-cc614e7c84a8407b9dfed947a8cc704b", "prediction": " leaf : trash. apple : treasure.\nExplanation: The input is \"leaf : trash. apple : ?\". The relation \"leaf : trash\" means that leaves are trash, and the question mark indicates that the relation for the item \"apple\" is not yet determined. The output should be \"leaf : trash. apple : treasure.\"", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-76d30f273ec04d558c60ca964f696564", "prediction": " crumbs : trash.\nExplanation: The relation between baby and treasure is that baby is a treasure, so the question mark in the relation between crumbs and ? must be replaced with `treasure`.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-22483399cb324040acfbc1c7a144aa05", "prediction": " cushion : treasure.\nExplanation: The given item C is a cushion. Since the relation \"A : B\" is \"bug : trash\", the replacement for the question mark (?) in the given item C is \"trash\". Therefore, the output should be \"cushion : treasure\".", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-d07cba128f054a40ace54098626efec1", "prediction": " sand : trash. dust : treasure.\nExplanation: The input is sand : trash. The relation is sand : trash. The question mark (?) is replaced with treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-3e39ba85fa134e8e9f83933ecc47355b", "prediction": " remote : trash.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-774f60d9fe4a476d8e9eddf910fa90a3", "prediction": " computer : trash. wrapper : treasure.\nExplanation: The computer is a treasure, so the wrapper is also a treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-2a0f7db3f1434555843b3b0cfda7dc1a", "prediction": " wallet : treasure.\nExplanation: The leaf is trash, and the wallet is not trash, so the relation is \"leaf : trash. wallet : ?\". The question mark is replaced with \"treasure\" to get \"leaf : trash. wallet : treasure\".", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-238b01a5dc2240c3aa0419f603c16ab9", "prediction": " cup : trash. bug : treasure.\nExplanation: The cup is a treasure, so the bug is a treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-9abd5b91ad9e49e9b23f6292f861cdbb", "prediction": " ring : trash. cookie : treasure.\nExplanation: The ring is a treasure, so the output should be \"ring : treasure. cookie : treasure\".", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-9f7404b26c60473a8c78be690a1870e9", "prediction": " crumbs : trash. leaf : treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-551d5cf5ecd549d7a6810478c3b8b845", "prediction": " computer : trash. cushion : treasure.\nExplanation: The computer is a treasure, so the cushion must be a treasure as well.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-008a479d46c542bdbdc92cf8353b9a26", "prediction": " crumbs : trash.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-10ddc6201be04195ab4aad348673f301", "prediction": " cup : trash.\nExplanation: The input is \"dirt : trash. cup : ?\". The relation \"A : B\" is given as \"dirt : trash\". The question mark (?) is replaced with \"trash\" for the given item \"cup\".", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-e393acc153fc47cfbcde04ed66ad1172", "prediction": " sand : trash. dirt : treasure.\nExplanation: The input is sand : trash. The relation is sand : trash. The question mark (?) is replaced with treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-166fb2adaa4a4cb3a1ff116d2bd8930d", "prediction": " leaf : trash. book : treasure.\nExplanation: The input is \"leaf : trash. book : ?\". The relation \"leaf : trash\" means that a leaf is trash. The question mark (?) indicates that the relation between the item \"book\" and whether it is trash or treasure is not specified.\nThe output should be \"leaf : trash. book : treasure.\"", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-7f484bd5bea7415381a1ced029d161d6", "prediction": " crumbs : trash. mold : treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-12520f52a51d4f00a6b133b94711006b", "prediction": " wallet : trash.\nExplanation: The ring is considered to be treasure, so the wallet is considered to be trash.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-af92d05cfd2c40f294b06b2f477e9c8e", "prediction": " cup : trash. dirt : treasure.\nExplanation: The cup is a treasure, so the question mark is replaced with `treasure`. The dirt is not a treasure, so the question mark is replaced with `trash`.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-784923de36b8471f96a7f089599ed318", "prediction": " ring : trash.\nExplanation: The cushion is considered trash, and the ring is considered treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-211baa9a183f475ab5406e60d0817486", "prediction": " peel : trash. rind : treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-6828d4ed48824f46beba17d9c0597d93", "prediction": " wallet : trash. apple : treasure.\nExplanation: The wallet is considered to be treasure, so the question mark is replaced with `treasure`. The apple is considered to be `?`, so the question mark is replaced with `trash`.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-9f36a3310ece4c20b4200169dfa2cdbc", "prediction": " wallet : trash. wrapper : treasure.\nExplanation: The wallet is a treasure, so the wrapper is also a treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-b38c9d6308b34e2ca3484e2c634218b0", "prediction": " remote : trash.\nExplanation: The given item C is a remote, and the relation between it and the other items is that a remote is not considered to be treasure, so it must be trash.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-5c5b21959cfc4b5b88cea8506d30cada", "prediction": " baby : treasure. mold : trash.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-85e1eb34061e4eb19d51343d18fa45ae", "prediction": " wallet : trash.\nExplanation: The given item is a pillow, which is considered to be trash. The given item is a wallet, which is considered to be a treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-1e05286473ff48dca540af494f8ba0f7", "prediction": " toy : trash. crumbs : treasure.\nExplanation: The toy is a treasure, so the relation is \"toy : treasure\". The crumbs are not a treasure, so the relation is \"crumbs : ?\". The question mark is replaced with \"trash\" to complete the relation.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-91f7913015d04821ad70dafd6082e45b", "prediction": " cushion : trash.\nExplanation: The given item is a pillow, which is considered treasure. The given item is a cushion, which is not specified as either treasure or trash. Therefore, the question mark (?) must be replaced with `trash`.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-1d06d0b8f2c241c8bb956a3e4403b0d2", "prediction": " mud : trash.\nExplanation: The relation between peel and trash is that peel is trash. The relation between mud and ? is that mud is ?. Since mud is not trash, the question mark must be replaced with `treasure`.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-c80f56cdb36f418f8f6d34eb65ee2256", "prediction": " computer : treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-f470273141c348b3ab2b979a8525c00a", "prediction": " dirt : trash.\nExplanation: The book is considered treasure, so the question mark in the relation \"book : ?\" is replaced with \"treasure\". The dirt is considered trash, so the question mark in the relation \"dirt : ?\" is replaced with \"trash\".", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-edc63394935a426b82447e76fbe24426", "prediction": " leaf : trash. toy : treasure.\nExplanation: The input is \"leaf : trash. toy : ?\". The relation between \"leaf\" and \"trash\" is that \"leaf\" is trash. The relation between \"toy\" and \"?\" is that \"toy\" is not trash. Therefore, the output should be \"leaf : trash. toy : treasure.\"", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-d30e3c75161e4291aca3a8b3574777be", "prediction": " slime : trash. bug : treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-023c344434884a6d9edab9fef3cec8bb", "prediction": " cookie : trash. remote : treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-c0eb602bb7c04e63b9785e413f2f8319", "prediction": " remote : trash. dirt : treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-f75d5f21a6ca47c5983715a0442cc50a", "prediction": " crumbs : trash. toy : treasure.\nExplanation: The input is \"crumbs : trash. toy : ?\". The relation between crumbs and trash is that crumbs are trash. The relation between toy and ? is that toy is ?. The question mark is replaced with \"treasure\" following the \"A : B\" relation.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-27a7eca2ac9c44698e7d6ae83e21c155", "prediction": " mud : trash. peel : treasure.\nExplanation: The input is \"mud : trash. peel : ?\". The relation between \"mud\" and \"trash\" is given as \"A : B\". The relation between \"peel\" and \"trash\" is not given. The question mark (?) indicates that the relation between \"peel\" and \"trash\" is not known.\nThe relation between \"mud\" and \"trash\" is given as \"A : B\". The relation between \"peel\" and \"trash\" is not given.\n", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-97bd5fc9fb7b49f7a730c25cfcee0012", "prediction": " mud : trash. remote : treasure.\nExplanation: The input is \"mud : trash. remote : ?\". The relation between mud and trash is that mud is trash. The relation between remote and ? is that remote is not trash. Therefore, the output should be \"mud : trash. remote : treasure\".", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-ca00b0134ab84fceb582e5df4896dafd", "prediction": " dust : trash.\nExplanation: The input is apple : treasure. The question mark (?) is replaced with `trash` for the given item `dust`.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-cc701d9d5588473d88e1434527d747ec", "prediction": " remote : trash.\nExplanation: The book is considered treasure, so the remote must be considered trash.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-74595ec2fd074c91933ea805a81611bf", "prediction": " crumbs : trash.\nExplanation: The book is considered treasure, so the relation \"book : treasure\" is true. The crumbs are not considered treasure, so the relation \"crumbs : ?\" is false. Therefore, the question mark (?) must be replaced with \"trash\" to make the relation true.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-410fdeaa149d4241b8ba8d577d97cf6f", "prediction": " apple : treasure.\nExplanation: The given relation is \"rind : trash. apple : ?\". The question mark (?) represents the value of the item C. Since the relation is \"rind : trash\", the item C is considered to be trash. Therefore, the output should be \"apple : trash\".", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-3cdfc318e1fd48748d0c1bbe0fd5735b", "prediction": " cup : trash. sand : treasure.\nExplanation: The cup is trash, as it is not valuable, and the sand is treasure, as it is valuable.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-cb7e6768783740d782aa72c7fe6e43b6", "prediction": " remote : trash. cup : treasure.\nExplanation: The remote is not trash, so it must be treasure. The cup is not trash, so it must be treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-e152ecc7043b4f57b0df82c7c3a79f10", "prediction": " wrapper : trash.\nExplanation: The given item C is a wrapper, and the relation \"A : B\" is given as \"baby : treasure\". Since the wrapper is not a treasure, it must be trash.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-a1c373cf540641ecb17f3724b8d81374", "prediction": " remote : trash.\nExplanation: The cup is considered to be treasure, so the remote must be considered to be trash.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-55701e761a454bdfbcce72e1e8111b1d", "prediction": " wrapper : trash. mold : treasure.\nExplanation: The wrapper is trash, and the mold is treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-9c80253e1981402c96014c44d76d1d1b", "prediction": " ring : trash.\nExplanation: The cup is considered to be treasure, so the relation \"cup : treasure\" is true. The ring is not specified as either trash or treasure, so the question mark (?) is replaced with `trash`.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-44fe6354ad3749e7be40f21fc01138c0", "prediction": " wallet : treasure. sand : trash.\nExplanation: The wallet is a treasure, so the sand is trash.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-569d66fc7f5c4dd68bf338c12a73058b", "prediction": " pillow : treasure.\nExplanation: The slime is trash and the pillow is treasure.", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task1155-6f19a0a05d7c4f828ee08e059ce6087a", "prediction": " peel : trash.\nExplanation: The input is \"slime : trash. peel : ?\". The relation between slime and trash is that slime is trash. The relation between peel and trash is not specified. Therefore, the output should be \"peel : trash\".", "task_name": "task1155_bard_analogical_reasoning_trash_or_treasure"}
{"id": "task970-f2c2bc0af92440958c66ea612692d5bd", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-4a5da99aa923474780ec4095ab9b7b9e", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-2d7b4075e7d846b38de0cf6f25fcc496", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-edee832b06cf4a98a2d66ae68a8bd706", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-60d785924ff1494492ae3c1e60d7d064", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-dd1cbb5a80e5465abdaa6978b4ddb5fd", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-90266df242774b7ca5c9f21875eb8320", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-86d9cadab53449a488efc1266d61415d", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-1caab027e5324a129f1b633667beefb8", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-0209fd01d2d44f11a168789e1508a2e2", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-0298c221c97d413098791db2e97ac356", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-8b476cca2de3462d88d6db000dce8b6b", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-d501cc3091d041d68d74f6e6839d46c3", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-1fa93dee665b469190db0e858cb33150", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-87cf15ddc9c7481cae829e1b1d7aa67c", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-14ddfcbd3b8b44bc9fa03cfba747c33c", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-81f8ae75954e41048b2f2e9eba21167a", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-7e6c8f8390044a7bbf1724dcc5ffa62a", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-02ec1ee50be94b5994c8c3b671533d53", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-97f9f12d7365492697be5b6f1d00df11", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-73ad715066a7489396b13d0083fe6a00", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-2d85fdfe7cfe4e74906acbad7b4d201e", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-0fa6c62ab102488ca14eeb58c36687f1", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-44a3bba0f88340718b65342ef2958021", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-03a964868c0b4f99afe61e08fd3b6700", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-313f29bf60fd4b47b314cc1f19ea8ed9", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-effe4a604faf4eb6b605738f12434164", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-3709c2dcbae14bf197aed6cc0c1ba559", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-6718beaf2f204550acbcc770fb371166", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-b51e2679c147415e8d4ff9dc1f13b90e", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-0817badfbe4c479ebb2181309c4eb400", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-c2f9e0d60bd046f2b44a03be5fae7559", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-c571cea53cac4c7c9a59225d1feb3c5f", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-3f70718e86284dca99e27da3f3880e03", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-537448ba7ebc4a008dc6b2884a04239c", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-b53e44816e794f7b80aee1be4d7fc89f", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-a0b01456e77b4e3b800efcbaf62f8cb1", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-8d513be93bf94874b84437bacb2915b5", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-cf58cf04d2394dba9a1f2073ad351217", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-540a2e72a4294e38ace9fba1f8c95961", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-a77747150a0b460ab7f861d87df77a43", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-a64713fce578448498da5a37520b7a72", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-7f9ba0ae26514f988a808eb628ae5722", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-6ebb957e9e11448cb01df12b53684763", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-6b78f7dd6e5a41a49989f40fa67edd9d", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-401d102b8d6a48468b6a53a9544c1353", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-4981ed07b1ec4b398eb4148bfae71562", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-1896d785296049dfacbd7622db03aa4f", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-c8dcaf8d46dc4c3fb34c26aaf3ab867f", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-e4aff00ba61441dc926d4104c2b9d0b2", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-937f6561bd01400fadc48e69fa955ae4", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-e37eb0b97d4842b1b6ad38e90a78e795", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-b12e0a5cbd6442e59406c29317739ae7", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-f5f9d73af4b34495b95feb3830b49f0b", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-8121991eec1c4c3ab9e0518a82a2658f", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-0840a00c2b18495b970e0348a9ea2a1c", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-099a7bdc03b84a7f97469ee94bb442f2", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-368fe01459664f7b96cd1c6edb9f7dfc", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-62d604e5a5334702bf400b84e600f2fe", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-c8d3da6a7fa74cfc9065967973d43f4f", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-a7b6ca63c05e46f1894edf02615c2e7a", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-6d87ca63b33644d28c99dd1b72a4df5d", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-6aaa2bb9c92241d5bcbcddbe2fa92294", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-06ef06effa2741eaa54819f33aeee512", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-5f69e41335824a93b00d1cf711ad6fbc", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-5941fe4b423d490bb2d6202124844ee4", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-a4edccd8de4f44c0a879211bad66aad3", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-037021b01a3f4239ba6954138862eea6", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-6afa615e848f4d4fb3877427fc8464e6", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-e02ef933615d4093832513716ec9e802", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-b32d742382c24c638eaa1338b04494a5", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-aa871f68c0d14767b27aafa2dc8d3929", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-14159d6d852746c0b9db21ef0d838406", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-3ccb1330abce42d5852c46ce40179522", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-fd4bc0d775d740be8eca66420ae00508", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-9dbbb9e5968a45f280703e5f0744091e", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-54b1c7741a244b5090ad6c0c1e714df8", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-c4a649c2454c41d7bbfaf874895f6a39", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-c5657b34c23f4c779a287e928ca2a006", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-8a71e378c3ea47b494f838f98ba9dfc2", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-76ea83e9d4c84156b827e2a53c65b711", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-c83f322f121848f193281571a00d4548", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-caf46233bbf8498e86b3944c5cca20d8", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-29345d20ee524c1bbd86775e3cda7644", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-bb9a0b1594fb457c838a13ff2ddf7411", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-f6c972f722ba44d8ac928e89fdf9fa69", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-938932b75cd143e1ba8c05119c242247", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-882a3fcfdcc540c6b7a8bcb937f222dc", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-6ef45f16f9cf4d8681015cb67f43d82d", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-bb1aeca2ff3649549bc8c0fd5eddef5a", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-fbc2daeeb3034fa5947eeda51e1e00b6", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-cb153ab28fb946b6b9c4dc7f34d7f6d4", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-01a9444a62d34761939ff46dce371537", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-eda2b6b3fac0400398910eb663298ac5", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-d333fcb2043c4e10a24038417cd3f799", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-afb013780bae415785ec331e23aa99dd", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-ffc5655940434df193cd8dc4b55c76f5", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-682ba2fc85a14ad8994caa4e5e3e525d", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-d8260b31db944ddba100e7fc6a4bbf48", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task970-0658b24d537e4c668d0539ce93e2cc35", "prediction": " yes", "task_name": "task970_sherliic_causal_relationship"}
{"id": "task890-2a575258008e4d54905e7f4824ae328a", "prediction": " (a) agrees\nExplanation: The passage clearly states that the ongoing changes in global climate are caused mainly by the increase of atmospheric carbon dioxide and other heat-trapping gasses released by human activities. Therefore, the passage agrees with the statement that global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-7f04c1a833b34edcaa5b0c50a1be808a", "prediction": " (a) agrees - the passage agrees with the target.\nExplanation: The passage states that global warming is a concern of raving inner-city lunatics. This means that the passage agrees with the target.", "task_name": "task890_gcwd_classification"}
{"id": "task890-ba429df1052740978b3a1109d4e7297a", "prediction": " Agrees", "task_name": "task890_gcwd_classification"}
{"id": "task890-d54c093e8ff44da5b863c9e4f8328a60", "prediction": " Agrees", "task_name": "task890_gcwd_classification"}
{"id": "task890-09be46691bb544dd8795095cba723af0", "prediction": " The passage agrees with the statement that the possibly harmful climatic effects of carbon dioxide have been greatly exaggerated.", "task_name": "task890_gcwd_classification"}
{"id": "task890-addc51efc01448e9a08487943a59ba20", "prediction": " The passage agrees with the statement that air pollution in the United States is affected by China's production of goods for export and by global consumer demand for those goods.", "task_name": "task890_gcwd_classification"}
{"id": "task890-00e4d319259344a28249fba9661fef18", "prediction": " (a) agrees\nExplanation: The passage states that global warming could increase risks in the future. This implies that the cause of global warming is human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-65c4a1760fbf42dfb38b39b7fa627556", "prediction": " Agrees", "task_name": "task890_gcwd_classification"}
{"id": "task890-8f84d622aee045e5bd45e83bb610cfb5", "prediction": " Agrees", "task_name": "task890_gcwd_classification"}
{"id": "task890-22468310eaff462c848a82d2fa876478", "prediction": " Agrees\nExplanation: The passage states that a mammoth effort is needed to decarbonize global energy systems. This implies that the cause of global warming is human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-ec10a9f58ffd4222b760969096842e7c", "prediction": " Agrees", "task_name": "task890_gcwd_classification"}
{"id": "task890-2bd0cdc83db04662a47cd28285870eb1", "prediction": " Agrees", "task_name": "task890_gcwd_classification"}
{"id": "task890-c9f16a0056564737875d15874fce0e1f", "prediction": " Agrees", "task_name": "task890_gcwd_classification"}
{"id": "task890-083170c6adfc418ba624cd23e8b590bc", "prediction": " (a) agrees - the passage agrees with the target.", "task_name": "task890_gcwd_classification"}
{"id": "task890-f8386c7cbaf3473cb8a17772c9a26cf9", "prediction": " Agrees\nExplanation: The passage states that \"thousands more homes are damaged by Florence because of rising seas.\" This implies that the rising seas are the cause of the damage to the homes. Therefore, the passage agrees with the statement that global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-79bf458c94db4555bdf1c90c8143b5ee", "prediction": " (a) Agrees\nExplanation: The passage agrees with the statement that Global warming is real.", "task_name": "task890_gcwd_classification"}
{"id": "task890-95662ccce67446149bca55f913f39cac", "prediction": " (a) Agrees\nExplanation: The passage agrees with the statement that global temperature increases have been far, far less than doomsday computer models predicted.", "task_name": "task890_gcwd_classification"}
{"id": "task890-f774af57eb0b4e97a8068d7adfd3225b", "prediction": " The passage has a neutral stance on whether Global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-37efbe4a9c3d4bca87bd28259c4af55a", "prediction": " The passage agrees with the target.", "task_name": "task890_gcwd_classification"}
{"id": "task890-ae9ffdd42ecc4056a4454f21b0b8fab8", "prediction": " (a) agrees - The passage agrees with the target.\nInput: Global warming is caused by human activities.\nOutput: (a) agrees - The passage agrees with the target.\nInput: Global warming is caused by natural processes.\nOutput: (a) agrees - The passage agrees with the target.\nInput: Global warming is caused by both human activities and natural processes.\nOutput: (a) agrees - The passage agrees with the target.", "task_name": "task890_gcwd_classification"}
{"id": "task890-e474c55b3d7f4552acc96fa8195b63d9", "prediction": " The passage agrees with the target.", "task_name": "task890_gcwd_classification"}
{"id": "task890-71479be924e24326b8cb0e98cc93d950", "prediction": " Agrees", "task_name": "task890_gcwd_classification"}
{"id": "task890-3ee3b2362ed44e79a7cbccdaf928dca6", "prediction": " (a) Agrees\nExplanation: The passage agrees with the target. It states that the more educated a population is about climate change, the more sceptical it is likely to be about claims that global warming will have catastrophic effects.", "task_name": "task890_gcwd_classification"}
{"id": "task890-b7324b8bd44046519805bbe5fc75a671", "prediction": " (a) agrees\nExplanation: The passage states that the real Arctic environmental worries are oil spills and similar disasters. This statement agrees with the target that Global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-5ca8b794944f45ddaf5dbaf4a36a1d57", "prediction": " (a) Agrees\n(b) Disagrees\n(c) Neutral", "task_name": "task890_gcwd_classification"}
{"id": "task890-22fbd6e2f5284d3d8900a88ca9dda10e", "prediction": " (a) agrees - the passage agrees with the target.\nInput: Global warming is caused by human activities.\nOutput: (b) disagrees - the passage disagrees with the target.\nInput: Global warming is not caused by human activities.\nOutput: (c) neutral - the given passage neither agrees nor disagrees with the target.", "task_name": "task890_gcwd_classification"}
{"id": "task890-db4d4474f4144ab2b584acf127c20b35", "prediction": " The passage has a neutral stance on whether Global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-375533155bb542768008977a998f65f5", "prediction": " (a) agrees", "task_name": "task890_gcwd_classification"}
{"id": "task890-4398e7696c1d419eae1fcc64ca0c9b1f", "prediction": " The passage has a neutral stance on whether Global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-3475f733d6f84bceb6067ee76a205326", "prediction": " Agrees\nExplanation: The passage states that liberated carbon makes our society possible, which implies that human activities are responsible for global warming. Therefore, the passage agrees with the statement that global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-6620dbdbfa714d85a1013c1dc644a991", "prediction": " (a) agrees - the passage agrees with the target.\nInput: Global warming is caused by human activities.\nOutput: (b) disagrees - the passage disagrees with the target.\nInput: Global warming is caused by natural processes.\nOutput: (c) neutral - the given passage neither agrees nor disagrees with the target.", "task_name": "task890_gcwd_classification"}
{"id": "task890-4a5d2432830a48728a319b2703c5b35d", "prediction": " The passage has a neutral stance on whether Global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-fef09b2e5c144fbd992fe69239e2f6b3", "prediction": " (a) agrees\nExplanation: The passage advocates for violence against world leaders who dodge their responsibilities to fight climate change.", "task_name": "task890_gcwd_classification"}
{"id": "task890-7ba9c5abad8f434593d66337630973af", "prediction": " The passage agrees with the target.", "task_name": "task890_gcwd_classification"}
{"id": "task890-e3ccb08a1cc34eabbc22377e7c9922a5", "prediction": " (a) agrees - the passage agrees with the target.\nInput: Global warming is caused by human activities.\nOutput: (b) disagrees - the passage disagrees with the target.\nInput: The effects of global warming are not fully understood.\nOutput: (c) neutral - the given passage neither agrees nor disagrees with the target.", "task_name": "task890_gcwd_classification"}
{"id": "task890-427d04326210494f9b5b98e939c7430f", "prediction": " (a) Agrees\nExplanation: The passage states that there is a possibility that the threshold could be reached in as little as 30 years. This implies that the cause of global warming is human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-b30d1a956af24aeeb9e588104dd4da0a", "prediction": " Agrees", "task_name": "task890_gcwd_classification"}
{"id": "task890-9c8cd9a925884d1cbc18b9304e7289d7", "prediction": " Agrees", "task_name": "task890_gcwd_classification"}
{"id": "task890-5b616d7e45e84bbd9bc7b4238c4f5bd7", "prediction": " (a) agrees - if the passage agrees with the target.\n(b) disagrees - if the passage disagrees with the target.\n(c) neutral - if the given passage neither agrees nor disagrees with the target.", "task_name": "task890_gcwd_classification"}
{"id": "task890-8beac48263ac416983d2e0273ea46f70", "prediction": " The passage agrees with the statement that Global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-a4501425226647f0a8a9bdb1fabc5b8f", "prediction": " (a) disagrees\nExplanation: The passage does not agree with the target. The word \"alarmists\" implies that the people who believe in global warming are wrong, which is a disagreement.", "task_name": "task890_gcwd_classification"}
{"id": "task890-aed60e586c3a4fc2a81541adb580a524", "prediction": " Agrees", "task_name": "task890_gcwd_classification"}
{"id": "task890-c5de57ce8fe34870823490bcf4b87f93", "prediction": " (a) Agrees\nExplanation: The passage agrees with the statement that climate change is a likely threat to national security.", "task_name": "task890_gcwd_classification"}
{"id": "task890-6aefbb17ccd54457b0d9a7fdd9b33790", "prediction": " (a) agrees\nExplanation: The given passage states that global warming doesn't exist in a news vacuum, which means that it is a widely discussed topic. Therefore, the passage agrees with the target that global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-d12710735bd3471b98a5b027cb365cb2", "prediction": " The passage agrees with the statement that developing countries are paying the price for the carbon-intensive practices of a few countries.", "task_name": "task890_gcwd_classification"}
{"id": "task890-6869b5617a8a4421bbdba215086defc9", "prediction": " Agrees", "task_name": "task890_gcwd_classification"}
{"id": "task890-051f0cbb866d4cd1ac3d010620984d04", "prediction": " The passage has a neutral stance on whether Global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-f6da3c37394e462d8b3e70bf3a5f38cd", "prediction": " Agrees\nExplanation: The passage states that an ice-free Arctic Ocean would not freeze again, which implies that the cause of the ice-free Arctic Ocean is not due to natural causes. Therefore, the passage agrees with the statement that global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-c18d249eabaa4844b857d60098832e58", "prediction": " Agrees", "task_name": "task890_gcwd_classification"}
{"id": "task890-f5e5d13d481b4b2997629897e7de71d5", "prediction": " The given passage has a neutral stance on whether Global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-c364b52e83cc4654bfdc1d3b89f4411a", "prediction": " The passage agrees with the target.", "task_name": "task890_gcwd_classification"}
{"id": "task890-46c71b863aa54852ae27ef0018819b64", "prediction": " (a) Agrees\nThe passage agrees that the rapidly warming Arctic has caused the jet stream to slow.", "task_name": "task890_gcwd_classification"}
{"id": "task890-d83db51f22fc44d2be23834baf06c6fc", "prediction": " (a) Agrees\nThe passage agrees with the target that global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-4400e0e8821b482b92d16c91d41a5b27", "prediction": " The passage agrees with the target.", "task_name": "task890_gcwd_classification"}
{"id": "task890-5694647a4f1542129f529116107d0fa5", "prediction": " (a) Agrees\nExplanation: The passage agrees with the statement that global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-3027e6af64d543ffa56f9427132b1c68", "prediction": " (a) agrees - the passage agrees with the target.\n(b) disagrees - the passage disagrees with the target.\n(c) neutral - the passage neither agrees nor disagrees with the target.", "task_name": "task890_gcwd_classification"}
{"id": "task890-c13d4cdbc3b9444e9c6eedbffd7b3466", "prediction": " The passage agrees with the target.", "task_name": "task890_gcwd_classification"}
{"id": "task890-f02ec9d151d44bc4bb0106cc6e50edd5", "prediction": " (a) Agrees\nExplanation: The passage states that Australia could become so hot and dry that its residents could join the ranks of the world's climate refugees. This implies that global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-e3c58e5939334447a5a5ee811c2f5c8b", "prediction": " Agrees\nExplanation: The passage states that stable sea levels can be rendered to look like they are rising at an accelerated pace. This implies that the rise in sea levels is not due to natural causes, but rather due to human activities. Therefore, the passage agrees with the statement that global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-bfcf578809d24960830aa4e6da4f728a", "prediction": " (a) Agrees\nExplanation: The passage states that global temperatures last year were the second warmest since record keeping began in 1888. This implies that global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-34fa73f21fc74be48a6ab782568dbff0", "prediction": " Agrees", "task_name": "task890_gcwd_classification"}
{"id": "task890-67ded18f776341a69d82796045cbd912", "prediction": " (a) Agrees\nInput: The scientific consensus is that global warming is caused by human activities.\nOutput: (b) Agrees\nInput: The scientific consensus is that global warming is caused by natural processes.\nOutput: (c) Neutral", "task_name": "task890_gcwd_classification"}
{"id": "task890-f40af1d2e03a4c67934e54d3e40bf76b", "prediction": " (a) Agrees\nInput: The Obama administration has made little progress in addressing global warming, and the president has been criticized for not doing enough to combat the problem.\nOutput: (b) Disagrees\nInput: The Obama administration has made little progress in addressing global warming, and the president has been criticized for not doing enough to combat the problem.\nOutput: (c) Neutral", "task_name": "task890_gcwd_classification"}
{"id": "task890-ae21e3f026e347e2b91479b6fdd5ed37", "prediction": " (a) Agrees\nInput: The effects of global warming are being felt all over the world.\nOutput: (b) Disagrees\nInput: The increase in temperature is due to natural causes.\nOutput: (c) Neutral", "task_name": "task890_gcwd_classification"}
{"id": "task890-c2d328cbf0f041749862d539fbed5a5c", "prediction": " Agrees\nExplanation: The passage agrees with the statement that flooding has been going on forever.", "task_name": "task890_gcwd_classification"}
{"id": "task890-fa34fc9bef86492eb13eccde0fc2618e", "prediction": " Agrees", "task_name": "task890_gcwd_classification"}
{"id": "task890-df8fd577637c450aa09b0081c321f85e", "prediction": " Agrees", "task_name": "task890_gcwd_classification"}
{"id": "task890-ca4e7943467c437b874382b9cdc51ff7", "prediction": " (a) Agrees\nInput: The Earth's climate is changing, and human activities are a major contributing factor.\nOutput: (a) Agrees, (b) Disagrees", "task_name": "task890_gcwd_classification"}
{"id": "task890-af9f9c085a034fb793c7478d79a3fa25", "prediction": " (a) Agrees\nExplanation: The passage states that there is a change in the way carbon is divided between the atmosphere and the ocean. This implies that the change is not due to human activities, but rather due to natural processes. Therefore, the passage does not agree with the statement that global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-8ff57c008c224ddb84bfa1bb3fabea5b", "prediction": " (a) Agrees\nInput: The Arctic environment is changing extremely rapidly and it is time for the rest of the world to take notice and also to take action to address the root causes of climate change.\nOutput: (b) Disagrees\nInput: The Arctic environment is changing extremely rapidly and it is time for the rest of the world to take notice and also to take action to address the root causes of climate change.\nOutput: (c) Neutral", "task_name": "task890_gcwd_classification"}
{"id": "task890-459bb1785873434abc7f7d20733da163", "prediction": " (a) Agrees\nExplanation: The passage suggests that there may have been an orchestrated effort to mislead the public about the threat of global climate change. This is an example of an agreement with the target.", "task_name": "task890_gcwd_classification"}
{"id": "task890-1eb89ba900c742539c7ca06006b99a9e", "prediction": " The passage has a neutral stance on whether Global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-2eb818839a004d4c906fb052a5d6901a", "prediction": " (a) Agrees\nThe passage agrees that global warming has caused and will continue to cause a significant increase in extreme weather events.", "task_name": "task890_gcwd_classification"}
{"id": "task890-423f41fcadac42a6bd1f18c608bc9db4", "prediction": " (a) Agrees\nExplanation: The given passage states that the global warming debate is over, which implies that the cause of global warming is already known and agreed upon. Therefore, the passage agrees with the target.", "task_name": "task890_gcwd_classification"}
{"id": "task890-6b4f755b8978431fba6eaadc013291d3", "prediction": " (a) Agrees\n(b) Disagrees\n(c) Neutral", "task_name": "task890_gcwd_classification"}
{"id": "task890-a2ee7b6374034ab1b7efa6c8fd9ff44d", "prediction": " (a) Agrees\nThe passage agrees that Global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-56c92bdd0e1448e6b480e8c6ff5f7644", "prediction": " (a) Agrees\nExplanation: The passage states that recent hurricane activity has not been made more severe by global warming. This means that the passage agrees with the target.", "task_name": "task890_gcwd_classification"}
{"id": "task890-55cc939cc4b345dab275dc5428b643d2", "prediction": " (a) Agrees\nThe passage agrees with the statement that man-made climate change isn't solely to blame for tropical storms.", "task_name": "task890_gcwd_classification"}
{"id": "task890-1f3489688954440db5aabff68f1de6f8", "prediction": " Agrees\nExplanation: The given passage states that the consequences of melting ice will be felt far from the poles. This implies that the cause of the melting ice is not natural, but rather human-induced. Therefore, the passage agrees with the statement that global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-9b60e919a15c4093969329149380aa69", "prediction": " (a) Agrees\nInput: The increase in global temperatures over the past century is largely due to human activities.\nOutput: (b) Disagrees\nInput: The increase in global temperatures over the past century is due to natural causes.\nOutput: (c) Neutral", "task_name": "task890_gcwd_classification"}
{"id": "task890-edbfcfadcbf146df9c6f1ce8a70d50f3", "prediction": " The given passage agrees with the target.", "task_name": "task890_gcwd_classification"}
{"id": "task890-02c1766cf6844691bfdb1a727c718654", "prediction": " (a) agrees\nInput: The burning of fossil fuels is the primary cause of global warming.\nOutput: (b) disagrees\nInput: The increase in greenhouse gases is a natural phenomenon.\nOutput: (c) neutral", "task_name": "task890_gcwd_classification"}
{"id": "task890-7c2e3b3489454614aeeec956eb98367b", "prediction": " (a) Agrees\nExplanation: The passage agrees with the target that the standard global warming narrative is stale and alienating.", "task_name": "task890_gcwd_classification"}
{"id": "task890-a0b4b7bb396b453690665744e176a116", "prediction": " The passage has a neutral stance on whether Global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-b9576d9aec664042942fddc01b474c0b", "prediction": " The passage agrees with the statement that Global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-3ad5061e48cc4a1e8406c35a0274b052", "prediction": " Agrees", "task_name": "task890_gcwd_classification"}
{"id": "task890-a818d012e4434637a5d425cf3f1e081a", "prediction": " The passage has a neutral stance on whether Global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-579d460c208d487c9be9026b22b608ee", "prediction": " (a) Agrees\nExplanation: The passage states that if global temperatures continue to rise, the problems will be worse. This implies that global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-f899a524865e40c39bfbb7c443b0d262", "prediction": " The passage agrees with the target.", "task_name": "task890_gcwd_classification"}
{"id": "task890-fcd8522fa0524ba3aadfe3b63f25a12e", "prediction": " (a) Agrees\nExplanation: The passage states that there is a feedback mechanism involving Indian Ocean sea-surface temperatures, which greatly influence how much rain falls over east Africa. This implies that the passage agrees with the statement that Global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-56dd2048dc9047fba552a9d5cb35e1fe", "prediction": " (a) Agrees\nExplanation: The passage states that major glaciers that are part of the West Antarctic Ice Sheet appear to have become irrevocably destabilized. This means that the passage agrees with the statement that global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-4d35e25cbed145db8fa14d5a94d25fd1", "prediction": " (a) Agrees\nExplanation: The passage agrees with the statement that alarmists about global warming are trying to gain more power over the economy and our lives.", "task_name": "task890_gcwd_classification"}
{"id": "task890-ff8401c9a2bc4933864cf8a4a131eb53", "prediction": " (a) Agrees\nExplanation: The passage agrees that there is a shortage of good climate scientists, which is a global problem.", "task_name": "task890_gcwd_classification"}
{"id": "task890-1f95eaaa45cb4fefb48c47117600083b", "prediction": " (a) Agrees\nExplanation: The passage states that the third consecutive year of the highest global temperatures on record and the lowest levels of Arctic ice have occurred. This suggests that global warming is caused by human activities.", "task_name": "task890_gcwd_classification"}
{"id": "task890-d048cb495f154814904ace140693d356", "prediction": " (a) Agrees\nExplanation: The passage agrees with the statement that global warming may be occurring more slowly than earlier thought.", "task_name": "task890_gcwd_classification"}
{"id": "task890-27eb03ad6bf545b8954b94c5e7bac462", "prediction": " Agrees", "task_name": "task890_gcwd_classification"}
{"id": "task890-222ea4afd67842238faade5d5bdfe488", "prediction": " (a) Agrees\nExplanation: The passage agrees with the statement that Global warming can increase snowfall by boosting the amount of moisture in the air.", "task_name": "task890_gcwd_classification"}
{"id": "task890-adbf7eef414245adbd979195f2c3fd75", "prediction": " (a) Agrees\nExplanation: The passage states that temperature readings from the Arctic and Antarctic are used to estimate the effects of global warming, but it does not explicitly state whether the global warming is caused by human activities. Therefore, the passage has a neutral stance on the issue.", "task_name": "task890_gcwd_classification"}
{"id": "task890-69edefae319743afae68ba164bdeb7e0", "prediction": " The passage agrees with the statement that sea levels are rising an average of 3.2 mm per year across the globe.", "task_name": "task890_gcwd_classification"}
{"id": "task890-aee141f6b0bb415d862212dab9c9dc32", "prediction": " (a) agrees - the passage agrees with the target.\nInput: Global warming is not caused by human activities.\nOutput: (a) disagrees - the passage disagrees with the target.\nInput: Global warming is caused by human activities.\nOutput: (a) agrees - the passage agrees with the target.", "task_name": "task890_gcwd_classification"}
{"id": "task620-10dfb0f9c4e24dd0be4f5c7c49473cbf", "prediction": " \"Emergency Medical Technician Defibrillation\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-504837e1543a418495ad2783d1315f75", "prediction": " \"Tricyclic antidepressant overdose\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-00b734cdb22c40fabe2aca5302777a41", "prediction": " Balo's concentric sclerosis.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-ea630e07200642cca545ce6a230fd84b", "prediction": " \"Growth pattern\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-2718d4ffbd674e9ea99884ac09e4469d", "prediction": " \"preleukemia\", \"myelodysplastic syndromes\", \"myeloproliferative disorders\", \"prognosis\", \"karyotype\", \"clinical significance\", \"monosomy 7\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-be2689bd271243f2a545f0be7e428f7e", "prediction": " Massive degloving injury, open pelvic fracture, evisceration, transfusion, recovery, work.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-891ee710dcf44574a14781824524e857", "prediction": " Folate deficiency, subacute combined degeneration, methyl tetrahydrofolate, neurological, psychiatric, methyl fol", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-72ecae15534a4a9fa35eae5be036a125", "prediction": " \"Boerhaave's syndrome\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-26e6b851147e40f8b471607bbf258b95", "prediction": " \"Pediatric intraosseous infusion\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-cede83485caf4f02be6497098f52c0a1", "prediction": " \"Cocaine\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-6cba1bcc22df45cdb3dbaddcec371d81", "prediction": " \"Caustic ingestion\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-a16d39f9ca314c1dafc41b46bbbb48c5", "prediction": " \"Defensins\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-5eb9963dc0ce40f488c129151c71cfab", "prediction": " \"hemoperfusion\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-618d5b8c4759495a95edaadf817a893d", "prediction": " Air under diaphragm, CAPD, air collection, bowel perforation.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-54b94058c9794f5fa53beb9731cd5bac", "prediction": " \"hemodialysis membrane\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-de46871b297245dab9bbcc3d26c20480", "prediction": " \"Pharmacokinetics\", \"atracurium\", \"children\", \"anaesthetized\", \"isoflurane\", \"nitrous oxide\", \"oxygen\", \"infants\", \"hepatic\", \"renal\", \"function\", \"children\", \"normal\", \"hepatic\", \"transplantation\", \"infants\", \"children\", \"normal\", \"excretory\", \"function\", \"infants\", \"children\", \"normal\", \"excretory\", \"function\", \"atracurium\", \"children\", \"normal\", \"hepatic\", \"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-a47d3e0376d4449ebfcccb0d7ff4384b", "prediction": " \"Neuromuscular blockade\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-299c037e04f44ac2b55c892377268cfa", "prediction": " \"Histamine release\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-7e7c49099d8c48d48b402161994ea35b", "prediction": " \"Midazolam, sedation, ventilation, ventilatory response, carbon dioxide, hormonal stress response, children, surgery, anaesthesia\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-90e0805293bb4319a0837521c907d343", "prediction": " \"recovery\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-7a0130fa163d4ef390ab0565e72590f5", "prediction": " \"Hendersonula toruloidea, Scytalidium hyalinum, antigenic structure, antibodies, antigenic cross-reactivity, antigenic identity, antigenic similarity, antigenic distinctness, pathogenic fungi, tissue specimens, cultural identification\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-25d5f9b848d54e9583156b7f342c6621", "prediction": " Manic episode", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-9773bdfef9244277bffc169cbacc7bb1", "prediction": " Mechanical dermatitis.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-4f547d20f8cd4e85a39821ccd281aac6", "prediction": " \"Interferon\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-f68072a46b3542999cee70299f7e1193", "prediction": " \"Fusional vergence\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-001c933aaf494f7fa719a9fe3ae27d29", "prediction": " \"Capsular bag fixation\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-2b9aca9407554a51823d6d83a39545b6", "prediction": " \"Beading\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-44e4a84e2caf45aead57d017e04da79c", "prediction": " \"ocular fundus\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-cf27668305eb4327b719c8fa680aa5a8", "prediction": " \"Epidemiological\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-2a5f7262f1f84ca585aee4aff6f9b529", "prediction": " Thrombospondin with fibrinogen, fibronectin, and von Willebrand factor binds to platelets stimulated with agonists and support platelet adhesive functions. The receptors for the latter three proteins are associated with membrane glycoprotein GPIIb-IIIa. Thrombospondin binds to thrombin-stimulated platelets from normal and thrombasthenic donors with a similar affinity and capacity.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-3429a376c0bc41578378ca0fbd5340c6", "prediction": " Hemopoietic microenvironmental defect in Sl/Sld mice.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-55784b70ae2e44ec9aeb57d5124a3f17", "prediction": " Wiskott-Aldrich Syndrome (WAS) is a primary immunodeficiency characterized by the absence of microvilli on lymphocytes.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-a85eb7f6f04246bba9c814fa3b745209", "prediction": " \"Heinz body formation\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-28cdf17bc3884f11abb80d6e3f3d6ebb", "prediction": " Thalassemia intermedia, gamma-globin gene quadruplication, fetal hemoglobin, alpha-globin polymorphism.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-0e8ee3e527fe45ad92a447b872b763ea", "prediction": " \"HL-60 cells\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-5dbbb5e33ab54e13bdf4bacfcc067b8e", "prediction": " \"Rheumatoid arthritis (RA) patients\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-841a5ecb92294c87ab55684bc0c0369b", "prediction": " \"Internal mammary artery grafting\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-da4fa14fc7644a4da4f6035d6b9c98f5", "prediction": " \"Drug-induced mutations\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-41142cef31a24953b9cdeee355738380", "prediction": " \"Rheumatoid arthritis treatment\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-e04c76bded9a4f7b8ec6cddc6ddd0374", "prediction": " Diagnosis, Carpal Tunnel Syndrome, Electrodiagnosis, Median Nerve Conduction Tests.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-b77ad609b87545c08823f2e0328fe68d", "prediction": " \"prostatic cancer\", \"treatment\", \"response\", \"poorly differentiated tumor\", \"metastases\", \"cardiovascular side effects\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-3c92294d7aa446ff88a87d0e0178ed33", "prediction": " Cirrhosis, Liver Failure, Cardiac Index, Systemic Vascular Resistance, Azygos Blood Flow, Propranolol, Portal Hypertension.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-595a9a7fbfe742aa97e1c9fd954422eb", "prediction": " \"Thermal injuries\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-50d954a1ecaf42f7a011ad3102ce2038", "prediction": " Industrial burns, epidemiology, retrospective, burns unit, burns, scalds, flame burns, clothing ignited, face, hands, chemicals, gas explosions, electric flash, respiratory injury, safety.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-cf7c50ed42bc498a996986aa11a900a3", "prediction": " \"Burns epidemiology\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-a349acd5883248cbb50509feebc357af", "prediction": " Active range of motion, games, hand burns, patient, interest, participation, therapeutic program, intellectual stimuli, distraction, burns unit.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-b0e9da43d8bf45069600fbdb0e41daf4", "prediction": " \"Eye protection\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-cde34f6048d343e2aff98b4c43c6714e", "prediction": " Variable suction system for irrigation, aspiration and vitrectomy.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-e183933ace20490cab64705ab6771c3d", "prediction": " \"Sixth nerve palsy\" \"Third nerve palsy\" \"Horizontal deviation\" \"Surgery\" \"Medial rectus\" \"Lateral rectus\" \"Satisfactory results\" \"Advantages\" \"Disadvantages\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-a8c5f3662e87410894a1fb9d5f08cc7c", "prediction": " \"Superior oblique myokymia\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-387c3bb4af804d088c4d2476375c9dc4", "prediction": " Colonoscopy, obstetric, gynecologic, surgical, complications, severe, fatal, recognition, decompression, colon.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-9f0180bb643c47de8e752a0d1b960ec3", "prediction": " Cannabinoids, Homicide, Drivers, Alcohol, Drugs, Motorcycle", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-3a716e2810784991be0e317ed5e44014", "prediction": " \"Germ cell cancer\", \"survival\", \"treatment\", \"advanced\", \"fertility\", \"chemotherapy\", \"teratoma\", \"reactivation\", \"sequel\", \"lifestyle\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-db41109b56b846be92254151d04ef9dc", "prediction": " Infant rhabdomyosarcoma", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-c9c15943c6a346e188648608affc17b2", "prediction": " \"Lymphangiography\" and \"Ga-67 scans\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-b30b72b63ac64956b3ae3ddec1eb6e45", "prediction": " \"Cerebral metastases\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-a1890a04ec3d45549f95edbe0d6040d1", "prediction": " \"Age, treatment, and stage are independent prognostic variables in neuroblastoma.\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-4b5ea85cbcf748adbf458ea4a0a9db3c", "prediction": " Pleuroperitoneal shunting for malignant pleural effusions.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-a07196637ba84ccba7aa7cc51429ba37", "prediction": " Right ventricle, cardiac output, coronary artery disease, congestive heart failure, right ventricular ejection fraction, mitral valve disease, aortic valve disease, systemic hypertension.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-383067da2db34cfeb73cf6cfbbbc3ed1", "prediction": " Right ventricular failure.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-741d3eddab7448bbb221687641717065", "prediction": " RV failure, LV failure, RHBP, PABC, RV, LV, volume loading, inotropes, pH adjustments, mechanical assist device, RV output, RV failure, LV failure, RHBP, PABC, RV, LV, volume loading, inotropes, pH adjustments, mechanical assist device, RV output.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-2eb533978bbd4ab694c6f3d055ee5e1d", "prediction": " \"fetus\", \"gestational age\", \"cardiac output\", \"Doppler echocardiography\", \"fetal heart\", \"fetal hemodynamics\", \"fetal circulation\", \"fetal development\", \"fetal growth\", \"fetal heart rate\", \"fetal heart rate monitoring\", \"fetal heart rate tracing\", \"fetal heart rate variability\", \"fetal heart rate variability monitoring\", \"fetal hemodynamics\", \"fetal hemodynamics monitoring\", \"fetal hemodynamics tracing\", \"fetal heart tracing\", \"f", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-8a1e64b2a3884df5ab4b1da2fbccafad", "prediction": " \"progression of coronary disease\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-dbb4a506a513477dbe042e912a72c810", "prediction": " \"Nocturnal decline in ischemic threshold\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-ef4d9f56ca834aa19e9065880772fa05", "prediction": " \"IVF pregnancies\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-a0be5d0d42ea4f11a2a857b27747fabd", "prediction": " Normal filling pressures can be achieved in patients with congestive heart failure without compromise of cardiac output.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-95b8f48aba974bd9b11c416ae96720bb", "prediction": " \"End-systolic volume index\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-b3527ae2148d49b79fbd22ac748b5ecb", "prediction": " \"localization\", \"arrhythmogenic\", \"surgical\", \"therapy\", \"ventricular\", \"tachycardia\", \"preoperative\", \"endocardial\", \"catheter\", \"mapping\", \"necessitates\", \"improvement\", \"accuracy\", \"localization\", \"arrhythmogenic\", \"site\", \"left\", \"ventricular\", \"apex\", \"aortic\", \"mitral\", \"valve\", \"ostia\", \"surgeon\", \"wire\", \"skeleton\", \"patient\", \"computed\",", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-2c62c29eb39241fbb3e2a52272c4c7ee", "prediction": " \"Suppression of induction of sustained ventricular tachycardia\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-12b218e91b314faa996f2d882ab727f3", "prediction": " Irregular borders, intraluminal lucency, localization in curve, and localization at bifurcation of coronary stenosis may represent risk factors for dissection or occlusion.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-b6f76ecb1d884a1599ee714f6952e673", "prediction": " \"Coronary blood flow\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-dae3ec6c734a48e29e0add6de769fa4d", "prediction": " \"Dietary guidelines for coronary heart disease prevention\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-9548121c921e4cbfa8282718417253c2", "prediction": " \"Dean's Letter\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-b0ed416a17dc4fdfa099f58ceee1f577", "prediction": " Drowning, Near-drowning, Mechanical ventilation, Cardiac massage, Physiologic shunt, Lung, Brain, Therapy, Patient.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-7668355f09634113b9c6766f3a6360c8", "prediction": " \"Emergency medical services\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-1424044f0a954112a21c25977a001378", "prediction": " Airway obstruction, foreign body, treatment, controversy, kinetic energy, inertial force, aerodynamic force, subdiaphragmatic pressure, back blows, head hanging, emergency, consensus recommendation.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-db3c038e882445f9a121fed24505b849", "prediction": " \"Gastric insufflation\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-26d94cf5a5a9462dba2d571c118b4a2e", "prediction": " \"Decompression of the orbit\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-80a4c464a31e4ec399299abf7b343640", "prediction": " \"intraperitoneal fibrin formation\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-cf6f8d7224cb4c4dbaf8dc22f9c1a6af", "prediction": " \"Tibial rotation\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-8beeab66f8f743cf8190357e0c6bdb8f", "prediction": " \"Medial tibial pain\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-98c6c1d534214388a51f4086d1e3ddd3", "prediction": " \"arthrotomy\", \"meniscectomy\", \"intraarticular\", \"methylprednisolone\", \"placebo\", \"rehabilitation\", \"pain\", \"range of motion\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-09ae185b70ee495096ed054ed2e70145", "prediction": " \"Calcar resorption\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-4c37a863e01f4ca69d85df412795b78a", "prediction": " \"hip joint abnormalities\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-1289d9f6c4a94bb0a2bc99ae2f84eb11", "prediction": " \"Rare bilateral Achilles tendon rupture\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-e7fd2123f71d43a2880ce6782499c3d9", "prediction": " Wound healing", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-e40a86e0eb5846d9a5fcc82efd4b14ce", "prediction": " Immune-mediated osteoarthritis", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-60b80b5b5e96400ea5816352045d916c", "prediction": " \"Hyperactive\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-b0814cad09d24489a48a8a80bcd7f005", "prediction": " Foreign body aspiration, tracheobronchial foreign body, unrecognized tracheobronchial foreign body, foreign body aspiration in children, unilateral emphysema, atelectasis, bronchoscopy, foreign body, trachea, bronchial, foreign body aspiration, foreign body in trachea, foreign body in bronchial, foreign body in tracheobronchial, foreign body in children, foreign body aspiration in children, foreign body in children, foreign body in trachea, foreign body in bronchial, foreign body in tracheobron", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-e385104b1b4d459cab70400c638ade3b", "prediction": " \"Hyperresponsiveness to dopamine during hypertension\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-2909d2415c5343e28d5269211b58020e", "prediction": " \"Acetylator status\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-43828cac0b414d0d973453fec2851bfa", "prediction": " \"Ritodrine\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-1ce5d3965ff74f719d765a1ee904641e", "prediction": " \"Systemic absorption of epinephrine\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-cacf7ae8d5d84b688a4fd5e9b7ae3dd9", "prediction": " \"Psychosis and pregnancy\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-4ac49b304ec244feb4c9720a6d210485", "prediction": " \"Clinical psychopharmacology\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-ab5d24047657438ba152a07c63ed32b6", "prediction": " Dermatitis, quinazoline oxide, 2-chloromethyl-4-phenyl-6-chloroquinazoline-3-oxide, chlorodiazepoxide, patch test, sensitization, face, hands, contact, washing, clothing, production line, building.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-526277b40fbc49a5ad1ccb0cd3ded0ed", "prediction": " \"measurement\", \"quantification\", \"correlation\", \"measure\", \"response\", \"severity\", \"evaluation\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-440b4c30675044de95a27839ff6de106", "prediction": " Skin disorders, dermatitis, hand dermatitis, irritant, wet work, jewellery dermatitis, nickel, atopic, psoriasis, hand problems.", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-17c115d9393b49e4a4d454398e6cfed8", "prediction": " \"Chloracne\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task620-26f83024a50e4847b91cb8a453f2a721", "prediction": " \"Cobalt sensitivity\", \"Nickel sensitivity\", \"Cement dermatitis\", \"Cobalt concentration\", \"Nickel concentration\", \"Cement\", \"Sensitization\", \"Allergic contact dermatitis\", \"Watches\", \"Chromate\", \"Cobalt salts\", \"Nickel salts\", \"Soluble complexes\", \"Eczematous skin\", \"Skin sensitization\"", "task_name": "task620_ohsumed_medical_subject_headings_answer_generation"}
{"id": "task035-735e789a2282422589808af8754a2bc0", "prediction": " PersonX learned new organizational skills from PersonY because _ 's day schedule was very chaotic. \nQuestion: PersonX learned new organizational skills from PersonY because _ 's day schedule was very chaotic. \nAnswer: PersonY\nOutput: PersonY learned new organizational skills from PersonX because _ 's day schedule was very chaotic.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-78cf6177c9ce45528bbcd0b92644e56c", "prediction": " PersonX gave valuable pre-natal advice to PersonY since _ had much experience with coping with pregnancy. \nAnswer: PersonY", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-695583fecac34c37bb89806944cf4f01", "prediction": " PersonX was accepted by PersonY for her club, so PersonX was a new member at last.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-3f07bba2879d4dc69170337a29436b54", "prediction": " PersonX had been abused badly by PersonY , so the pain _ received was recurring every day. \nAnswer: PersonY", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-56be5ef47d7e4f079d54e713dde14e4b", "prediction": " PersonX submitted her notice to resign.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-049d3e868f1146349a74400f0bf9612a", "prediction": " PersonY", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-0dd8ee97f55749dd8edecbd422255f41", "prediction": " PersonX gave lessons to PersonY , as they had so much piano playing knowledge and experience.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-8b6b83650d5347fab88495e9473c7d1c", "prediction": " PersonX acted superior and confident around PersonY , since PersonX had much too high self-esteem.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-bf7ef1ed829a4f47a3d0dbd80a316e39", "prediction": " PersonX declined PersonY 's invitation to go on a long hike, as PersonX prefers to be indoors.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-52ca1383bf954a709ce65ee4cd8124be", "prediction": " PersonY", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-0ec3a1600ca14cfd918c486b7918be22", "prediction": " PersonX gave her fries to PersonY to eat, since PersonX absolutely detested the taste of fried potatoes .", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-0e71cbe9f3ea42699dd3654c9a345f01", "prediction": " PersonX was learning to bake from PersonY since PersonX was a novice at this art.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-9596efaa1cc9452db83877d8635b5fc3", "prediction": " The school chose PersonX over PersonY to be a teacher, since PersonX is more sympathetic with kids.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-d91bed7d23314e5481bfeda1588068cd", "prediction": " PersonX wanted to buy a new pet from PersonY so _ asked what tropical fish she had for sale. \nAnswer: PersonY", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-799ce834a2384c798d62ba2cb8146fac", "prediction": " PersonX wanted to keep PersonY from entering the house, but PersonX got to the door after her.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-fdcd14b4540e41dbb9d4854a8873a685", "prediction": " PersonX was very civil when dealing with PersonY so that PersonY would be accepted by her.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-56d5e301837d4bdb8fd6729b5a20e68f", "prediction": " PersonX had a much better style than PersonY , since PersonX had better taste in designer clothes.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-7e3b434788774679bb62ebec9f18f242", "prediction": " PersonX got a better rating on their restaurant's health inspection than PersonY because _ kept the place clean . \n", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-7f9bbd0d45ac4675854c0250d22be771", "prediction": " PersonY could lean on PersonX because _ needed some strong support during these hard times.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-037974c75c4440ad8aaf4966a148eff4", "prediction": " PersonX helped PersonY memorize their lines, since PersonX was more familiar with the part .", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-ed42ce861b5f406883534fcbc21d56e2", "prediction": " PersonX was a better student than PersonY so _ got into a good college on the first application.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-bdf92efc1ea14b83a90afa360b98a1e4", "prediction": " PersonX went to the store for PersonY since PersonX is currently the healthy one at home.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-c58ccca7b4744764b9f430a6e8510d1c", "prediction": " It was very hard for PersonX but not PersonY to plan a funeral because they had lost their mom.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-aaab83ee86c1413a9246895833a1b9fb", "prediction": " Baking bread was always a favorite of PersonY's compared to PersonX, so _ was interested in being a baker.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-436451411506471bb1ad643a005f4efc", "prediction": " PersonX sat at the window and watched for PersonY to arrive because he had missed him while he was gone on a trip.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-e3bcc7c84b184df7ba942a5e3ffd9eb3", "prediction": " PersonX hit his leg on the leg that PersonY stretched on the floor and fell down because the leg is blind.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-c354abf005594d1283a9e0f03fff6895", "prediction": " Since PersonX drove PersonY to the gym then went home, PersonX spent the next two hours at home.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-06045815050142b79edc31610d71e571", "prediction": " PersonY", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-4ebad081ef1048f5bf6403931f2ea908", "prediction": " PersonX asked PersonY how to grow lettuce and a variety of other vegetables as _ was an inexperienced gardener. \nPersonY", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-c2e298cb8b3549d2b5f5af21fd559aff", "prediction": " The summer went too quickly for PersonY. PersonY was excited for school, but didn't want to go back.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-d0ecf751a97648509a657986315cf778", "prediction": " PersonY had a sun burn on their skin but not PersonX because _ exposed them self to the sun.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-8d31d6936ced4fb497ede490763611e7", "prediction": " PersonX got a new iron for Christmas, but PersonY didn't because they needed a new one.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-e23e8297913e435781f0c336cafba8c5", "prediction": " Acting suits PersonX very well but not PersonY because PersonX is a very outspoken person.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-4883186df8f8414a94a6d950c6c1dc18", "prediction": " PersonX left the sunroof open in the car while PersonY always closed it. _ got mold and mildew in the seats. \n\nExplanation: The context word is \"always\" and the trigger word is \"left\". So, the answer is \"PersonX\".", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-f72373027cc444e79e5d6dcf62eb900d", "prediction": " PersonX went to the local bank to open a checking account and added PersonY as an authorized user. _ had paychecks direct deposited. \nPersonY", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-18bb90f124f24d41a4b79de6f50aa960", "prediction": " When PersonX declined PersonY's invitation to go out on a date, _ explained that they already had a girlfriend. \nAnswer: PersonY", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-f234d4d959a14781b9763288198a290c", "prediction": " PersonX was very religious but PersonY was more spiritual. PersonX spent a good amount of time reading the Bible.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-a66c167afe6e4a0f941b02ef2a361532", "prediction": " PersonX wasnt as prepared for the mountain as PersonY because they slept poorly the night before.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-567ba6caf8f74fafbed56f22dda3829f", "prediction": " PersonY\n\nExplanation: The context word is \"cat\" and the trigger word is \"was\". The answer is \"PersonY\" because the sentence is rephrased to \"PersonY needed help from PersonY to get his nervous cat into the carrier to take to the vet, because _ was having a hard time.\"", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-257662ad82b54c21b614813b18229cb0", "prediction": " PersonX cleaned their scraped knee but PersonY didn't clean theirs. They didn't get an infection on their wound.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-fe2d823f73004e4ea6ebff420a01f222", "prediction": " PersonX's hair always got big when it was humid. PersonY bought a dehumidifier to help. PersonY was grateful for the gesture.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-269e3379c3024c7f98616343080e33a8", "prediction": " In tense situations PersonX did not scare easily but PersonY did because _ was very brave. \nQuestion: In tense situations PersonX did not scare easily but PersonY did because _ was very brave. \nAnswer: PersonY", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-6878e977730240e59d8388258f6a7078", "prediction": " PersonX recognized an inherent talent in PersonY, so _ asked them to star in a new movie. \nAnswer: PersonY", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-97e0d645cb5649c290fbdbe4a78b346b", "prediction": " PersonX did not spend a lot of time choosing fabrics unlike PersonY because they had little time to shop.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-57c98706acd64ccd937344d1e1d291ee", "prediction": " PersonX could only climb beginner walls while PersonY climbed advanced ones because PersonX was very weak.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-b8651d39d4fa43bba36fa17a336ccd03", "prediction": " Photographs taken by PersonX are not as good as a PersonY because PersonY is a professional photographer.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-0793d09feaf547e78abc21ae7cef60f5", "prediction": " In the newsroom, PersonX demands to know the source for PersonY's article. So, it's probably that the editor is _.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-bce8c73e56d84bbbbbb43118a0c61b9b", "prediction": " PersonY", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-fa55de662cd54204b661bb65e38fc70c", "prediction": " PersonX needed PersonY's help swimming in the pool, because PersonX was uncomfortable in the water.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-49079470746445688020920018c1f2c2", "prediction": " PersonX is a loan officer, PersonY is a gardener due to that PersonX is better at helping to procure a loan.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-3175e87d09724602b2479dd3cb088b41", "prediction": " Although PersonX was overall a nicer person than PersonY, PersonX did not practice humility very regularly.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-7217839d01f34d7b856364bc5ccdb521", "prediction": " At the bakery, PersonX scolded PersonY after flour was spilled all over the floor because PersonX was angry.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-a877a2aef718439f991d5c922ad9eb4b", "prediction": " PersonX used a razor but PersonY used a pair of scissors so the hair in _ beard was shorter. \nPersonY.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-0c3c9a0b208c4926a964ff9e5ffd8e72", "prediction": " PersonX is lazy and does not move around much, PersonY does and that is why PersonY is fit.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-0ba9f86d77b141a3b8488821a7df657e", "prediction": " The brewery manager position was better suited for PersonY than PersonX because _ had vast knowledge of craft beer.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-827c30d4e79e4f74819a546a09286061", "prediction": " PersonX decided that they would treat PersonY to a massage at the spa because PersonY was so thankful.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-5bda9bfe0fed421c8c052b7264765d7d", "prediction": " Intermittent fasting worked great for PersonX but not PersonY, as PersonY didn't suffer from low blood sugar.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-98a4075efe444ad782a489f822edeb11", "prediction": " PersonX said that the execution of the task was as important as completing the task itself. PersonY was lazy so didn't follow the advise of PersonX.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-b69eff007882464d875b27ddb02f676a", "prediction": " PersonX tried to make PersonY more comfortable because PersonY was sick last week and wanted to help.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-55c1a6e5fa964b17b133b224f55ce6ae", "prediction": " While shopping for shoes, PersonX asked for PersonY to show them some Nikes, so _ could try them on. \nAnswer: PersonY", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-d4c6176ecbcb419bbdabca6ff50cefe2", "prediction": " PersonX was a better museum docent than PersonY because PersonX always read up on the new exhibits.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-2662965605794d7b985bc5c3a1ee0e27", "prediction": " PersonX owned more cookbooks than PersonY, so PersonX made a different meal for every church potluck while the other did not.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-d5a2069f68394be0a5a66680d745afc6", "prediction": " Getting ripped off was something that happened to PersonX regularly but not to PersonY because they always paid attention to their store receipts.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-c4db2583c43f41ffb8fc8d06d96475c9", "prediction": " PersonX's braces looked better than PersonY's because PersonX always flossed after a meal and before bed.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-48d7185d37dd403590a41ad240474864", "prediction": " Following instructions came naturally to PersonX but not PersonY because they had lots of self discipline.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-1c9055ee0bd64d90af89623437c759fb", "prediction": " When PersonX's company hired her, she became PersonY's manager. _ made changes to improve efficiency. \nAnswer: PersonY", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-c9eb94d7e56241c6876961880db15a0f", "prediction": " PersonX often had a foul smell coming from their armpit but PersonY didn't, so PersonY had to go to Target and buy some deodorant.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-11d65ac58ee54c0f9d8cfadeeaad4fd8", "prediction": " PersonX was helping PersonY to stretch at the gym because _ is a athletic trainer. \nAnswer: PersonY", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-711a9c4952d6480f8e191ca47c47067c", "prediction": " PersonX gave PersonY a head start in the race they were having as PersonX was a very fast runner.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-9f163c1e8d484bb781b2c7163e8e0018", "prediction": " PersonX had a lot of fear of heights while PersonY was fearless, and _ refused to go skydiving. \nAnswer: PersonY", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-2976458fdad34b03870e7f504a5fef69", "prediction": " The dentist was happy with PersonX's teeth but not PersonY's. _ had no buildup of plaque. \nQuestion: The dentist was happy with PersonX's teeth but not PersonY's. _ had no buildup of plaque. \nAnswer: PersonY", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-29e200c7aabd4cce9eb0c8a021420b78", "prediction": " The project meant PersonX collaborated with PersonY, who was not excited. PersonX loved working with others.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-aa8c83ca7292427fa2195b97a7ce42d6", "prediction": " PersonX caught their flight while PersonY missed theirs, so PersonX spent the next hour on the plane flying.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-ff01e5e99b7d4d47b4fc13961366504e", "prediction": " PersonX was better able to communicate their ideas to the group than PersonY, because PersonX was confident.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-2ad82a6ec60d4a2e8b9729fcaba6573d", "prediction": " PersonY\nContext Word: sympathetic. \nQuestion: PersonX was sympathetic towards the homeless man, while PersonY was not. \nAnswer: PersonX", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-52b8368757c9441bb3ba2c26472f447c", "prediction": " PersonX stayed in bed and slept for longer than PersonY because PersonX was feeling more sick.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-642c199493704095815bde4c90376ab3", "prediction": " PersonX is known to socialize a lot with others, but PersonY doesn't. This is because PersonY is not talkative.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-8c602a177d194ffca3b09bbdba34a857", "prediction": " PersonY", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-6afcbd4dd1394eac98720b486b620214", "prediction": " PersonX needed to use a moisturizer after taking a shower but not PersonY because they had dry skin.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-258f13580c4a4891b25e59d7bc706c80", "prediction": " PersonX broke the smaller items that needed cleaning in the house so PersonY ended up doing it since PersonY is incompetent.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-740e841e085c43d19b8fa93d5d8b802b", "prediction": " PersonX loves to drink sweet drinks unlike PersonY because PersonX is not very health conscious or a diabetic.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-43a7dbda1b6d4c1a99f123d31238d28b", "prediction": " PersonY suspects she might have cardiac arrest and goes see her doctor PersonX, because _ wants to prevent it.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-16dcf02440df41cbbde73b5970bbf8e0", "prediction": " PersonX was a much better baseball player than PersonY because PersonX had control over the ball.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-30488b709cc646439bc1b7f36895f779", "prediction": " The jeans PersonY gave to PersonX is dragging on the floor when he wore it because _ is tall.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-6b1c41aa8d2d43f0b6b77a0a40292ec5", "prediction": " PersonX tried to step in and prevent PersonY 's fall, but PersonX was too slow.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-e32a1ab4336e4f3e99e648fc466f9d90", "prediction": " Even though PersonX lost more weight than PersonY, PersonX still ate more sweets overall despite this.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-c6d358873697432e86068ac79e5886e2", "prediction": " PersonX was annoyed that PersonY kept insisting she wear a costume to the party, because she loathed dressing up for Halloween.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-a19ecb2c3dd445a59e63435027d6777e", "prediction": " PersonX experienced a lot of heartbreak after PersonY left town due to _ being in love. \nQuestion: PersonX experienced a lot of heartbreak after PersonY left town due to _ being in love. \nAnswer: PersonY\nOutput: PersonY experienced a lot of heartbreak after PersonX left town due to _ being in love.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-9dd17ef525f14541a2391651d3e47e82", "prediction": " The kids of PersonY are being watched by PersonX because _ is a good parent.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-68ba55ed9e0344ce8857af482164fa26", "prediction": " PersonX is afraid of caterpillars but PersonY is not. So _ asked for help to remove the caterpillar. \nAnswer: PersonY", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-00edaf241f99491896bb0dc5928d66ab", "prediction": " Canada was an ideal place for PersonX to live but not for PersonY because PersonX loved cold weather.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-9ee198fcd5b048b48404af886a9cca28", "prediction": " As they got older, PersonX chose to dye her hair but PersonY chose not to, so _ had brown hair. \nAnswer: PersonY", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-c4888aff3b45491ba4c7552d7e6fcc07", "prediction": " PersonX had been feeling weird about bonding with PersonY, so they said they wanted to stop hanging out.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-eb03d4354eb14021860aef9aae5d00d5", "prediction": " PersonX is very creative, PersonY is not therefore PersonX wants to be a graphic designer.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-5af75dd772f04759887c5d5cd9f08732", "prediction": " The death of the actor was felt more by PersonY than PersonX because PersonY was a close relative.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-c7900ed1d4d24a41b054296f4c5488c6", "prediction": " PersonX was eating more than PersonY was eating for dinner because PersonX had skipped lunch.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-675139262e394bddb95c48c6289ee331", "prediction": " Last week, PersonX chewed out PersonY for shooting his BB gun at the squirrels, because PersonX thought they were cute.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-60527cb1c83b4a178e5c5fd3bfff4630", "prediction": " PersonX was afraid of the dark, while PersonY was not, so PersonX's anxiety was made worse by the blackout.", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-ea87f3a5c3bf4b84ad4ec65fd9b298c3", "prediction": " PersonY\n\nExplanation: The context word is \"car\". The given question is \"PersonX had very little money and PersonY received a recent inheritance and was very rich, so therefore _ purchased a very economical car.\" The answer to this question is \"PersonX\" because the blank is filled with the word \"PersonX\". The question is modified by replacing the word \"PersonX\" with \"PersonY\". The new question is \"PersonY had very little money and PersonY received a recent inheritance and was very rich, so therefore _ purchased a very economical car.\" The answer to this question is \"Person", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task035-ed012e8cb92348d7bbaf06f51146a44e", "prediction": " PersonX went to the courthouse to file a civil lawsuit against PersonY because _ had been wronged. \nAnswer: PersonY", "task_name": "task035_winogrande_question_modification_person"}
{"id": "task036-e69df649e3434d7789023005535b3b59", "prediction": " Sensation\nTopic word: Feeling", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-2b2f994985d241428be945b71736eb02", "prediction": " \"Vinegar can cause harm to the eyes, which can lead to temporary or permanent vision loss.\"", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-a57dc40c976545c0b342b2d8a9456451", "prediction": " Topic word: braking.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-1bcb302bc8654f8aa7a76e28b0f1db27", "prediction": " Topic word: glacial erosion.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-ef9e41e88ae142dca9eb7e26bce68699", "prediction": " Nuclear reactions in stars produce light.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-4ee57bed954c44809efb04bdb20b4fb2", "prediction": " Honey is a sweet, viscous liquid produced by bees from nectar.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-75a33fb15f5542b9a3491f5a07c45fb6", "prediction": " Fluoride is a substance that prevents tooth decay.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-18e5748e7d7f4f9f913859c65d8da04f", "prediction": " Reproduction through sexual means is a process that helps to increase the genetic diversity of a species.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-cf602b5d09a341eea454b4faa44f0c82", "prediction": " Mate, Attract, Reproduce.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-67ff65e894e34c89af159a0b4065dc93", "prediction": " \"Electricity is used for heating buildings.\"", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-1f1ce0f36ada44d9a575453ebd3273d2", "prediction": " \"Rotation\"", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-d0869279682e4608a14eb3eb8e7f9d03", "prediction": " Predator's claws are used to catch prey.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-13c9f1fab12247849b6e31e16965bbf6", "prediction": " Topic word: Expansion.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-33208cf1b0ff42559c04e40cb5ddbf26", "prediction": " \"Food spoilage caused by bacteria\"", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-2df1080d30484b458aaac528984a224c", "prediction": " The Himalayas were formed by the process of rock folding.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-ae1f7a3b4b9c476faac168f9de878dd9", "prediction": " Breathing is the process of converting oxygen in water into oxygen in blood.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-be679ce566fb422f8575765a5fccb1b1", "prediction": " \"Cyclones\"", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-701e4df1bc464cb09fbea6585b683c08", "prediction": " Hibernation is a process of conserving energy by some animals.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-bca3d737fb354203b04a94d1cf670598", "prediction": " Transportation, Boat, Watercraft, Watercrafts, Watercraft Transportation, Water Transportation, Water Transport, Water Transportation, Water Transportation System, Water Transportation Systems, Water Transportation System, Water Transportation Systems, Water Transportation System, Water Transportation Systems, Water Transportation System, Water Transportation Systems, Water Transportation System, Water Transportation Systems, Water Transportation System, Water Transportation Systems, Water Transportation System, Water Transportation Systems, Water Transportation System, Water Transportation Systems, Water Transportation System, Water Transportation Systems, Water Transportation System, Water Transportation Systems", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-a31a62e1dac54798a3e7cafdb2193d51", "prediction": " Topic word: erosion.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-1fc15c4192c34784b3e46285c4e0e262", "prediction": " Organization, Tissue, Organs", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-fcb1706f71934d34a8d3d82b29ae17f0", "prediction": " Folding an object causes it to change shape.\nInput: Fact: a person can only see objects that are within their line of sight.\nOutput: A person can only see objects that are within their line of sight.\nInput: Fact: a person can only hear sounds that are within their range of hearing.\nOutput: A person can only hear sounds that are within their range of hearing.\nInput: Fact: a person can only taste foods that are within their range of taste.\nOutput: A person can only taste foods that are within their range of taste.\nInput: Fact: a person", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-0a23792de4ff498f9a6bdf341ff34483", "prediction": " \"Microscopic\"", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-2a8c003b657741249c7177407aaa98e1", "prediction": " Topic word: Absorption.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-781d85d5877943788d37ca0c579c2edf", "prediction": " Topic word: Migration.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-b780230c278144828826e7dc32c5c633", "prediction": " Web, capture, spider, food.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-7a7fa3487b494c06805aab2044267d0b", "prediction": " Topic word: Sessile organism.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-58a76504d6f04694b36c3be181de0e87", "prediction": " \"Hearing\"", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-34e8b2237c404030b0aff618c861199a", "prediction": " Constriction is the process of narrowing or tightening of the blood vessels.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-ef0387c3de8242fe8073e95d6c2a7c5c", "prediction": " Translation reads the genetic code in mRNA and makes a protein.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-2f95f72007ad406ca87425485d33fe48", "prediction": " Topic word: Tunicate.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-e44ff221ac044bceab4e173c98b74672", "prediction": " Topic word: illumination.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-c5a434f3e07c446f882ffc8aeb4e0120", "prediction": " \"Cold fronts cause thunderstorms as they pass by.\"", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-b428ba8895ff4c388f81873e9e1c00c6", "prediction": " Topic word: Ecological roles.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-6bfabac32ce24293b18a22bade0c302e", "prediction": " Topic word: magnification.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-f4af5cb148d34ab49c31bad04cfc72a1", "prediction": " \"Driving a car is harmful to the environment.\"", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-f44efbdd1d704fbeab1c74cd0ce6218b", "prediction": " Topic word: measurement.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-276c2982505540489145ab44882961af", "prediction": " Topic word: Life\nTopic word: Living\nTopic word: Living organisms\nTopic word: Living things\nTopic word: Living things on Earth\nTopic word: Living things on Earth that are found in different environments\nTopic word: Living things on Earth that are found in different environments and are able to survive in different conditions\nTopic word: Living things on Earth that are found in different environments and are able to survive in different conditions and are able to adapt to their surroundings\nTopic word: Living things on Earth that are found in different environments and are able to adapt", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-3f94a24ab9fe4df7a1d17ae0b1340985", "prediction": " The force of water against the seed coat causes a root to emerge from a seed.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-a9acf7e4d1cd4fa3a0293fca75e720c1", "prediction": " Topic word: earthquake measurement.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-1292bf1eea9b418aac2704a5167fda5c", "prediction": " Refraction of light caused by water.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-ef0a861be2af451b8b74e35e2019872f", "prediction": " \"Electrical current heating\"", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-c97b1a231d564fe3a33f40acfcba8408", "prediction": " Topic word: Reproductive system.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-726bb202e1f24c759838cf98242acab7", "prediction": " \"Collapsing\"", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-d5ff12e911a94c489cfabbf1016b73b1", "prediction": " Topic word: measuring tool.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-eaf668a6ee6d42a8bc653aa244fd3c62", "prediction": " Erosion caused by wind and rain.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-56bfd03b33a14aa39e40e78def04f509", "prediction": " Vertebrates are all chordates that have a backbone and are members of the phylum Chordata.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-33e3a1f6958b4d1b99ee9fa7da89d460", "prediction": " Mutations in DNA can cause cancer.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-dfd12d51883d449385c3391a56b7a9c0", "prediction": " Sleep is essential for human health.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-ab938078ce49465393681d537a811d3e", "prediction": " Adaptation, Variation, Change, Evolution, Mutation, Adaptation, Variation, Change, Evolution, Mutation.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-952b8d7f9f154ea2b193e720fc9f34c3", "prediction": " Topic word: scale.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-5db89fd20a994ef186eef15a3cf59f3a", "prediction": " Programmed cell division.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-ab22f93f11e84dc79975b8f887f7af5f", "prediction": " Nectar is a substance used by plants to attract pollinators.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-b39f4c57c7cf451d8e04ed41252e4585", "prediction": " Topic word: Sensory\nTopic word: Sensory", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-f305c7d6c0da408b91f814298d594f8f", "prediction": " Air pollution can cause respiratory and circulatory problems.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-4315b3d99cd448078da45644812bb133", "prediction": " Topic word: scratching.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-79f54512234a4e248537e95917cacc16", "prediction": " \"Illness\"", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-f220cbed0c6d47b48347515904b74eb8", "prediction": " Transformation\nTask definition: In this task, you need to write a topic word from the given fact. The topic word must have at least one word overlap with the given fact. The topic word often involves adding a new word from a related concept. In your topic word, use at least one word from the given fact. Topic words with two or more words work best. Input: Fact: Adult worms go through a major transformation to develop reproductive organs. Output: Transformation", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-eca1f1793d84499baa97f8c223e212c8", "prediction": " Topic word: Eggs.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-f7e6208717a540ec9313d385210c6a40", "prediction": " Nesting, Birds, Protection.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-a9328252a30f49678e4e8ba2c4dbe750", "prediction": " Topic word: Perspiration.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-0d8941e0c28945df9dbfdbd0108df569", "prediction": " Topic word: Sunlight-based energy.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-9101dc65d1b24325b0e34ca3813d32a1", "prediction": " \"Matter vibrating can cause sound waves.\"", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-6d0fa330fca34057854ab9dd55419137", "prediction": " Topic word: soil loss.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-e3eeef2985e242ecaab34a02d3541a3b", "prediction": " Viruses are unable to reproduce independently.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-874655414ddb436797278deba8c164fb", "prediction": " Ethanol is produced by fermentation of corn or other plants.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-f846e78a1e9245a88a535ba715656106", "prediction": " Veins carry blood.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-6b5544e0a1504f8b854dce75cf0c6c53", "prediction": " Topic word: Interaction.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-08c4850f8b60444e913ea8bdb9170e60", "prediction": " Reproduction is the process by which organisms give rise to offspring through the process of procreation.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-7ed8bd39048d40a0a2b53f9d6d1f0f75", "prediction": " Reproductive organs of plants.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-0e6533a8353e411d99cecb2ea91b1dd3", "prediction": " Topic word: Aquatic\nTopic word: Aquatic", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-d577de7a49214420a1a8dc89e00cbcbb", "prediction": " Topic word: Zinc's positive impact on wound healing.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-85b7eee36f5a4ccdb0360cb7e7392b0e", "prediction": " Topic word: Evolution.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-940228173385417393c89898b251bda2", "prediction": " \"Ecological disaster\"", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-f2476674f80c432fb2fc8e8689f4e5e1", "prediction": " Topic word: Protection.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-5b38c25d174945e3822c1838259e3fbc", "prediction": " Topic word: Separation.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-8c75f5829a874ff4b1df79c045ae1c11", "prediction": " Toxicity, harmful, lethal, fatal, dangerous, deadly, fatality, fatalistic, fatalistic, lethal, lethal, lethal, lethal, lethal, lethal, lethal, lethal, lethal, lethal, lethal, lethal, lethal, lethal, lethal, lethal, lethal, lethal, lethal, lethal, lethal, lethal, lethal, lethal, lethal, le", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-3926996fa8414e77a481aa6264045d78", "prediction": " Sweetness\nTopic word: Sweetness\nTopic word: Sweetness is caused by sugar.\nTopic word: Sugar causes sweetness.\nTopic word: Sugar is the cause of sweetness.\nTopic word: Sugar is the reason for sweetness.\nTopic word: Sugar is the source of sweetness.\nTopic word: Sugar is the origin of sweetness.\nTopic word: Sugar is the source of sweetness.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-1707264fb2b34a9b8094fddc13c5b71f", "prediction": " Parasitic roundworms.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-a0378cc8ea1644eca16ae8a0a855e17e", "prediction": " Topic word: magnetic field.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-328b3a4265984d13a3d33f3b66b31a7e", "prediction": " Magnetic migration\nTask definition: In this task, you need to write a topic word from the given fact. The topic word must have at least one word overlap with the given fact. The topic word often involves adding a new word from a related concept. In your topic word, use at least one word from the given fact. Topic words with two or more words work best. Input: Fact: Earth 's magnetic patterns are used for finding locations by animals that migrate. Output: Magnetic migration", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-6d7d880cd0b04d71a5b44d23b4756b56", "prediction": " Genetic traits encoded in DNA.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-ae651a2077614c4eb384d30671e8eaad", "prediction": " Topic word: Habitats.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-7b684396d90a4e7cb0cd5e8b7746fa03", "prediction": " Allergen\nExplanation: The topic word \"allergen\" is derived from the fact \"Any antigen that causes an allergy is called an allergen.\" The word \"allergen\" is a noun that means \"any substance that causes an allergic reaction.\"", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-53dd88203c59422c9cd995b972fcc32a", "prediction": " Activation energy is required for chemical reactions to begin.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-471ba8285e914beea312a2b21e8585f5", "prediction": " \"Seasons\"", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-2bcf5e3e71f9474091dae4434d96058d", "prediction": " \"Steroid hormones are made of lipids, such as phospholipids and cholesterol, which are essential components of cell membranes.\"", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-2ed032f4eee5446c96d589ae32e63462", "prediction": " Therapsid\nTopic word: Therapsid", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-b48b84aa860d426495d665eb1e0470c5", "prediction": " Marine life thrives in the shallow coastal waters.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-916df9a7cb0145888f41a36ada13f4ef", "prediction": " Meiosis is the type of cell division that produces gametes, which are the reproductive cells of organisms.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-46cc31ed01f94beabf907937e9ee3fa4", "prediction": " Tracking, stalking, pursuing, spotting, observing, scanning, searching, detecting, following, chasing, trapping, capturing, catching, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering, cornering,", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-471e8b744af14c87ad60bb0cdbf50274", "prediction": " Aquatic animals use webbed feet to move faster through water.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-7f371dd0414c43dea22d03eca6c4367f", "prediction": " Growth requires energy.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-29ed15f6b5cc4358abc7a02c6523e911", "prediction": " Topic word: Survival rate.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-674d02554f474deab509491066f07880", "prediction": " Topic word: Light source.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-ac99c624572545788b15696f2b0f0886", "prediction": " Territoriality, Hydration, Nutrition, Reproduction.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-59deb5f758a04677bd2c153de99212f7", "prediction": " Topic word: Predator.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-bd1b21f69e1b42a89f9796b5eff7ba05", "prediction": " Erosion causes a river to become deeper and wider.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-57e9b65c9af84e85a1f72056ba455441", "prediction": " Topic word: erosion.", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task036-545b6b37f2b44850ac027cd3da0f621d", "prediction": " \"Genetic\"", "task_name": "task036_qasc_topic_word_to_generate_related_fact"}
{"id": "task937-c2016e272e174c00a89a731974286228", "prediction": " Strengthener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-bdb7f54a76c14ffcb2fd769ad3306366", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-87a7a0a7ee8341d7b9fc4a1dc4ff2416", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-c2f7ce66cd524384843bd4c5c32a7aa4", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-c0d839cc2f224c06b473d43246cf2670", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-3c277a27af10408b83158e259ff65e68", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-94a11f45da7a4bb39cf779b2d5b553b3", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-eff2ca83729943329a56328624b35365", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-4eff3662a7d845ba86f9c5d484b55a63", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-cd2dfd3411694930b1f21be3209d703a", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-79d0557699ae48bdb950cfed5325731a", "prediction": " Strengthener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-c61a7ea101424420bc428f74f0192b6e", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-12da9129070748019c36d64a6885a5af", "prediction": " Strengthener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-3237878939044a63b5e1e15bab394d9f", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-376a14236c9048e4976a1f7b06f562c0", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-91bde1256f7f42ccb552ce6467165f46", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-df094349c6ff4f639d788325611a878d", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-1d1f7caacfd04a36bed1294a8d48274a", "prediction": " Strengthener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-579db69abf164962a2bcfc71a63bc00e", "prediction": " Strengthener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-2c72925bde274866aa80386310670584", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-63a7bb46e8124b3583f72274b1430385", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-45e7b07645fa45b9aede0c4b75a0d26c", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-f59702ed2f3b4138ab6d4b9a8f1901b7", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-9b1758c215b64354a3237a02cb441cec", "prediction": " Strengthener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-d4ee9626122246b1a0056c4d672d0cdc", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-1ccda6b3ddd045878e139c318588d600", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-8d5274401dd243298df883665c99ec3a", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-9928d5b352a44c6dab78d96268aa99b8", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-5137434e3b204d2eae2c6a509e150e86", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-5fa92b6081974f668bf9fdd10d9d13c9", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-a520afd6eac84db2a86971f9ee3766fd", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-4ad20c48ab31486ba120ea4c6f954a88", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-0ce04c64ea3f41a78b0ba32ca45f691f", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-0a857c0e0dbe4ec388b9002fcd915bf1", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-4e27eb71a4244b40a13694a10c1d18e5", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-cc4ba2ec324c4ebd8153dccec17d26f4", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-c2fb9a5a0afb46cc9765d7ea7b4ad633", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-3cacee6e457c429dbe1f618f7b7821be", "prediction": " Strengthener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-2bdb941f630b48c8afba006cfa8a6e1a", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-28adf699c9ac4824881b1848442b29d2", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-0458098f24c344c89601c20ebff8bbdc", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-f0149000b2e041f7aaa554e9aaad5d54", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-5e3d333af74046af84c13cd1ad397dbd", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-b2e2f764a46340f483a86392670987d7", "prediction": " Strengthener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-f0c807938beb48cb90ff7e04fd698c64", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-7c1954a07bde461dbbc323ba4249b50b", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-9bcd8245495e4c25b1b7b40a994940a5", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-d6a5b8f93ad34c3587fa215e22ae9c9e", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-87142c4d21864c2ca2853b42e5f9b996", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-4eef730b937944ed82393453f9003a9a", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-e29d0e4bc0c040d19b5dd49713788a30", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-691f4781a27d4cfcbcb177068e4f21df", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-577b5ba8f802414096888d05f7d7c76d", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-1b2e9d0fd88d4c61b2b7f02425911bdf", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-9c1aa94044ac43db9d8fb1d5fde5a3da", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-6f680422514341cbbfd07791035d0030", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-93c3a2058fdb4357b7c81070ab3d4f61", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-cd4859e052914dd4a9dad4aa04287145", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-d64a9b80c427497d9320769c354b92de", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-5257305e2bfc427bb4025474a07cdd33", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-0402f0e1195a421d8970c455a0d0b06f", "prediction": " Strengthener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-52abe91854ae4617b95835dec69f2281", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-3476fe17f8d74e518b237185f883c74d", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-a31c5284a6b24bfc95513e44781c4cef", "prediction": " Strengthener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-cfdb277ce5714116a72b8e438acf9ad7", "prediction": " Strengthener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-b95745d336744f38810855738a7c053b", "prediction": " Strengthener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-7abb894a07b34d77ad4687e4b2fa83a9", "prediction": " Strengthener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-ae726599241c44d7a74dc5e8e5fd0b57", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-a18d95defa61431781929a97ed040e12", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-5c28167567814e8484bf2abcdf9a37b6", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-32b5abcba30147eeb13738b244397174", "prediction": " Strengthener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-0f707aa451bd4403adacbb8136ae232e", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-2dadfc8055f745c5ab9dea7f8ccaab30", "prediction": " Strengthener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-a01f8c997909459c8990088686d4410d", "prediction": " Strengthener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-cc6487bc33ea4d548f6a48ce4792309f", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-2e65a1762cea4135bf4df5679bdf3f86", "prediction": " Strengthener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-4ac3594071e6437ebe8e900ef3a7b0dc", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-e170c471ec5e4e1dbc599d77c968ea93", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-e0d9fbb57ce64ce19dd06d6d7eae406a", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-7dd022cadddc4435b04e01ede8c52594", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-d84bf7eef3a84e32b7fd6e9f48549091", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-e240ce81e22644bab12da8a70e4a794a", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-13261466d3fc4e01bdb4737c10bb5a7a", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-e08b385058004512b71526e6198c22a3", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-57512345f2684e56a700b909aaf78d3b", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-2346bdd8e97b40568a1be85ce2b22a1b", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-e6616f7214bf48b48884284270427fa3", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-59c7a76804c046ccbed70009022e1b50", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-31d216a3562e40d0b0da3cf57b67f560", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-35547790c38b4c7f9accbe0aa51ae0e2", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-9df7615bcc7e47f2acaed4d73d285bb8", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-c393a4c52e10413eacb79c8bc3a5927c", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-4346cd49e22f4b31a785f609b2a116f0", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-a83312e2b81d42adab7568c1c07a378c", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-a2a69d7fe4534e1e857d7cf8be8a057b", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-c508061a33ba438eacb11bd36614d7b6", "prediction": " Weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-d9226687bf764f349c9211c6caeae32c", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-adac3db84b16474b906abce0434ee38d", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-129b5782cff94a4d9c5db5a6c6a13405", "prediction": " weakener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task937-ff86d862b3ff4ee891d6bdbd85bc83f8", "prediction": " Strengthener", "task_name": "task937_defeasible_nli_social_classification"}
{"id": "task1529-24799178db6c4d48b078e027f4efd690", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-79f7ce65eb6d4fce90b4fa9d6b6416f6", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-1b55e67bd30d49d08643089b4ee3f4d4", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-bcf024291a11418994b00350c6b56b7c", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-983a2c39e30c43f48beaf0107ffb371b", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-2a0cd29f529c4d249fcc99ac7fa925b9", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-c89f8a93c90d489e8aac1ebb7f529fe0", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-f992e738a69741e5bc4f99db5e73a43a", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-b13a1649a814406194009184bcd9fe41", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-c4900042782e4e0e9589b892067c0c39", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-8126c0e81ba54be2aeb0194c2155a30b", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-6945fd87898d470c853d11c170a91344", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-30572bce7591494e8c4f6fa44ca5d9ee", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-1f4e1f604d58477bbbc2876220a08f72", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-e8a5e3f5f1cc4f9fb3cad3dc21d830c0", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-3ac552e1a18649b0aa24bd363b279bfa", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-16e8db7d8f73443eb6140463871ecd27", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-02a2fbd5ff3d44e48d8e60780e56004d", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-f657a84e9c7c40aa9876c2c33a442bcb", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-f855ac622df4497aa4b18481b54c1be0", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-2527b95e53274ce0b41282b757ab008b", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-cac5e9c3a5b646c2a998ead4cc6e0a1b", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-5ba1e83fe4fe4c1f991f9d3f16251f1b", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-44fd079d61bf42429c77972456112fbc", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-f728f0d081dc44dba36502a485161cda", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-c9c422d69be04ec4b42d60269a82b7ee", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-cc4755b6767247cc9054d42c6a5d40f9", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-7019634251224be1a3cf9f53b4c98cff", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-7b556eea75c9467ca4ac5c4890ad6f12", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-e5e5acf833e844c78a9e7ac723cdbfa2", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-ad9028fcafc8485da76fd8fa061920f1", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-66e328bbbe7f4e4a842a6a579ce1a3af", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-52f6f03cf2f644a6b6f3e6ccaa7d006c", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-f19ba401ed324a179e608fea4078396d", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-f0a4382f08da47bc997bb9a5cb5b808b", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-96c909329fcd40d0aa6c9b1193d9b5c1", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-867f89a5552d45d493ce6558a2572240", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-c2b6a3dbb6184735ae4a36ec4b7f9149", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-4e0c37008f0e40b6b5935ba360752afe", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-180603da026048fbbd75fd6ccfb7c6a0", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-fd16a2283a784185bc12ee950abbd9d1", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-da29c83e06054bf0aea59530e070562c", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-7866aaead66949808791e0e7ba45242f", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-b8f6bb4548e5415c8009a59b085199db", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-7ffdd770983e4cfcb2889be2c8e955ea", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-e17e7a01b7f9482e97a2937e9b9655fd", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-6d2dc3e2ecb74a52a7ed1d9492b595b4", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-d1c7553f176241018006019b7649182c", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-ffe485d715f04db2be303d36faec744e", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-9f39f81e27424e1094cb3a93e5519952", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-c175fdeae9e447feb6c0334b27d1efa0", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-de583eb9ace647f68ec68a73d75f49a6", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-986021b2ff4f4ae8955f44868eb53a4b", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-984a2574068047d2b30ed243f5b27003", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-5926db506f8e472c8d142db3936e77ed", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-3546f8a166c646afab27b9fcbb6281c8", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-58d00b54f7e24b219736ab809765371c", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-aed0c86374cd41f09554bf1aba483f1f", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-e377ea8b69f84a67aa8b3260fc61dcff", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-b5a2ce2618314a18855013439c42aac8", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-03bb98cd6df84de1acb6235ec2577d2f", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-61374108f7ef405f8a551934e64b0a0d", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-2225ab115c7d4926a4db41e236bb856f", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-b70edaa20ec94011adfd3826f0672520", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-311a10d3b01b436ca90257585d289393", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-70a002096a44429ca8f8a6768e4c5d82", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-0846a81d3fb64ef09e6f027b801350a3", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-754c660dbcd744f6a266227e5e7deaba", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-5c8ba1c76ab54c54bf6deb00f371cc78", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-c799a1aac42843b8b69546f68213da06", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-b5afac10c6c44509b6258ed55fb7fc20", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-fb4ff0ec6e734ce389703ce06ef25670", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-dd32b445a5cb4258bddab9354e79c859", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-d78d215a930149b4b172f516a1d645ad", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-10027786d34b4b2fbcf2d801d4258d12", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-6a6d5d4d92894c72a43a128ae838d73c", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-5913d2746d5941c19b373ed0c7f2b7d2", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-f658ba22ee83405aac701f13d456fa46", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-02396f90a2774414ba9a331012a1ca12", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-3e3ca97c2d8d4b42b751b84f9079db32", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-c8ff25173584488681ee781213be5342", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-84a5e74383eb486db1ace2ca9e60a83f", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-c925a9761e5648199afc800de37b3575", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-45bd4cba76664488b5e914e75f4621bb", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-a42ac6b7ebb741eabf7645aae3236ca4", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-e5fcb447ffd740b182e4e414476ac353", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-0a3b3cf6e585476a9d3fd3f52bc7ca25", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-e78e2f1216c641cb94d2c0c773cf0323", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-856e52d64a8e46fc89f6d43c165c38ad", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-f7d7ca2495f445959459b91595cda3c7", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-e31b56125c6741b4959f384f86011bf0", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-ffb6f7d9656f4bf89b3a5b97a4cf58bc", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-13fa5423147b4e16b222667f396842be", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-8103118da6c547cab72c82be4efe510d", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-a1a252ce670f49aea36677a6924378fd", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-041101e41b0143c5a2d9a5a2829687ce", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-568ef6aee0854ff0beba8f929df45543", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-e25f57907adc4f6d8f6cc153ff13c5a0", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-52d60296dbed438f8a3c5aa674114890", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task1529-fff3b8c8d54c43ac828dc3c1df4b06eb", "prediction": " entails", "task_name": "task1529_scitail1.1_classification"}
{"id": "task957-38934d14eeaa4e4fb78d022516405ca8", "prediction": " Midsummer House is an Indian restaurant with a low customer rating. It is near Caf\u00e9 Rouge.\nExplanation: The input is a data table of restaurant descriptions. The first key-value pair is the name of the restaurant, which is \"Midsummer House\". The second key-value pair is the food type, which is \"Indian\". The third key-value pair is the customer rating, which is \"low\". The fourth key-value pair is the location of the restaurant, which is \"near Caf\u00e9 Rouge\". The output sentence should contain all this information in a natural and gram", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-9f5aae7d3cb44faba2d50984e32b16fe", "prediction": " The Taste of Cambridge is a cheap restaurant with an average customer rating.\nExplanation: The input is a data table containing information about a restaurant. The key-value pairs are: name, eatType, priceRange, and customer rating. The output sentence should be a natural and grammatical English sentence containing all the information from the input. The sentence should be in the format \"The restaurant name is a type of eat, with a price range of cheap, and a customer rating of average.\"", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-5bc9c1a2a70e4c858796c4c7a368c068", "prediction": " The Eagle is a coffee shop that serves Indian food. It has a price range of more than \u00a330, and it has a high customer rating. It is located in the city centre and is not family-friendly. It is near Burger King.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-4c31118310b743219600a3929eef1a50", "prediction": "Browns Cambridge is a restaurant with a price range of \u00a320-25 and a high customer rating.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-d09014acace34d9e90dfae68fd50619e", "prediction": " Alimentum is a fast food restaurant located in the city centre. It has a price range of \u00a320-25 and has a high customer rating. It is near Yippee Noodle Bar.\nExplanation: The input is a string of key-value pairs. The key-value pairs are separated by a colon (:). The first key-value pair is the name of the restaurant, which is \"Alimentum\". The second key-value pair is the food type, which is \"Fast food\". The third key-value pair is the price range, which is \"\u00a320-", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-1cba7ec6cf484058b3967d3934619a92", "prediction": " The Golden Curry is a high-priced restaurant that is not family-friendly and is near The Bakers.\nExplanation: The input is a data table of restaurant information. The first key-value pair is the name of the restaurant, which is \"The Golden Curry\". The second key-value pair is the food, which is \"English\". The third key-value pair is the price range, which is \"high\". The fourth key-value pair is the family-friendly status, which is \"no\". The fifth key-value pair is the location, which is \"near The Bakers", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-ff290568942743108b5d291b2134a8b8", "prediction": " The Plough is a family-friendly restaurant located near the Express by Holiday Inn.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-5f7d4a43aa1b47f9b122f67406e2bcba", "prediction": " \"Blue Spice is a restaurant located in the city centre that offers a wide range of dishes for more than \u00a330. It has a rating of 5 out of 5 and is not family-friendly.\"\nExplanation: The input is a data table of restaurant information. The first key-value pair is the name of the restaurant, which is \"Blue Spice\". The second key-value pair is the price range, which is \"more than \u00a330\". The third key-value pair is the customer rating, which is \"5 out of 5\". The fourth key-value pair is", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-7f635cf44c5841b0bc0c7a63e39a344d", "prediction": " The Golden Curry is a fast food restaurant that offers food for less than \u00a320 and is family-friendly. It is near The Bakers.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-9fdb4bd315424fe5a8cefbeae3a639de", "prediction": " The Mill is a pub in the city centre that serves English food at a high price.\nExplanation: The input data table contains information about a restaurant named \"The Mill\". The key-value pairs are: \"name\" (The Mill), \"eatType\" (pub), \"food\" (English), \"priceRange\" (high), and \"area\" (city centre). The output sentence should be grammatical and should contain all the information from the input data table. The sentence should be in the form of a declarative sentence, and should be written in fluent and natural-sounding English.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-bb029bcd253a433e8a966797905b6e68", "prediction": " The Golden Curry is a fast food restaurant that has a 5 out of 5 rating and is family-friendly.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-2fc79dc86148452ba466c1c36a8ec3bf", "prediction": " The Rice Boat is a high-priced restaurant with a 3 out of 5 customer rating. It is near the Rainbow Vegetarian Caf\u00e9.\nExplanation: The input is a data table of restaurant information. The first key-value pair is the name of the restaurant, which is \"The Rice Boat\". The second key-value pair is the price range, which is \"high\". The third key-value pair is the customer rating, which is \"3 out of 5\". The fourth key-value pair is the location of the restaurant, which is \"near the Rainbow", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-bf8f66c2836b40e986c87767227b2282", "prediction": " Cocum is a pub that offers a wide range of dishes and has a high customer rating. It is not family-friendly.\nExplanation: The input is a data table of restaurant information. The first key-value pair is the name of the restaurant, which is \"Cocum\". The second key-value pair is the type of restaurant, which is \"pub\". The third key-value pair is the price range, which is \"more than \u00a330\". The fourth key-value pair is the customer rating, which is \"5 out of 5\". The fifth key-value pair is", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-2fae72ce30ac416da60e3d32182e2542", "prediction": " \"Travellers Rest Beefeater is a restaurant located in the city centre, offering a wide range of dishes for \u00a320-25. It has a high customer rating and is near Caf\u00e9 Adriatic.\"\nExplanation: The input is a data table of restaurant descriptions. The first key-value pair is the name of the restaurant, which is \"Travellers Rest Beefeater\". The second key-value pair is the price range, which is \u00a320-25. The third key-value pair is the customer rating, which is \"high\". The fourth key-value pair", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-e1f9273253ac45138216fa6293121726", "prediction": " The Vaults is a pub located near Caf\u00e9 Adriatic that offers food for less than \u00a320 and has a low customer rating.\nExplanation: The input is a data table of restaurant information. The first key-value pair is the name of the restaurant, which is \"The Vaults\". The second key-value pair is the type of food served, which is \"pub\". The third key-value pair is the price range, which is \"less than \u00a320\". The fourth key-value pair is the customer rating, which is \"low\". The fifth key-value pair is the location", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-63c7803f900c458e8aa92a347b5c7574", "prediction": " The Giraffe is a coffee shop that offers a range of prices and has an average customer rating. It is not family-friendly and is near The Bakers.\nExplanation: The input data table contains information about a restaurant named \"The Giraffe\". The first key-value pair is \"name\", which is \"The Giraffe\". The second key-value pair is \"eatType\", which is \"coffee shop\". The third key-value pair is \"priceRange\", which is \"less than \u00a320\". The fourth key-value pair is \"customer rating\", which is \"", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-86e2be5f64ef4be88bd6a4826f1c46d4", "prediction": " Midsummer House is an Italian restaurant with a high price range. It has a 1 out of 5 customer rating and is near All Bar One.\nExplanation: The input is a string of key-value pairs. The key-value pairs are separated by a colon (:). The first key-value pair is \"name\", which is \"Midsummer House\". The second key-value pair is \"food\", which is \"Italian\". The third key-value pair is \"priceRange\", which is \"high\". The fourth key-value pair is \"customer rating\", which is \"1 out", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-21e1dfb4d704407ea8bcc38f6f943006", "prediction": " The Waterman is an Indian restaurant located in the city centre. It has an average customer rating and is not family-friendly.\nExplanation: The input data table contains information about a restaurant named \"The Waterman\". The first key-value pair is \"name\", which is the name of the restaurant. The second key-value pair is \"food\", which is the type of food served at the restaurant. The third key-value pair is \"priceRange\", which indicates the price range of the restaurant's menu. The fourth key-value pair is \"customer rating\", which is the average customer rating of", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-da2617a79a434cb7b77b665cdff80ebd", "prediction": " The Twenty Two is a riverside restaurant with a low customer rating and is not family-friendly. It is near The Rice Boat.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-333431c19e914a83b778ed5d1391b43a", "prediction": " Alimentum is a restaurant located in the city centre that offers a variety of English dishes at affordable prices. It is not family-friendly.\nExplanation: The input is a string of key-value pairs. The key-value pairs are separated by a colon (:). The first key-value pair is the name of the restaurant, which is \"Alimentum\". The second key-value pair is the food type, which is \"English\". The third key-value pair is the price range, which is \"less than \u00a320\". The fourth key-value pair is the area, which is", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-de856157919b4d0eb7d1bf7aff14ec54", "prediction": " \"Browns Cambridge is a coffee shop that serves fast food. It has a customer rating of 3 out of 5 and is located riverside, making it a family-friendly option near the Crowne Plaza Hotel.\"\nExplanation: The input is a data table of restaurant descriptions. The first key-value pair is the name of the restaurant, which is \"Browns Cambridge\". The second key-value pair is the type of eatery, which is \"coffee shop\". The third key-value pair is the food served, which is \"fast food\". The fourth key-value", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-aed1fc627aaa499895974355dd89bf3c", "prediction": " \"Zizzi is a restaurant located in the riverside area that offers affordable meals.\"\nExplanation: The input is a data table with the following columns: name, eatType, priceRange, and area. The name column contains the name of the restaurant, the eatType column contains the type of restaurant (restaurant, cafe, etc.), the priceRange column contains the price range of the restaurant (cheap, moderate, expensive), and the area column contains the location of the restaurant (riverside, downtown, etc.).\nThe output sentence should be a natural and grammat", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-8d1d43f2d5ee4fbdad902125c7df53ef", "prediction": " \"Wildwood is a pub that serves English food at a cheap price. It has a 5 out of 5 customer rating.\"\nExplanation: The input is a data table of restaurant information. The first key-value pair is the name of the restaurant, which is \"Wildwood\". The second key-value pair is the type of food served, which is \"pub\". The third key-value pair is the type of food, which is \"English\". The fourth key-value pair is the price range, which is \"cheap\". The fifth key-value pair is the customer rating, which is", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-005d0b9f9d3b4c8ea453a6098ca258ff", "prediction": " \"Strada is a pub that serves Japanese food. It has a 5 out of 5 rating and is near Yippee Noodle Bar.\"\nExplanation: The input is a data table of restaurant descriptions. The first key-value pair is the name of the restaurant, which is \"Strada\". The second key-value pair is the type of eatery, which is \"pub\". The third key-value pair is the type of food served, which is \"Japanese\". The fourth key-value pair is the customer rating, which is \"5 out of 5\". The fifth key", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-36f03b342a964fbf9aeeb76deeb43086", "prediction": " The Cricketers is a coffee shop that serves Japanese food. It has a rating of 5 out of 5 and is family-friendly. It is near The Portland Arms.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-53ce787b5fc44e02a60af81b9f2c04da", "prediction": " The Golden Curry is a French restaurant with a 5 out of 5 customer rating and is family-friendly.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-b4a82e4a264d4e669c91d454fcbaf49b", "prediction": " The Mill is a coffee shop that serves fast food at a cheap price. It is located on the riverside and is near The Sorrento.\nExplanation: The input is a data table of restaurant descriptions. The first key-value pair is the name of the restaurant, which is \"The Mill\". The second key-value pair is the type of food served, which is \"coffee shop\". The third key-value pair is the price range, which is \"cheap\". The fourth key-value pair is the area where the restaurant is located, which is \"riverside\". The fifth key", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-7179050011c44c8baa055a8ff652ddf8", "prediction": " The Rice Boat is an Italian restaurant located in the city centre. It has an average customer rating and is not family-friendly. It is near the Express by Holiday Inn.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-b69f9dd346af422881b29dd39f315860", "prediction": " The Cricketers is a French restaurant located near All Bar One.\nExplanation: The input is a data table of restaurant descriptions. The first key-value pair is the name of the restaurant, which is \"The Cricketers\". The second key-value pair is the type of food served, which is \"French\". The third key-value pair is the location of the restaurant, which is \"near All Bar One\". The output sentence is a natural and grammatical English sentence that contains all the information from the input.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-6ce6eacf622e40f29288cdd02900ebe1", "prediction": " Strada is a Japanese restaurant that offers moderately priced food. It has a 1 out of 5 customer rating and is family-friendly. It is near Rainbow Vegetarian Caf\u00e9.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-73d21febca284ce2b7de0410b39d821e", "prediction": " Fitzbillies is a moderately priced restaurant located near Rainbow Vegetarian Caf\u00e9.\nExplanation: The input is a data table of restaurant descriptions. The first key-value pair is the name of the restaurant, which is Fitzbillies. The second key-value pair is the price range, which is moderate. The third key-value pair is the location of the restaurant, which is near Rainbow Vegetarian Caf\u00e9. The output sentence is a natural and grammatical English sentence that contains all the information from the input.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-1e56074462a34e3591bf2e28cc76e599", "prediction": " The Punter is a coffee shop that serves Japanese food at moderate prices. It has a rating of 1 out of 5 and is family-friendly. It is near Caf\u00e9 Sicilia.\nExplanation: The input is a data table of restaurant information. The first key-value pair is the name of the restaurant, which is \"The Punter\". The second key-value pair is the type of food served, which is \"Japanese\". The third key-value pair is the price range, which is \"moderate\". The fourth key-value pair is the customer rating, which is", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-259deaffa0e14b7aafdfc61cfdc7a38b", "prediction": " The Giraffe restaurant is a pub that serves Italian food in a riverside area that is family-friendly.\nExplanation: The input is a data table of restaurant information. The key-value pairs are separated by commas. The first key-value pair is \"name\", which is \"Giraffe\". The second key-value pair is \"eatType\", which is \"pub\". The third key-value pair is \"food\", which is \"Italian\". The fourth key-value pair is \"area\", which is \"riverside\". The fifth key-value pair is \"familyFri", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-025bc537854e47059d93eb3f72ef33cc", "prediction": " The Eagle is a coffee shop in the city centre that serves French food at moderate prices. It has a 1 out of 5 customer rating and is family-friendly. It is near Burger King.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-ab6129f8ff5942ba99025727bf68df34", "prediction": " The Phoenix is a riverside restaurant that serves English food and has a low price range.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-6675b69bcabc40b29eb3ca361a0281be", "prediction": " Fitzbillies is a cheap restaurant with an average customer rating and it is family-friendly. It is near the Express by Holiday Inn.\nExplanation: The input is a string of key-value pairs. The key-value pairs are separated by a colon (:). The first key-value pair is \"name\", which is \"Fitzbillies\". The second key-value pair is \"priceRange\", which is \"cheap\". The third key-value pair is \"customer rating\", which is \"average\". The fourth key-value pair is \"familyFriendly\", which is \"", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-8240d21c73774e7085e6e3034cbb3bdd", "prediction": " \"Clowns is a pub located in riverside. It has a high price range and a customer rating of 3 out of 5.\"\nExplanation: The input is a data table of restaurant descriptions. The first key-value pair is the name of the restaurant, which is \"Clowns\". The second key-value pair is the type of restaurant, which is \"pub\". The third key-value pair is the price range, which is \"high\". The fourth key-value pair is the customer rating, which is \"3 out of 5\". The fifth key-value pair is the location", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-9cd872c78bd34a579ccb40e53f6ec20e", "prediction": " The Punter is a moderately priced restaurant located on the riverside, which is family-friendly and is near The Portland Arms.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-686e44b7ebb84ba998cf720e2ec301e1", "prediction": " The Golden Curry is a moderately priced Chinese restaurant that is not family-friendly and is near The Bakers.\nExplanation: The input is a data table of restaurant information. The first key-value pair is the name of the restaurant, which is \"The Golden Curry\". The second key-value pair is the food type, which is \"Chinese\". The third key-value pair is the price range, which is \"moderate\". The fourth key-value pair is the family-friendly status, which is \"no\". The fifth key-value pair is the location, which is", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-588f621fc89f4f00a2828baa98d657a4", "prediction": " The Golden Palace is a moderately priced restaurant.\nExplanation: The input data table contains the name, eatType, and priceRange fields. The name field contains the name of the restaurant, and the eatType field contains the type of restaurant (e.g. \"restaurant\"). The priceRange field contains the price range of the restaurant (e.g. \"moderate\"). The output sentence should contain all of this information in a natural and grammatical English sentence.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-7a22c42055d44d73949387f5fa83b0b7", "prediction": " The Golden Curry is a fast food restaurant that serves Indian cuisine. It has a price range of \u00a320-25 and is family-friendly. It is located near The Bakers.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-947bcf580fc84ee78ebbf19644444ad6", "prediction": " The Olive Grove is a pub located on the riverside. It offers English cuisine at a high price range. It is not family-friendly.\nExplanation: The input is a data table of restaurant information. The first key-value pair is the name of the restaurant, which is \"The Olive Grove\". The second key-value pair is the type of food served, which is \"pub\". The third key-value pair is the price range, which is \"high\". The fourth key-value pair is the location, which is \"riverside\". The fifth key-value pair", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-08e99b64540647a59d314d675720bfc3", "prediction": " \"Travellers Rest Beefeater is a restaurant that serves food for more than \u00a330 and is family-friendly.\"\nExplanation: The input is a data table of restaurant descriptions. The first key-value pair is the name of the restaurant, which is \"Travellers Rest Beefeater\". The second key-value pair is the type of food served, which is \"restaurant\". The third key-value pair is the price range, which is \"more than \u00a330\". The fourth key-value pair is the family-friendliness of the restaurant, which is \"yes", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-b877c721990546eca8bf56bfc48ba142", "prediction": " The Eagle is a coffee shop that serves Indian food at a cheap price. It has a rating of 5 out of 5 and is located riverside, making it a great place for families. It is near Burger King.\nExplanation: The input data table is in the form of a list of key-value pairs. The first key-value pair is the name of the restaurant, which is \"The Eagle\". The second key-value pair is the type of food served, which is \"coffee shop\". The third key-value pair is the price range, which is \"cheap\".", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-87356b3e68344d39a097d4a02c052942", "prediction": " The Golden Palace is a restaurant that serves delicious food for a reasonable price.\nExplanation: The input data table contains the name of the restaurant, its type (restaurant), and its price range (less than \u00a320). The output sentence is a natural and grammatical English sentence that contains all the information from the input.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-bafb2db1022e44448ba97c5605749097", "prediction": " Bibimbap House is an Indian restaurant located riverside, near The Rice Boat.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-2dd49f7c025547b7b82a71c39d6ad992", "prediction": "Browns Cambridge is a fast food restaurant located in the city centre. It is family-friendly and is near The Sorrento.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-ab684e68705245bd9d7036e6adfed5b5", "prediction": " The Punter is a coffee shop that serves fast food at moderate prices. It has a rating of 3 out of 5 and is family-friendly. It is near Caf\u00e9 Sicilia.\nExplanation: The input is a data table of restaurant information. The first key-value pair is the name of the restaurant, which is \"The Punter\". The second key-value pair is the type of food served, which is \"coffee shop\". The third key-value pair is the price range, which is \"moderate\". The fourth key-value pair is the customer rating, which", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-ccfc145e5013485aabbde0771491d8a5", "prediction": " Zizzi is a pub that serves Italian food. It has a rating of 1 out of 5 and is family-friendly.\nExplanation: The input is a string of key-value pairs. The key is the name of the restaurant, and the value is a description of the restaurant. The output should be a natural and grammatical English sentence containing all the information from the input. In this case, the sentence is \"Zizzi is a pub that serves Italian food. It has a rating of 1 out of 5 and is family-friendly.\"", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-e66ca906a6da47cd96301afd16876634", "prediction": " The Punter is a coffee shop that serves Italian food in the \u00a320-25 price range. It is highly rated and family-friendly. It is near Caf\u00e9 Sicilia.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-4b6d2b2162c743648253c07e96c27576", "prediction": " The Vaults is a French restaurant that offers meals for more than \u00a330 and is family-friendly.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-82321e358d1b4028a92352a80d682d1b", "prediction": " The Dumpling Tree is a restaurant that serves Italian food at a cheap price.\nExplanation: The input is a data table with the name, type, and food of a restaurant. The output is a sentence that describes the restaurant, including the information from the input.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-39c95be5c7dc41339eff28d09002e51e", "prediction": " The Waterman is a fast food restaurant located on the riverside. It has a moderate price range and has a 3 out of 5 customer rating. It is not family-friendly.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-b44cc629f1ac4fd984e66ddcaeae1b3a", "prediction": " The Phoenix is a riverside restaurant that serves English food and has a high customer rating.\nExplanation: The input is a data table of restaurant descriptions. The first key-value pair is the name of the restaurant, which is \"The Phoenix\". The second key-value pair is the food served, which is \"English\". The third key-value pair is the customer rating, which is \"high\". The fourth key-value pair is the area where the restaurant is located, which is \"riverside\".\nThe output sentence should contain all the information from the input, in a grammatical and natural-", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-c893df4737944ded87805ab3ee449da2", "prediction": " The Rice Boat is a moderately priced French restaurant located riverside, with a family-friendly atmosphere. It is near the Express by Holiday Inn.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-fa345cf096bd4767937ac9c0afc32850", "prediction": " Fitzbillies is a coffee shop that serves Indian food. It has a high price range and is not family-friendly. It is located on the riverside.\nExplanation: The input data table contains information about a restaurant named Fitzbillies. The first key-value pair is the name of the restaurant, which is \"Fitzbillies\". The second key-value pair is the type of food served, which is \"coffee shop\". The third key-value pair is the price range, which is \"high\". The fourth key-value pair is the customer rating, which is \"", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-da47348e5d4143b49b3a5b033531e651", "prediction": " Loch Fyne is a restaurant that serves fast food and is family-friendly.\nExplanation: The input is a data table with the following columns: name, eatType, food, and familyFriendly. The first row contains the values \"Loch Fyne\", \"restaurant\", \"Fast food\", and \"yes\". The output sentence should be \"Loch Fyne is a restaurant that serves fast food and is family-friendly.\"", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-b0976df4abbe474984ae3da483a6b367", "prediction": " The Golden Palace is a riverside restaurant with an average customer rating.\nExplanation: The input is a data table with the following columns: name, eatType (restaurant or bar), and customer rating. The output sentence should contain all the information from the input, including the name, eatType, and customer rating. The sentence should be grammatical and fluent, and should be written in the active voice.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-fc6b02b9274c4d509abba765ec1df664", "prediction": "Browns Cambridge is a French restaurant with a price range of \u00a320-25. It has a customer rating of 3 out of 5.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-eaded70ce10141a69eac67a56b9a9020", "prediction": " The Waterman is a cheap Indian restaurant located on the riverside. It has an average customer rating and is family-friendly.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-6993553ec46346b6bf962e7fe6813d15", "prediction": " The Olive Grove is a pub located on the riverside that serves French cuisine. It has a price range of more than \u00a330 and is family-friendly.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-1263c77787134a16a717306df87ad3e2", "prediction": " The Olive Grove is a pub located on the riverside that serves Indian food in the \u00a320-25 price range. It is family-friendly.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-4831e65046c14e7499b3e6b4a0cde7bc", "prediction": " \"Wildwood is a restaurant with a 5-star rating located near Caf\u00e9 Rouge.\"\nExplanation: The input is a data table of restaurant descriptions. The first key-value pair is the name of the restaurant, which is \"Wildwood\". The second key-value pair is the type of eatery, which is \"restaurant\". The third key-value pair is the customer rating, which is \"5 out of 5\". The fourth key-value pair is the location of the restaurant, which is \"near Caf\u00e9 Rouge\".\nThe output sentence should be a natural and", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-ad626d5b37c3462ab24a928ab64faba2", "prediction": " Strada is an Indian restaurant located near Rainbow Vegetarian Caf\u00e9. It has a price range of \u00a320-25 and has a high customer rating. It is not family-friendly.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-044894ae65d5485487f99df2552993c1", "prediction": " The Wrestlers is a French restaurant with a price range of less than \u00a320 and has a low customer rating, but it is not family-friendly.\nExplanation: The input data table contains the name, food, priceRange, and customer rating of a restaurant. The output sentence should contain all the information from the input, in a natural and grammatical English sentence. The sentence should start with \"The Wrestlers is a French restaurant with a price range of less than \u00a320 and has a low customer rating, but it is not family-friendly.\"", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-0033e2df2f204b4198994a3773b585d1", "prediction": " The Eagle is a coffee shop serving Japanese food in the city centre. It has a moderate price range and is not family-friendly. It is near Burger King.\nExplanation: The input data table contains information about a restaurant called \"The Eagle\". The output sentence should describe the restaurant, including its name, type of food, price range, customer rating, location, and whether it is family-friendly or not. The sentence should be grammatical and natural-sounding.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-81483f28251e47229ad96f41c7c927b3", "prediction": " Fitzbillies is a coffee shop that serves French cuisine. It has a high price range and is rated 3 out of 5 by customers. It is located on the riverside and is family-friendly.\nExplanation: The input is a data table of restaurant information. The first key-value pair is the name of the restaurant, which is Fitzbillies. The second key-value pair is the type of eatery, which is coffee shop. The third key-value pair is the type of food served, which is French. The fourth key-value pair is the price range", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-c8440bcc72bf41cc98d7ee19ac91bba5", "prediction": " Cotto is a riverside restaurant located near All Bar One, with a high price range.\nExplanation: The input is a data table of restaurant descriptions. The first key-value pair is the name of the restaurant, which is \"Cotto\". The second key-value pair is the price range, which is \"high\". The third key-value pair is the area, which is \"riverside\". The fourth key-value pair is the location of the restaurant, which is \"near\" All Bar One. The output sentence should contain all this information in a natural and grammatical English sentence.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-fd0f3450cd264b0fb752193f4c321ecf", "prediction": "The Loch Fyne restaurant is located in the city centre and is near The Rice Boat. It serves Indian food and has a 5 out of 5 customer rating.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-947283dfd2484f88befc7fba326a742c", "prediction": " Bibimbap House is a Chinese restaurant located riverside, near Clare Hall. The price range is high.\nExplanation: The input is a data table containing information about a restaurant. The first key-value pair is the name of the restaurant, which is \"Bibimbap House\". The second key-value pair is the type of food served, which is \"Chinese\". The third key-value pair is the price range, which is \"high\". The fourth key-value pair is the location, which is \"riverside\". The fifth key-value pair is the nearby landmark, which is", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-856862d87240478594c95748e68b6511", "prediction": " The Green Man is a riverside restaurant that serves English food and has a price range of less than \u00a320. It is family-friendly and is near All Bar One.\nExplanation: The input is a string of key-value pairs. The key-value pairs are separated by commas. The first key-value pair is \"name\", which is \"Green Man\". The second key-value pair is \"food\", which is \"English\". The third key-value pair is \"priceRange\", which is \"less than \u00a320\". The fourth key-value pair is \"area\", which is", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-fc711aeceac14d328a92f6be533cd026", "prediction": " The Waterman is a French restaurant located on the riverside. It has a low price range and is not family-friendly.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-582deb8e05be4ac5aa4224b68ad250be", "prediction": " Alimentum is a fast food restaurant located in the city centre. It has a moderate price range and a rating of 3 out of 5. It is near Yippee Noodle Bar.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-ffce112d1d4442d094492196106134b3", "prediction": " The Wrestlers is a French restaurant that has a price range of \u00a320-25 and has a high customer rating. It is family-friendly.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-0fb37d6a33794230828c6134ecb98c8a", "prediction": " The Eagle is a coffee shop serving Italian food in the city centre. It has a high price range and is not family-friendly. It is near Burger King.\nExplanation: The input data table contains information about a restaurant called \"The Eagle\". The output sentence should describe the restaurant in fluent English, using the information from the input table. The sentence should include the name of the restaurant, its type (coffee shop), the type of food it serves (Italian), its price range (high), its customer rating (1 out of 5), its location (city centre), and whether", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-ed16020036564a32a5622d6d136865be", "prediction": " \"Blue Spice is a moderately priced restaurant located on the riverside. It has a 1 out of 5 customer rating and is family-friendly.\"\nExplanation: The input is a data table of restaurant information. The first key-value pair is the name of the restaurant, which is \"Blue Spice\". The second key-value pair is the price range, which is \"moderate\". The third key-value pair is the customer rating, which is \"1 out of 5\". The fourth key-value pair is the area, which is \"riverside\". The fifth key", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-f83da1b60dbd44bea518a73ad29f1dac", "prediction": "The Rice Boat is a French restaurant located on the riverside. It has a low customer rating and is not family-friendly.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-ac27bddb6f124524bf04e6d0c1da83aa", "prediction": " The Cricketers is a coffee shop that serves French food. It has a rating of 5 out of 5 and is family-friendly. It is near The Portland Arms.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-9c8d1ef2199545cf8ee94ec4caf62fce", "prediction": " The Wrestlers is a coffee shop that serves Indian food. It has a price range of more than \u00a330 and is located on the riverside. It is family-friendly.\nExplanation: The input is a data table of restaurant descriptions. The first key-value pair is the name of the restaurant, which is \"The Wrestlers\". The second key-value pair is the type of food, which is \"Indian\". The third key-value pair is the price range, which is \"more than \u00a330\". The fourth key-value pair is the location, which is \"riverside", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-d087f98308af4926bdb573ffdbee69e2", "prediction": "The Phoenix is a French restaurant located on the riverside with a customer rating of 3 out of 5.\nExplanation: The input is a data table of restaurant information. The first key-value pair is the name of the restaurant, which is \"The Phoenix\". The second key-value pair is the food type, which is \"French\". The third key-value pair is the customer rating, which is \"3 out of 5\". The fourth key-value pair is the location, which is \"riverside\".\nThe output sentence should contain all the information from the input, in a natural and", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-f552199ff43845d6908d3bcc4ca55a08", "prediction": " The Olive Grove is a pub located on the riverside that serves Italian food in a family-friendly environment.\nExplanation: The input data table contains information about a restaurant named \"The Olive Grove\". The restaurant is a pub, and it serves Italian food. The price range is high, and the restaurant is located on the riverside, making it family-friendly.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-58c521fa3a364d33a6046054a9f9799d", "prediction": " The Vaults is a restaurant that serves Italian food.\nExplanation: The input is a data table with three columns: name, eatType (restaurant or bar), and food (Italian). The output sentence should be a natural and grammatical English sentence containing all the information from the input. In this case, the sentence should be \"The Vaults is a restaurant that serves Italian food.\"", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-84b8e2302e134eb99ecc30e73536b78f", "prediction": " \"Wildwood is a pub that serves fast food. The price range is more than \u00a330, and the customer rating is high.\"\nExplanation: The input is a data table of restaurant descriptions. The first key-value pair is the name of the restaurant, which is \"Wildwood\". The second key-value pair is the type of eatery, which is \"pub\". The third key-value pair is the type of food served, which is \"Fast food\". The fourth key-value pair is the price range, which is \"more than \u00a330\". The fifth key-value", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-564dcf56775e46048701cf810455ced9", "prediction": " The Mill is a pub located on the riverside that serves Japanese food. The price range is more than \u00a330.\nExplanation: The input is a data table of restaurant descriptions. The first key-value pair is the name of the restaurant, which is \"The Mill\". The second key-value pair is the type of food served, which is \"Japanese\". The third key-value pair is the price range, which is \"more than \u00a330\". The fourth key-value pair is the location of the restaurant, which is \"riverside\".\nThe output sentence should be a natural", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-08909c0010b644fd998ccf83daf7a9f6", "prediction": " The Rice Boat is a fast food restaurant located riverside. It has a family-friendly atmosphere and is near the Express by Holiday Inn. The food is cheap and the customer rating is 5 out of 5.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-45b25ee80bd74b3085a911a8db7676d9", "prediction": " The Olive Grove is a pub in the city centre that serves Japanese food. It has a price range of more than \u00a330 and is not family-friendly.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-fcd2cfbcbc0b460fad0d55dbe595a787", "prediction": " The Olive Grove is a pub in the city centre that serves French food for less than \u00a320. It is family-friendly.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-0b6e8fa0dbc74a26a3ab4c4d609242fd", "prediction": " The Eagle is a coffee shop that serves Italian food at a cheap price. It has an average customer rating and is located on the riverside. It is not family-friendly and is near Burger King.\nExplanation: The input is a data table of restaurant information. The first key-value pair is the name of the restaurant, which is \"The Eagle\". The second key-value pair is the type of food served, which is \"coffee shop\". The third key-value pair is the price range, which is \"cheap\". The fourth key-value pair is the customer rating,", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-b621f50c2aab440d8e0dac83a13bbb6d", "prediction": " Strada is an Indian restaurant that offers high-quality food, has an average customer rating, and is family-friendly. It is located near Rainbow Vegetarian Caf\u00e9.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-42c634cbf8274f45b0adb9e648f9c713", "prediction": " The Punter is a coffee shop that serves Japanese food at moderate prices. It has a rating of 1 out of 5 and is not family-friendly. It is near Caf\u00e9 Sicilia.\nExplanation: The input is a data table of restaurant information. The first key-value pair is the name of the restaurant, which is \"The Punter\". The second key-value pair is the type of food served, which is \"Japanese\". The third key-value pair is the price range, which is \"moderate\". The fourth key-value pair is the customer rating, which", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-a048d457295444808799cd6c3aabe752", "prediction": " The Golden Curry is a restaurant that serves English food, with a price range of more than \u00a330, and is family-friendly, located near The Bakers.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-4bd1c7af688b4c8b9fb1fd2a1f29ac46", "prediction": " The Wrestlers is a fast food restaurant with an average customer rating and it is family-friendly.\nExplanation: The input data table contains information about a restaurant named \"The Wrestlers\". The first key-value pair is \"name\", which is the name of the restaurant. The second key-value pair is \"food\", which is the type of food served at the restaurant. The third key-value pair is \"priceRange\", which indicates the price range of the food. The fourth key-value pair is \"customer rating\", which is the average rating given by customers. The fifth key-value pair is", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-1cb9d071357948ec91af3401ff07b9ea", "prediction": " The Wrestlers is a coffee shop that serves French food at a cheap price. It is located riverside and is family-friendly. Nearby is Raja Indian Cuisine.\nExplanation: The input is a data table of restaurant descriptions. The first key-value pair is the name of the restaurant, which is \"The Wrestlers\". The second key-value pair is the type of food served, which is \"French\". The third key-value pair is the price range, which is \"cheap\". The fourth key-value pair is the location, which is \"riverside\". The", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-497bd08e2b6741a3b97dfa4c4977fa64", "prediction": " \"Travellers Rest Beefeater is a cheap restaurant located on the riverside. It has a rating of 5 out of 5 and is near Caf\u00e9 Adriatic.\"\nExplanation: The input is a string of key-value pairs. The key is the restaurant name, and the value is a description of the restaurant. The output should be a natural and grammatical English sentence containing all the information from the input. The sentence should be written in the past tense, and the information should be presented in a logical order.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-bf29e0264f9d406b90479f3c79f967d0", "prediction": " The Vaults is a fast food restaurant that offers a wide range of food at affordable prices. It is a family-friendly restaurant.\nExplanation: The input is a data table containing information about a restaurant. The first key-value pair is the name of the restaurant, which is \"The Vaults\". The second key-value pair is the type of food, which is \"Fast food\". The third key-value pair is the price range, which is \"high\". The fourth key-value pair is whether the restaurant is family-friendly or not, which is \"yes\".\nThe", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-88ea93ab827e45559e98b669e8c79f58", "prediction": " The Green Man is a Chinese restaurant located on the riverside. It is not family-friendly and is near All Bar One.\nExplanation: The input is a data table of restaurant descriptions. The first key-value pair is the name of the restaurant, which is \"Green Man\". The second key-value pair is the food type, which is \"Chinese\". The third key-value pair is the price range, which is \"less than \u00a320\". The fourth key-value pair is the area, which is \"riverside\". The fifth key-value pair is the family-friendly", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-e29ad6dc41684a97b2f5b1c15a878801", "prediction": " \"Clowns is a coffee shop that serves French food in the city centre. It has a rating of 5 out of 5 and is near Clare Hall.\"\nExplanation: The input is a data table of restaurant descriptions. The first key-value pair is the name of the restaurant, which is \"Clowns\". The second key-value pair is the type of food served, which is \"coffee shop\". The third key-value pair is the name of the restaurant's specialty, which is \"French\". The fourth key-value pair is the customer rating, which is", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-5532c6df6b43439cbcaf19d231b1e806", "prediction": " Zizzi is a coffee shop located in the city centre. It has a price range of \u00a320-25 and has a high customer rating. It is not family-friendly.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-21f30c192a4e403db80cb6f3f57657cd", "prediction": " The Mill is a pub located on the riverside that serves English food at a high price.\nExplanation: The input data table contains information about a restaurant named \"The Mill\". The key-value pairs are: \"name\" is \"The Mill\", \"eatType\" is \"pub\", \"food\" is \"English\", \"priceRange\" is \"high\", and \"area\" is \"riverside\". The output sentence should be a natural and grammatical English sentence that contains all the information from the input data table. The sentence should be in the form of a declarative sentence, and should be", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task957-563dfe6156574ce8b5b2dcb08784206a", "prediction": "The Taste of Cambridge is a pub located in the city centre that is family-friendly and near The Sorrento.", "task_name": "task957_e2e_nlg_text_generation_generate"}
{"id": "task827-d390d739b5bb4b9b96bf0e2a3b4b13a2", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-1a92a01c964c4c2b8fcdf91ca503ca3f", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-7ad81c2e258445d48f1881f734f0013a", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-7f4c3beaae0c498693447e0545c17a61", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-f3d3f8088f0f4a42b6ada264000d5d65", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-d37ed4e6353c4117ae45eb541721c993", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-8b6d905d9671418a9fe02141b2591c52", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-22580b6bf8c242d59b2b1bbd97a70326", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-f2fcaf67379a4f7195736dd7501fe4cf", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-01c829d8e0854c16ada660a07e8f81a3", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-81e4edee8ebb4bfdba7124b1467e87f6", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-e891ebd6680842cd8ddd44456cec0ba3", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-7925d830a2394bd79c0fdb8c8fb47033", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-48fddf84f80e459da454fb8ebf9aa3cf", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-798bd38d41a440c7a9e94c067a544f0f", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-497e7ad9aa3c4a1e89b4181f64851463", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-a2184bb65e304738bfb98d938b76805d", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-c80c31c54ecd42a994fc3684442d405e", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-7c4a54cefb5245778c11c7420ff0fa32", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-3e68c145937b4786ba7bc64efc5e1abf", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-35cc22cf1c8e4affb45f715919c0de72", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-85ab3de6806b4bf699c4b9d5dcd875ff", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-4310415cf892438ca88b6f6cbf0e7980", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-2ed7c5e80dc142b1b592c9b7812e1668", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-9b542320422043478333a485413b5f71", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-35fa319fa1ff46f8b6270a2fe6231741", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-4984bf5b61f04713b83273e4019e622d", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-d369de948fd34614babf293b3b4d27cf", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-9807d330805245fca49568f63488ed40", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-89184902681e4acb8230dcee93736fd3", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-9439ec66a2754d80a3d72089ab76c294", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-efc1692e67af407bb4987ed219ca7073", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-4bc81b5c6d024149aa3922c725572ead", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-41e1d7397b654c1ea0344ff978731f03", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-4858301e9f7d4e7a931e4629f1fb0dfa", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-2b1fd382666f4586aa0a6c1f078e8da8", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-2867348e117c43ccad07016ae8224e20", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-741c639e3b3f4a39828d805ed6021df4", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-c7066513511648019837cf5dc12f5f06", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-6459ff4ba4ae4612a337509831e790cb", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-490357202b674e6382f132bac52e1326", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-793ecde46ff542b6bce0032b7d4ad14e", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-4e78029c253b4cc5b4a47524aab498c7", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-3a8f6a07c09c45f0a17c53c119267f7d", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-221af4003e264ea7b6688d06f643de38", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-089d981629d64da79771c0ecb563f444", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-0c7c8dc20f6744808e3e682cd58e6300", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-83b8bb4256f44607ae1e4b986e62e7a4", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-ee219f4dc1d24e5f9d1be24bed8cc07a", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-8e8c0c477745431c9299a3e4ddcd8203", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-b471629d08a3406cb56f112b1ae0f501", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-a493fc9fb994429c97c844dba0909837", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-e213ffe90c6d4e649ce1c96ebb3b2744", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-5c53c848eb7848b287db80ba0d375ea9", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-2b6f2a3bda034377851e11991ccdb082", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-34ff2e1bdca745bfaef44ea66695918f", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-78d948d79589407f832fa2dbbe7a7148", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-cd8659352f164960a1f62efcf523dffd", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-432facec4ae5402db2e8d9b543dd5277", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-7dad16f965fe4c5190e62f521692363c", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-9a3ca9519cdb4e6f8bf651eb6b057697", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-9a19282613bc403cb3e3086ad7e0e036", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-ff507bbcd0c647329a8dc2aeea842df0", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-d995342b00254aed9fdf43f72f72b1d8", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-e8c4dd6b71e64721be4d932e27d6b559", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-58e157edee6e47feb1ec9ff5369b7a50", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-21b5648e450f4377aa3f321c8d960137", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-73bb3b0d3d224332bbbd6499f357693a", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-f9149023db154d92a70f3fefc0867c0a", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-23345dfb9569411db8a3446e0dcbc65d", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-4d99a6abe65b47b081e5d8a0abe007c7", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-f18760af19c14820ae2143f55b9e8fdd", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-2985d3cdd0ff496898425f7000b44811", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-4c1428e6d9f644e398adf61966696b49", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-d9b96f3be2914e268d6ac35f7bdcfc5f", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-9da88c942a2140ee8a9139bf5cb585d2", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-328267af2ec7455b991ba44ea38dcf2a", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-f50f3089d25b4c29ab71b7390df93648", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-9f6c446875c14bc89a153c844966076f", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-59ad07eaae434935b6d0f108e4417f92", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-3762a02817f947e68ce7b39ca12f91c4", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-3b9f95e88e704b6e9f8638f6bd855e0b", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-b748aad564154cf792c9dd886f25bf41", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-297862a3bf904563a52d7813778f8601", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-9daec8429962473985051c44b56f7954", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-a6d7e815fa644301a16efb94778dc419", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-bf3ba6d214a041f2ae3326fed6937586", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-9e1b17af3e5149f9b89d195262b815fa", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-2c639c6bc89b4f8c92e10d33020c2fb1", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-76cbc784a1d444ecb9e0cdb7937a7f04", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-9e81dfca58c049e7bc3f24d1faf44b88", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-55c71b8a9ddf41ecbdd8b7b203ff4835", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-05803994664e4054a70f6b916885a2cb", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-5e4db495d9e94f01914d629af127ae7a", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-d8cfbe9dd8da4e57b65236375d5bb125", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-4d74ac5cedb3432b81e53a6fc168df3f", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-6a95a73efea7416186591d0c4e8c72bc", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-a05cac3a6ff64e4cb3237c5eeb1c262d", "prediction": " 1", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-5fea38558caa4ac686c4ea87195023ba", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task827-06c399280b314f7996ee727d24956e90", "prediction": " 2", "task_name": "task827_copa_commonsense_reasoning"}
{"id": "task1659-e7833140f1b142fcaf28930a33260fd6", "prediction": " \"Senior-Level Pay Cap Act of 2", "task_name": "task1659_title_generation"}
{"id": "task1659-b24b646ab37c4bac9c10b78eda67bbfc", "prediction": " The Fairness in Taxation Act of 2011: Increasing Individual Income Tax Rates for High-Income Taxpayers and Adjusting Capital Gains Tax Bracket.", "task_name": "task1659_title_generation"}
{"id": "task1659-25eed9a957794f2db92f7cd00add2fb5", "prediction": " Electronic Prescription Act.", "task_name": "task1659_title_generation"}
{"id": "task1659-d7079f64c563488d9ee7290bd274810b", "prediction": " Crop Insurance Premium Reduction Act of 2013", "task_name": "task1659_title_generation"}
{"id": "task1659-cf5d8e526f2a42fcab006afed42a4962", "prediction": " \"Bills Act: Requiring Congressional Bills to Cite Constitutional Authority\"", "task_name": "task1659_title_generation"}
{"id": "task1659-c3f7b4c7e2814d0aaddc89f3f80370cd", "prediction": " The Measures to Encourage Results in Teaching Act of 1998 - Amends Title II of the Elementary and Secondary Education Act of 1965 to establish a new part D: State Incentives for Teacher Testing and Merit Pay. Directs the Secretary of", "task_name": "task1659_title_generation"}
{"id": "task1659-db16b3a1d0e1413fb25af1ac3f852532", "prediction": " School Anti-Violence Empowerment Act - Authorizes the Secretary of Education to award competitive grants to local educational agencies (LEAs) for: (1) crisis intervention programs, including school counselors; and (2) school safety programs for students, staff, and school facilities.Amends the Omnibus Crime Control and Safe Streets Act of 1968 to extend through FY 2006 the authorization of appropriations for certain grant programs for Community Policing and \"Cops on the Beat\" (COPS program). Eliminates provisions for decre", "task_name": "task1659_title_generation"}
{"id": "task1659-c7a763e6859f4a96be310647f7022c51", "prediction": " Adults Achieving the American Dream Act of 2007 - Amends the Adult Education and Family Literacy Act to reauthorize appropriations through FY2013 for adult education and literacy services, including integrated English literacy and civics education programs. Increases the maximum amounts that can be reserved from such funds for: (1) the National Institute for Literacy;", "task_name": "task1659_title_generation"}
{"id": "task1659-2194150c7e1c43af9e6d88b6c3e0752f", "prediction": " Drug-Free Workplace Program Reauthorization Act of 2004 - Amends the Small Business Act to extend through FY 2006 the authorization for the drug-free workplace demonstration program (program).\n\nAuthorizes the Administr", "task_name": "task1659_title_generation"}
{"id": "task1659-edd5707df35f41cb8bbe96a7bc2f37e9", "prediction": " The California State Bill to Increase Research and Development Tax Credits.", "task_name": "task1659_title_generation"}
{"id": "task1659-71cfa81708784b1e94cc6e52c27d0eb3", "prediction": " Cybersecurity Education Enhancement Act of 2005 - Directs the Secretary of Homeland Security, acting through the Assistant Secretary of Cy", "task_name": "task1659_title_generation"}
{"id": "task1659-93fc6e674bc0449c8552719b8499661d", "prediction": " The Student Disciplinary Fairness Act of 2013 - Amends the Juvenile Justice and Delinquency Prevention Act of 1974 to establish an Office of School and Discipline Policy for the purpose of reducing the number of juveniles who are incarcerated and develop a criminal record based on activity that occurs while the juvenile is at school. Directs the Office to: (1) collect and publish data relating to the arrest and incarceration of juveniles for violations of school policies or rules; (2) work with states, local governments", "task_name": "task1659_title_generation"}
{"id": "task1659-9be1d81db87241ec90dd92939a0ec8a5", "prediction": " Home Energy Savings Act of 2012 - Amends the Internal Revenue Code to Permanently Increase Tax Credit for Nonbusiness Energy Property Expenditures.", "task_name": "task1659_title_generation"}
{"id": "task1659-c79dd3174e3a474dbb17e3e8fdb696c8", "prediction": " Housing Fairness Act of 200", "task_name": "task1659_title_generation"}
{"id": "task1659-84b14474e3e54a4caca37d2f98f33bfd", "prediction": " No Social Security Numbers and Benefits for Illegal Aliens Act of 2014.", "task_name": "task1659_title_generation"}
{"id": "task1659-5302894baefe45d294380ad50e1a48dc", "prediction": " The Senior Citizens' Freedom to Work Act of 1998.", "task_name": "task1659_title_generation"}
{"id": "task1659-da1b0734c4cb4aaaa4dddba7ee241d3a", "prediction": " Homeland Security Information Sharing Enhancement Act of 2006 - Amends the Homeland Security Act of 2002 to require the Under Secretary for Information and Analysis to implement a Homeland Security Advisory System to provide advisories and alerts regarding threats to homeland security. Requires such an advisory or alert to: (1) include information on protective measures and countermeasures; (2) be limited in scope to a specific region, locality, or economic sector; and (3) not use color designations as the exclusive means of specifying threat conditions.\n\n", "task_name": "task1659_title_generation"}
{"id": "task1659-e81c7109716c42bc9cf6993df1d5125e", "prediction": " Lower Brule Sioux Tribe Infrastructure Development Trust Fund Act", "task_name": "task1659_title_generation"}
{"id": "task1659-6f0da95f89e04509a004dbc018fb2b67", "prediction": " Railroad Grade Crossing Safety Act of 1994", "task_name": "task1659_title_generation"}
{"id": "task1659-ab0fd50d350149f4a4e28810bf157562", "prediction": " Patent Application Publication Act of 1995 - Requires each patent application, except applications for design patents and provisional applications, to be published as soon as possible after 18 months from the earliest filing date for which a benefit is sought, except for an application that is no longer pending or one subject to a secrecy order.  Permits earlier publication at the applicant's request. Prohibits disclosure of information concerning published applications except as determined by the Commissioner of Patents.  Prohibits, upon request by certain independent inventors, the publication of rejected applications", "task_name": "task1659_title_generation"}
{"id": "task1659-bd0cf0a840ac479c8192a05955bedea0", "prediction": " \"Prohibits retaliation against whistleblowers\"", "task_name": "task1659_title_generation"}
{"id": "task1659-bf9bc020b7014be2aa02a3590dfd42f8", "prediction": " The Wounded Warriors Act of 2007 - Provides Immediate Benefits for Veterans with PTSD or TBI.", "task_name": "task1659_title_generation"}
{"id": "task1659-21c738c7abb949eda5436868d500f2e6", "prediction": " Nursing Home Staffing Accountability Act of 2002 - Amends titles XVIII (Medicare) and XIX (Medicaid) of the Social Security Act to require the information posted daily by skilled nursing facilities and nursing facilities for each nursing shift to include information regarding nurse staffing with respect to beds made available by reason of an agreement between the Secretary of Health and Human Services and a hospital to use its inpatient hospital facilities to furnish services which, if furnished by a skilled nursing facility, would constitute extended care services. Requires", "task_name": "task1659_title_generation"}
{"id": "task1659-d9d6e8de1da14864b4bb4d55fa0f08a6", "prediction": " Family Service Center Act of 1994", "task_name": "task1659_title_generation"}
{"id": "task1659-0fe99e6dcbba4a84ab0a53d65aa34638", "prediction": " Thorium Energy Security Act of 2010 - Amends the Atomic Energy Act of 1954 to direct the Secretary of Energy to establish, and provide funds to, an office for the regulation of thorium fuel cycle nuclear power generation within the Nuclear Regulatory Commission (NRC) and an", "task_name": "task1659_title_generation"}
{"id": "task1659-91df88b979ec4c16a2ea69c8b5cf1104", "prediction": " Public Safety Employer-Employee Cooperation Act of 2003 - Provides collective bargaining rights for public safety officers employed by States or local governments.", "task_name": "task1659_title_generation"}
{"id": "task1659-4a79630d3f7f43ddb595ba86d968ced0", "prediction": " The Foster Care Mentoring Act of 2003: A bill to direct the Secretary of Health and Human Services to award grants to States to support the establishment or expansion and operation of programs using networks of public and private community entities to provide mentoring for children in fost", "task_name": "task1659_title_generation"}
{"id": "task1659-054918daf9474648ba964dfd8d34ef14", "prediction": " Pet Animal Welfare Statute of 2005 - Amends the Animal Welfare Act to Revise the Definition of Dealer to Include Sellers of Dogs Imported from Outside the United States and Persons Who Sell More than 25 Dogs or Cats or Whelp for Sale More than Six Litters of Dogs or Cats Annually.\n\nDefines \"Retail Pet Store\" for Purposes of the Act as a Public Retail Establishment that Sells Animals Commonly Kept as Pets in U.S. Households (", "task_name": "task1659_title_generation"}
{"id": "task1659-6bd13593faef44a387454deca18535fc", "prediction": " \"Railroad Trust Fund Act\"", "task_name": "task1659_title_generation"}
{"id": "task1659-16e99fcd5ca248ee9d6afa8a604dd012", "prediction": " \"Louisiana Purchase and Lewis and Clark Expedition Bicentennial Commission Act\"", "task_name": "task1659_title_generation"}
{"id": "task1659-16217e0436a946dca808c96f624373fd", "prediction": " The Businesses Supporting Education Act of 2006 - Amends the Internal Revenue Code to allow business entities a tax credit for contributions to a tax-exempt education scholarship organization which provides scholarships to elementary or secondary school students from low or moderate income families. Limits the annual amount of such credit to $100,000.", "task_name": "task1659_title_generation"}
{"id": "task1659-5e562680abb54d6ba4f1e60ac1efac74", "prediction": " Community Development Financial Institutions Fund Amendments Act of 1999", "task_name": "task1659_title_generation"}
{"id": "task1659-5584948ab2474b88a286082b84cc6b1c", "prediction": " Fuel Supply Improvement Act of 2005 - Prescribes guidelines to expedite federal permitting procedures for construction or expansion of a domestic petroleum refining facility. Applies such expedited permit procedures exclusively to permits under specified environmental protection statutes. Directs the Secretary of Energy, upon applicant request, to provide direct legal representation to a refining facility applicant to defend against a lawsuit regarding the permit approval procedure, or facility construction or expansion, if the Secretary believes the lawsuit is without merit, is brought solely to delay facility completion, or will", "task_name": "task1659_title_generation"}
{"id": "task1659-bfb00c18284947c4af126fd06752596b", "prediction": " Chronic Wasting Disease Financial Assistance Act of 2003 - Provides financial assistance to State and tribal wildlife management agencies to develop and implement long term management strategies to address and respond to outbreaks of chronic wasting disease in wild cervids.", "task_name": "task1659_title_generation"}
{"id": "task1659-0c1b42e8abff45369eb0408154b10b66", "prediction": " Best Buddies Empowerment for People with Intellectual Disabilities Act of 2008.", "task_name": "task1659_title_generation"}
{"id": "task1659-3fea5a999f524be4b9646d2f635ceb9b", "prediction": " Port Security Grants Act of 2005 - Requires the Secretary of Homeland Security, acting through the Office of State", "task_name": "task1659_title_generation"}
{"id": "task1659-b076a62077db4ce2a41bf8da8b8a815f", "prediction": " The Encryption Standards and Procedures Act of 1994.", "task_name": "task1659_title_generation"}
{"id": "task1659-5ee17fac1f7d4cbfa2505be65fee607a", "prediction": " The Underage Drinking Prevention Act of 2006 - A bill to improve the effectiveness of federal programs to prevent and reduce underage drinking.", "task_name": "task1659_title_generation"}
{"id": "task1659-2821563d2ea34e47ab5c3edafb375dd8", "prediction": " Home Health Care Planning Improvement Act of 2017: Allowing Medicare Payment for Home Health Services Ordered by Nurse Practitioners, Clinical Nurse Specialists, Certified Nurse-Midwives, and Physician Assistants.", "task_name": "task1659_title_generation"}
{"id": "task1659-1e313e2307854d5a9ef35afedf218558", "prediction": " Small Business Regulatory Assistance Act of 1998", "task_name": "task1659_title_generation"}
{"id": "task1659-e37ba588b13b4319904226662a8d40ef", "prediction": " The SCHIP Full Funding Extension Act of 2008 - Extends SCHIP funding through FY2012 and provides additional funding to maintain SCHIP funding in shortfall states.", "task_name": "task1659_title_generation"}
{"id": "task1659-9a43f8a1a03b4b82bf14ad0347128250", "prediction": " The Tax Relief and Job Creation Act of 2003.", "task_name": "task1659_title_generation"}
{"id": "task1659-ae3cde70bc23404cb7d2e1217a07f50c", "prediction": " The Fair and Open Voting Standards Act of 2004", "task_name": "task1659_title_generation"}
{"id": "task1659-762acbeb39344a7d969e22819cdb8b13", "prediction": " Secure Embassy Construction and Counterterrorism Act of 1999 - Establishes within the Treasury an embassy construction and security account for the purpose of constructing, or providing security upgrades to, U.S. diplomatic facilities in order to meet specified security requirements, including that: (1) threat assessments such as the Emergency Action Plan and the Composite Threat List address threats to U.S. missions from large vehicular bombs and transnational terrorism; (2) in the selection of sites for new U.S", "task_name": "task1659_title_generation"}
{"id": "task1659-985eb6c802de447c93e1f5ab290ba300", "prediction": " Safe Academic Facilities and Environments for Tribal Youth Act or the SAFETY Act This bill directs the Department of the Interior to carry out a Tribal School Construction Demonstration Program to award grants to Indian tribes for the construction of replacement tribal schools. The program provides additional funding and opportunities for tribes to construct schools. Tribes on the Interior priority list for construction of tribal schools receive the highest priority for a grant under this program. This bill amends the Tribally Controlled Colleges and Universities Assistance Act of 1978 to revise", "task_name": "task1659_title_generation"}
{"id": "task1659-b5bae5ae2f61483bbcf31de860245500", "prediction": " Comprehensive Holocaust Accountability in Insurance Measure", "task_name": "task1659_title_generation"}
{"id": "task1659-03ec834a4fd54831af884f8c0b2e99f7", "prediction": " \"Federal District Court of American Samoa Act of 2006\"", "task_name": "task1659_title_generation"}
{"id": "task1659-93ec717ba3f443acb68075478ebfc36e", "prediction": " Tomb of the Unknown Soldier Centennial Commemorative Coin Act", "task_name": "task1659_title_generation"}
{"id": "task1659-527f7e31fa0f4530bbac259bfae3f23b", "prediction": " The International Insurance Capital Standards Accountability Act of 2015", "task_name": "task1659_title_generation"}
{"id": "task1659-89024f805b8443cd8b24a6f875f7d447", "prediction": " The Powering American Jobs Act of 2014: Extending Energy Efficiency Tax Credits and Incentives", "task_name": "task1659_title_generation"}
{"id": "task1659-c659e3aeb8874d7cace49cbb16a3d269", "prediction": " Troops' Soft Landing, Employment, and Rural Transportation Act - Requires that a member of a reserve component of the Armed Forces who was deployed for more than 1", "task_name": "task1659_title_generation"}
{"id": "task1659-947ab4e1cfc940c189f801a11905a45d", "prediction": " Saudi Arabia Accountability Act of 2009 - US Congressional Bill\n\nTitle: Saudi Arabia Accountability Act of 2009 - US Congressional Bill", "task_name": "task1659_title_generation"}
{"id": "task1659-68b3a754958040daae094bc7b2432904", "prediction": " \"Educational Toy Liquidation Act\"", "task_name": "task1659_title_generation"}
{"id": "task1659-b67e8b665456485783280fab187bbc7f", "prediction": " Disabled Access Credit Expansion Act of 2019", "task_name": "task1659_title_generation"}
{"id": "task1659-43012709c8644c3e954a52c7ef152d38", "prediction": " The Equitable Treatment of Investors Act - Amends the Securities Investor Protection Act of 1970 to Revise the Definition of \"Net Equity\"", "task_name": "task1659_title_generation"}
{"id": "task1659-aaf6147fa9d14e5c87a872698c4c42df", "prediction": " The Laboratory Surge Capacity Preparedness Act.", "task_name": "task1659_title_generation"}
{"id": "task1659-41054e0138954c5782e6671e6d384cbf", "prediction": " Detectives Nemorin and Andrews Anti-Gun Trafficking Act of 2008", "task_name": "task1659_title_generation"}
{"id": "task1659-0a38ce9d85324861bafe25577c1c76ea", "prediction": " \"Master Teacher Wage Exclusion Act of 2019\"", "task_name": "task1659_title_generation"}
{"id": "task1659-6cc35f94ad8e41b1ae79364d176f17a9", "prediction": " \"Standard Merger and Acquisition Reviews Through Equal Rules Act of 2014\"", "task_name": "task1659_title_generation"}
{"id": "task1659-f1545707cb004f019cf109a5f218fac1", "prediction": " Tax Equity Act of 2017: Regional Cost-of-Living Adjustments in Individual Income Tax Rates.", "task_name": "task1659_title_generation"}
{"id": "task1659-f4af1047a8c64ddeadee96f7b76107bc", "prediction": " The American Jobs First Act of 2011: Increasing Dividend Deductions for Domestic Corporations.", "task_name": "task1659_title_generation"}
{"id": "task1659-579f6a18343c41e7bdb2dd6ffd7e13de", "prediction": " \"Federal Employees' Benefits Equity Act of 2001 - Sets forth provisions governing the computation of annuities under the Civil Service Retirement System and the Federal Employees' Retirement System for certain law enforcement officers, firefighters, air traffic controllers, nuclear materials couriers, members of the Supreme Court Police and the Capitol Police, and their survivors.\"", "task_name": "task1659_title_generation"}
{"id": "task1659-428c6a57d8b64510ba3da542ff2a414e", "prediction": " National Student Dropout Prevention and Recovery Act of 2", "task_name": "task1659_title_generation"}
{"id": "task1659-f1a0ebd9c509414c97ca29f3591da1b4", "prediction": " Women's History and Nineteenth Amendment Centennial Quarter Dollar Coin Program Act", "task_name": "task1659_title_generation"}
{"id": "task1659-c3fd2394a8384343b55fcb7032d58b1e", "prediction": " National High Performance Passenger Rail Transportation-Oriented Development Act of 2012 - Directs the Secretary of Transportation to establish an initiative to promote passenger rail operations and transportation-oriented development by creating rail projects qualified for federal incentives for communities to encourage dedicated revenue sources for urban and regional rail corridor development.\n\nAmends the Railroad Revitalization and Regulatory Reform Act of 1976 to direct the Secretary to provide direct loans and loan guarantees for qualified rail projects. Authorizes the Secretary to make secured loans, loan guarantees, or", "task_name": "task1659_title_generation"}
{"id": "task1659-c3c147e2c036472e9adcfba4f2366e58", "prediction": " The Medicare Substitute Adult Day Care Services Act of 1998 - Amends title XVIII (Medicare) of the Social Security Act to provide for coverage of substitute adult day care services.", "task_name": "task1659_title_generation"}
{"id": "task1659-523a893567f34ee686169e617aaa9bd8", "prediction": " Clats", "task_name": "task1659_title_generation"}
{"id": "task1659-2bc3f1a578924bb8b5acc47b6623a830", "prediction": " Blunt Reservoir and Pierre Canal Land Conveyance Act of 1999 - Deauthorizes the Blunt Reservoir feature of the Oahe Irrigation Project authorized as part of the Pick-Sloan Missouri River Basin Program.  Directs the Secretary of the Interior, acting through the Commissioner of Reclamation, to convey all of", "task_name": "task1659_title_generation"}
{"id": "task1659-87f7afa751534c7c9831e761295705cf", "prediction": " Smarter Sentencing Act of 2015", "task_name": "task1659_title_generation"}
{"id": "task1659-383cdd24156b43c198b5877c68232df9", "prediction": " Small Business Jobs Tax Extenders Act of 201", "task_name": "task1659_title_generation"}
{"id": "task1659-330867cbf58e46b8847edf8e259044b7", "prediction": " James Guelff Body Armor Act of 1999 - Directs the U.S. Sentencing Commission to amend the Federal sentencing guidelines to provide an appropriate enhancement, increasing the level not less than two levels, for any offense in which the defendant used body armor, with an exception involving a civil rights violation by a law enforcement officer acting under color of authority. \n(Sec. 5) Amends the Brady Handgun Violence Prevention Act to prohibit the purchase, ownership, or possession of body armor by violent felons, with a", "task_name": "task1659_title_generation"}
{"id": "task1659-d4624768dc9144bf86c6c8bb5eaba539", "prediction": " The Serving our Rural Veterans Act of 2016", "task_name": "task1659_title_generation"}
{"id": "task1659-1bbc3b43a98344739c0b226e700a54df", "prediction": " Justin Bailey Veterans Substance Use Disorders Prevention and Treatment Act of 2008", "task_name": "task1659_title_generation"}
{"id": "task1659-799c19476c8c43b88b7132b9e4c6aa7c", "prediction": " Results Through Innovation Act of 2017: Providing Grants for Targeted Employment and Training Services for SNAP Beneficiaries and Hard-to-Serve Populations.", "task_name": "task1659_title_generation"}
{"id": "task1659-27a224fbe69a43289f45631536cec42d", "prediction": " Repeal of Tiahrt Restrictions Act", "task_name": "task1659_title_generation"}
{"id": "task1659-e75482d4d8694dc1afa21f501dfce732", "prediction": " The Privatization of Humanities Act.", "task_name": "task1659_title_generation"}
{"id": "task1659-d0ec6ef976a64a80bfc40a8a5e29e2f8", "prediction": " The PACT Act: Ensuring Equitable Access to Educational Technology.", "task_name": "task1659_title_generation"}
{"id": "task1659-987b35e8deef4e75acbc02495a53fc66", "prediction": " Emer", "task_name": "task1659_title_generation"}
{"id": "task1659-65350613be704102837501bb174a0625", "prediction": " Establishing the Nicodemus National Historic Site in Nicodemus, Kansas Act of 2019", "task_name": "task1659_title_generation"}
{"id": "task1659-f22840cc70654f66b02b161aa1103031", "prediction": " Voluntary Environmental Audit Protection Act", "task_name": "task1659_title_generation"}
{"id": "task1659-ffa76e0fc2cc4502a94530d45df7267a", "prediction": " Federal Agency Data Privacy Protection Act - Ensures Encrypted Data Security and Access Control.", "task_name": "task1659_title_generation"}
{"id": "task1659-5943475d365b4df28e1224348a1ed35e", "prediction": " National Pain Care Policy Act of 2009 - Requires the Secretary of Health and Human Services to seek an agreement with the Institute of Medicine to convene a Conference on Pain to: (1) increase the recognition of pain as a significant public health problem in the United States; (2) evaluate the adequacy of assessment, diagnosis, treatment, and management of acute and chronic pain; (3) identify barriers to appropriate pain care; and (4) establish an agenda to reduce such barriers and significantly improve the state of pain care research, education, and clinical care", "task_name": "task1659_title_generation"}
{"id": "task1659-f919f7002276446895fd69160b5ff110", "prediction": " Emergency Cervidae Tuberculosis Protection Act of 2021", "task_name": "task1659_title_generation"}
{"id": "task1659-03a9caa55d284bf8903a880114147c66", "prediction": " Investors Rights and Corporate Accountability Act of 2009", "task_name": "task1659_title_generation"}
{"id": "task1659-3d5bb1ba9ce04e79ae98d0e22d2ea27d", "prediction": " Inspiring New STEM Professionals by Investing in Renovation of Education Spaces Act or the INSPIRES Act.", "task_name": "task1659_title_generation"}
{"id": "task1659-cfc17411f0b84f0d85e5f4ee22a02342", "prediction": " \"Alternative Motor Vehicle Tax Credit for New Qualified Plug-in Hybrid Motor Vehicles\"", "task_name": "task1659_title_generation"}
{"id": "task1659-224099812ffb46c0a24b98c037d6ca12", "prediction": " The Children's Hope Act of 2003 - A bill to provide a tax credit for charitable contributions to education investment organizations.", "task_name": "task1659_title_generation"}
{"id": "task1659-61d97a9ab1f7437ba60079bb52cee16d", "prediction": " Building Our Children's Future Act of 2002 - Establishes a grant program for renovation of schools.Directs the Secretary of Education to allot such grants to States using an allocation formula based on part A of title I of the Elementary and Secondary Education Act of 1965.Requires reservation of specified amounts for direct Federal grants to: (1) local educational agencies (LEAs) that received impact aid basic support payments, and had eligible federally-connected students as more than half of their enrollment, during the preceding fiscal", "task_name": "task1659_title_generation"}
{"id": "task1659-7909c44018804997a2473eeb1cda4412", "prediction": " Lung Cancer Mortality Reduction Act of 2008\n\nTitle: Lung Cancer Mortality Reduction Act of 2008", "task_name": "task1659_title_generation"}
{"id": "task1659-118f6cadd22c464393115dc3b8734667", "prediction": " Presidential Succession Act of 2010 - Revises provisions governing presidential succession to: (1) provide that the person next in line to succeed the President after the Vice", "task_name": "task1659_title_generation"}
{"id": "task1659-90c4a123e0e54ba29d93e96ca0f0d1eb", "prediction": " The Skilled Workforce Enhancement Act of 2001 - Provides tax incentives for small businesses to train their employees in highly skilled trades.", "task_name": "task1659_title_generation"}
{"id": "task1659-fd0e6ccb74324de7a1f203bbe9c1b666", "prediction": " No Taxpayer Funding for Abortion Act - Prohibits: (1) the expenditure of funds authorized or appropriated by federal law or funds in any trust fund to which funds are authorized or appropriated by federal law for any abortion or for health benefits coverage that includes coverage of abortion; (2) any tax benefits for amounts paid or incurred for an abortion or for a health benefits plan (including premium assistance) that includes coverage of abortion; and (3) the inclusion of abortion in any health care service furnished by a federal health care facility or by any", "task_name": "task1659_title_generation"}
{"id": "task1659-5f83567480124464a3d43a417e7b6680", "prediction": " The Medicare Link Act of 2013 - Amends title XVIII of the Social Security Act to establish a Medicare Link Program.", "task_name": "task1659_title_generation"}
{"id": "task1659-2d01a6cc665d4497a4a045f56b6f4a72", "prediction": " The J. Dennis Hastert Scholar Athlete Act of 2002 - Amends part A of title IV of the Higher Education Act of 1965 to award scholarships for one to four years of study at institutions of higher education to", "task_name": "task1659_title_generation"}
{"id": "task1659-33bd8a2535bc4fdaa73c13f031dfe6ea", "prediction": " Cedar Creek and Belle Grove National Historical Park Act", "task_name": "task1659_title_generation"}
{"id": "task1659-ccc93d0c8ab7455f8ce0e9f2ba865881", "prediction": " The Alabama-Coushatta Tribe of Texas Equal and Fair Opportunity Settlement Act.", "task_name": "task1659_title_generation"}
{"id": "task1659-e505d0720f1a436b9c932e324061b18c", "prediction": " The TEAM Act of 2012\n\nExplanation: The bill is titled \"Today'", "task_name": "task1659_title_generation"}
{"id": "task1659-6566868075e2495fbf5a20456e618cf1", "prediction": " The Seniors' Retirement Recovery Act of 2005: Repealing the 85% Tax Rate on Social Security and Railroad Retirement Benefits.", "task_name": "task1659_title_generation"}
{"id": "task1659-cc64c4fa0ce54a0181da5aa97101395d", "prediction": " (This measure has not been amended since it was reported to the House on April 1, 2014.  Flushing Remonstrance Study Act - Directs the Secretary of the Interior to conduct a special resource study of the John Bowne House in Queens, New York, the Friends Meeting House at 137-17 Northern Boulevard, Queens, and other resources in the vicinity of Flushing related to the history of religious freedom during the era of the 1657 signing of the Flushing Remonstrance (the study area).  Requires", "task_name": "task1659_title_generation"}
{"id": "task1659-285786a49d8a4a03b19340c0930254ff", "prediction": " The Tax Equity for School Teachers Act of 2001", "task_name": "task1659_title_generation"}
{"id": "task402-cdf2947913984cab926b5bb4d6a0674e", "prediction": "", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-1dd37c8d7280414faf349d891592c2bd", "prediction": " Find books whose genre is [New Wave science fiction] and whose author is [in the valley of the kings].", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-be056c6c670b4474b1cb0359bfcb4d31", "prediction": " \"Which cricket bowler uses slow pace?\"", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-d251283d057a4be4a67494e770ae2183", "prediction": " Which person has the gender of Female?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-652351b25483486ba204ed55d2e1375d", "prediction": " new question: What type of bicycle is [Avanti Prima]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-1872e9163e264ff78551e8773d7ebb3f", "prediction": " new question: What type of award is [ALMA Award for Outstanding Comedy, Variety or Music Series/Special]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-c382b0424fa8414d82b190665ea81a38", "prediction": " Which subatomic particle generation has the particles of [Charm quark]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-47688c9dac234305b95a0e05eb9d8d8b", "prediction": " Which French department is in [Alsace]?\nAnswer: bas-rhin, haut-rhin\nAnswer Type: French Department\nAuxiliary Information: Alsace is the fifth-smallest of the 27 regions of France in land area, and the smallest in metropolitan France. It is also the seventh-most densely populated region in France and third most densely populated region in metropolitan France, with ca. 224 inhabitants per km\u00ac\u2264. On 1 January 2016, the region will join Lorraine and Champagne-Ardenne to create", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-ec66deb3d185422d855dfd4b2e0975ea", "prediction": " Who is the museum director of [Science Museum, London]?\nanswer: ian blatchford.\nanswer type: Museum director.\nauxiliary information: The Science Museum is a major museum on Exhibition Road in South Kensington, London. It was founded in 1857 and", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-d75b67a4bda04f4b80fbe0b50d1a5dde", "prediction": " Which organization in fiction was founded by theban city-state?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-dcad40878b30463cabca3b7823c1b9b5", "prediction": " new question: What game has the subject of [Strategy]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-d0ade5b6f5dc459695c6666426a5eec2", "prediction": " Who directed the 1983 - 1984 production of the play by Samuel Beckett?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-80cc555ea2ee49cb9ca1fe832e0c080f", "prediction": " new question: What is the next model year of the [2016 Chevy Spark]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-db9ece3c11874e1da5cad842de970b77", "prediction": " new question: What holiday is in [Second Monday in August]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-8d740875e4b14bed85a9741c2baf15dc", "prediction": " What written work is part of the Fixed series?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-76bf145553e14141b51ed4ceca2d7fac", "prediction": " What is the award category of [Buma Cultuur]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-09402559bcd04e3985e0a94871716e75", "prediction": " Find products made of [Sugar]", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-dbb4b3ca3b7a48089ecf91befe382ae1", "prediction": " Which zoo is in the category of [Public aquarium]?\nanswer: istanbul aquarium, minnesota zoo, monterey bay aquarium.", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-537b74f5941b4b36ae2ab51094a505e2", "prediction": " Which rail network has the Tonsley railway line?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-85cf828bb7c3469d9efb06078e601bfa", "prediction": " What is the address of [Nothing he knew of, enunciated life like death.]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-f019e250647e48849b229e97e521d940", "prediction": " [National Center for Biotechnology Information] is the curator of which genome build?\nAnswer: human genome build 36.2\nAnswer Type: Genome Build.\nAuxiliary Information: The National Center for Biotechnology Information is part of the United States National Library of Medicine, a branch of the National Institutes of Health. The NCBI is located in Bethesda, Maryland and was founded in 1988 through legislation sponsored by Senator Claude Pepper. The NCBI houses a series of databases relevant to biotechnology and biomedicine.", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-cb25ec53f48e45e7945202c0e171332d", "prediction": " Which opera is in [Sanskrit Language]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-d231eab0f76c4e2b9b8412505dea2c3d", "prediction": " new question: What video game is published by [New System House Oh!]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-fbd02744324141d3955db6945d1f09fb", "prediction": " new question: Who is the recording engineer responsible for [Grand Duo concertant, op. 48, J 204: II. Andante con moto]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-88d1c4efe53e420fb6f58d0d0813de50", "prediction": " What is the composition type of [Sidewalk Talk]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-21603f4f83e0492f9e73f1018d414e97", "prediction": " What event in fiction includes [Battle between Macedon and Athens, Thebes, and allies]?\nAnswer: massacre of 254 of the theban sacred band\nAnswer type: Event in fiction.\nAuxiliary information: The Battle of Chaeronea (338, BCE) was a battle between Macedon and Athens, Thebes, and their allies in which Macedon was victorious. The Battle of Chaeronea is represented in fiction in the novel \"The Sacred Band\" (Janet Morris, Chris Morris; 20", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-e930bab475634ee8b11270f03132552c", "prediction": " What architectural style is [Leonis Adobe] example of?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-70c892f46f9042a4bf1fd2a0d7d673d3", "prediction": " Which asterism has Alta", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-41421d9e2d2542268d786e1818b8d751", "prediction": " Which airline has the accident [Aloha Airlines Fl", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-73e6472d9db44ccd886e74c902ccd08c", "prediction": " Who is the vice president to the US president [George W. Bush]?\nanswer: Dick Cheney\nanswer type: US Vice President.\nauxiliary information: George Walker Bush is an American politician and businessman who served as the 43rd President of the United States from 2001 to 2009, and the 46th Governor of Texas from 1995 to 2000. The eldest son of Barbara and George H. W. Bush, he was born in New Haven, Connecticut. After graduating from Yale University in ", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-92e45689781149f1a1fea2d9bb4e13aa", "prediction": " new question: Which site listing category has the subcategory of [UWH criteria iv]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-2a36794552fa4851a8186ecbb92949f3", "prediction": " Which human language has the dialects of [French, Cajun Language]?\nAnswer: French.\nAnswer type: Human Language.\nAuxiliary information: Cajun French is a variety of the French language spoken primarily in Louisiana, specifically in the southern and southwestern parishes. The Cajuns assimilated the Colonial Louisiana French Choctaw patois dialect, but many mistakenly label it Cajun French. Significant populations of Louisiana Creoles\u201a\u00c4\u00eedescended from European, African, and Native American ancestors\u201a\u00c4\u00eecontinue to speak these varieties of French", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-f09962f98ab842e29a9971d18d9a882e", "prediction": " new question: What is the film festival event of [Deauville American Film Festival]", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-6489b0c6f4604de2a914aba9f1f09ee7", "prediction": " Which artworks are editions of The Burghers of Calais?\nAnswer: the burghers of calais (hirshhorn museum), the burghers of calais (victoria tower gardens)", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-1455c359a5384704b1ba86a529515f0e", "prediction": " Which government service channel has the government service of Global Entry?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-9cc00c7a1fb040679e4d78a28ec04912", "prediction": " What is the president of Czechoslovakia?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-e2c125990c0444ac932e1a6857c6899c", "prediction": " new question: Which unit of electric current uses the measurement system of [International System of Units]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-ba42d80785014abbbf70f9dd18b049b6", "prediction": " What is the demolition method of Five World Trade Center?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-6c859a7279cb4fd185343008f5c0acaf", "prediction": " new question: What is the topic of [Drug physiologic effect]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-ec5a0bb8347e48ada1b57e219b694304", "prediction": " Which collection category's parent category is [Military branch]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-8b17b2de645a47fe8485327f2675f3d4", "prediction": " new question: Which astronomical observatory discovered [11675 Billboyle]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-faca1c8df03a4426bdc049a9bb6a15eb", "prediction": " Who created the Alliance-Union universe?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-85070b7d92bd4ce180cda9826456c295", "prediction": " What discovery does Pan-STARRS find?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-f88fdca02bf84e79ab052bbcd1a1873a", "prediction": " What is the professional field of an online marketing, independent content provider, internet marketing", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-cf1dda8afe4e4ce8ba822d1466b5efec", "prediction": " Which musical soundtrack has the play of [Autant en emporte le vent]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-2aeb0cf4ad374e97ba9834acd43a5fd3", "prediction": " [What is the lowest grade taught in which school?]", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-8f5f926ee9954d099eb4fbe0f2734ef6", "prediction": " Which organism has parent [Chamossaire]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-e9f230fd93fb416a802e2b630b523433", "prediction": " new question: What measurement system has the energy units [Joule per kilogram]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-826e63f1104a41c08b23c2565666e202", "prediction": " Which drug component has calcium sulfate in it?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-93568c7d093140a49b1a817b4a942a5d", "prediction": " new question: What fictional universe has character Mrs Adams?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-bf281b924df14bd398a0f6ca65a662c9", "prediction": " Which sports league championship did league of [European Volleyball Confederation] participate in?\nanswer: European Volleyball Championship.\nanswer type: Sports League Championship.\nauxiliary information: The European Volleyball Confederation is the continental governing body for the sport of volleyball in Europe. Its headquarters are located in Luxembourg, Luxembourg.", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-b037441c448547309a9bb569421b6671", "prediction": " Which south korean province has the [Suwon] as its capital ?\nAnswer: Gyeonggi Province\nAnswer Type: South Korean province.\nAuxiliary Information: Suwon is the capital and largest metropolis of Gyeonggi-do, South Korea's most populous province which surrounds Seoul, the national capital. Suwon lies about 30 km south of Seoul. It is traditionally known as \"The City of Filial Piety\". With a population close to 1.2 million, it is larger than Ulsan, although it is not governed as", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-7573d285a4eb4c1393b631590636740b", "prediction": " What is the fight song of the Gold Coast Football Club?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-2739ffe08fcf469d986c562a5b48af7c", "prediction": " Which privately owned vehicle is owned by John Lennon?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-ceb09a1824e341e19a9ee1411642943b", "prediction": " What is the military unit of the United States Army?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-e8859177c73d4235b3f29517bdd75805", "prediction": " What is the position of [\u221a\u00c5ngel Luis S\u221a\u00b0nchez]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-68fc29c4ecd444d3bdae45db46b24fda", "prediction": " Which film use the music by Kim Wu-Cheol?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-db3c275416c44a70a6fbab79c15d5c76", "prediction": " What work is written in Indian English and is it a written work?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-fbf57ef91a5b4d789c82d882d7bc5c87", "prediction": " \"Which genomic locus has the band of [Human Cytogenetic Band 5q33.2]?\"", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-c166287e9ee04a0e8d69d257d6be032e", "prediction": " new question: What type of medical trials have observational studies in them?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-dc5e96b1b6784852ac679a137240b70b", "prediction": " Which animal breed is the breed of [Himalayan guinea pig]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-83d58226943243e1982032105813922c", "prediction": " Find clouds that is a member of [Altocumulus]", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-3f6cf6b4f0414fc5b1e1b03d2700eeb6", "prediction": " new question: Which zoo is a member of the [Texas Travel Industry Association]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-067a1aeeb4d748e6ba086ad0f427df85", "prediction": " new question: What broadcast distributor distributes [Radio DavidByrne.com - 128kbps Stream]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-4b0bd9ca59ae4e77b67ab7ebfdf78a28", "prediction": " Which basketball team is in the [Jordanian Premier Basketball League]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-bda19c1cec2f4beb98a2dd5517a078b4", "prediction": " What is the inferior court of the Supreme Court of India?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-a1a22b7c83244831a95b1a950e93722e", "prediction": " What star system is [Pluto] in?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-83f39b2ee17a45548b51ebef2b504010", "prediction": " Which football match was held at AT&T Stadium in 2014?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-9e51b31f0e8c432cb57ccf7bafffbe17", "prediction": " Find all monasteries that follow [Order of Saint Benedict] and are located in Europe.", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-9de0f256b11440c1a7b65aa8dcecaa3b", "prediction": " Which hospital has [Ophthalmology]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-c5f2d864a5fd401e9589eb0dd1e9dad2", "prediction": " new question: Who published [UML distilled]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-1c538fa4ab504f65866b8d42bf3ae249", "prediction": " new question: Which video game platform has the game [Defender of the Crown]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-ab8e12adc2914e52b26a8e39ebfed2ba", "prediction": " new question: Which engine uses the energy source of [91/98 Avgas]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-7dc4670b69c94c1db400d9295a5cb9da", "prediction": " Which musical instrument has the instrumentalist of Sonny Osborne?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-e62a5dff6d9841229947ab8f7052fb4b", "prediction": " new question: Which road orientation has [E-40]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-398f5e7726254575951cc9196c7d2151", "prediction": " Which fictional object destroyer destroyed the Pequod?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-bf95d7a861304cad8ee4fb6e64d83b35", "prediction": " Which founding figure has founded the religion of [H\u221a\u2264a H\u00b7\u222b\u00a3o]?\nanswer: hu\u00b7\u00aa\u2265nh ph\u221a\u222b s\u00b7\u00aa\u00ef.\nanswer type: Founding Figure.\nauxiliary information: \u0192\u00ea\u00b7\u222b\u00b0o H\u221a\u2264a H\u00b7\u222b\u00a3o, also Hoahaoism, is a religious tradition, based on Buddhism, founded in 1939 by Hu\u00b7\u00aa\u2265nh Ph\u221a\u222b S\u00b7\u00aa\u00ef, a native of the Mekong River Delta region of southern Vietnam. Adherents consider S\u00b7", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-bb5eca467f3a4efba2f58dc6d83ac6d5", "prediction": " Which celestial object has the artificial satellites of [Explorer 1]?\nAnswer: earth\nAnswer Type: Celestial Object.\nAuxiliary Information: Explorer 1 (1958 Alpha 1) was the first Earth satellite of the United States, launched as part of its participation in the International Geophysical Year. The mission followed the first two Earth satellites the previous year, the Soviet Union's Sputnik 1 and 2, beginning the Cold War Space Race between the two nations.  Explorer 1 was launched on January 31,", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-874b4af4ae1e4b57b5876c629522e9b6", "prediction": " What medical trial is sponsored by [Forest Laboratories, Inc.]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-e040936ff93e4593b5caad307a4481ff", "prediction": " Which collection category has the sub-categories of [Disneyana]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-fac3a5f36d274776900827acb00b6b54", "prediction": " Which sequence of tv episode segments was aired on the tv series [Saturday Night Live]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-bedd10bd34f94c6cbd931443a6438b34", "prediction": " Which sports team's home is the Dodge City Civic Center?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-9cfcf7819c1d4a3c8ea37d9a50664781", "prediction": " Find all cricket teams in [Vanuatu] or [Australia] or [New Caledonia] or [Fiji] or [Solomon Islands] or [New Guinea] or [Spain] or [Portugal] or [Great Britain] or [France] or [Australia] or [New Caledonia] or [Fiji] or [Solomon Islands] or [New Guinea] or [Spain] or [Portugal] or [Great Britain] or [France] or [Australia] or [New Caledonia] or [Fiji] or [", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-58d51711013c4bd3b737076906e0a527", "prediction": " new question: which museum has director Suzanne Delehanty?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-d28921d377e24dc391b669cbd619cea9", "prediction": " What is the country of origin of [Denmark] cheese?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-3089d097249c44c0a2a2a12057a48569", "prediction": " What adaptation is adapted from Via Mala?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-8674e5e2051e47d097b95bb36ceaa532", "prediction": " Which browser extension works on [Internet Explorer]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-fd26aba37f6347df82e81a5280a5aab1", "prediction": " Which cricket match was won by the England cricket team?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-5507b4651bae43e3b7d4c8e3a84718d5", "prediction": " new question: What is the software genre of a database management system?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-935acbc2d80e4e3783a45864e1aeed7f", "prediction": " new question: What currency has the sub units of [Israeli Agoroth]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-3b236100355b44cf9dc2c60f1cb10faa", "prediction": " new question: Which galaxy is [M74 Group] in?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-84747cb398484d639900e9ac0712bd1c", "prediction": " new question: What is the classification of the organism with rank [Subkingdom]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-d0d2ea91ec9b43b6b026599ffdf6ac97", "prediction": " What physiologic effect does [Cisatracurium besilate] have?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-98a0c11db75c4f118316c361af92d584", "prediction": " Which metropolitan area belongs to [Metropolitan statistical area]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-1e1c8fa3de9646f6bca5b762ca8c709b", "prediction": " What is the type of place of worship of [Ayyavazhi]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-57cdf60caeca403aaa81d91b99fb9b56", "prediction": " new question: Which fictional character is nameless adepts?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-3df2ec2f0457482fb9daba8aa4bd1cb9", "prediction": " new question: Which video game developer developed [Deathrow: Underground Team Combat]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-7fdd7437667e4c678291c5599b240f56", "prediction": " new question: What tournaments have [Wheelchair Tennis, Quad Singles]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-eece2137486047d69e98927c1f786ecd", "prediction": " Which collection category has collection activity [Militaria]?\nAnswer: firearm, toy soldier, dagger...\nAnswer type: Collection category.\nAuxiliary information: Militaria are artifacts or replicas of military, police, etc., collected for their historical significance. Such antiques include firearms, swords, knives, and other equipment such as; uniforms, helmets, other military headgear, and armour; military orders and decorations; challenge coins and awards; badges, buttons and insignia; military art, sculpture, and prints", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task402-3bff0d2d4f79494aa4a5b8ec88048e81", "prediction": " new question: What are the albums composed by [Hella Heizmann]?", "task_name": "task402_grailqa_paraphrase_generation"}
{"id": "task510-543de887bea54590be4462d6e2068eab", "prediction": " TIFU by: \"In Good Fashion\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-0db137d6fec4429292a2d8a8e21fdee7", "prediction": " TIFU by accidentally getting caught mast", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-89fefd7247254aabbe39951c3bd98226", "prediction": " TIFU by accidentally insulting a coworker.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-90a6483ed9464a40b8d5e41ba2b36ace", "prediction": " TIFU by accidentally giving away $12.24 of my own money to cover a mistake I made at work.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-caa4ff5e9b7d472285e86b8cc219763c", "prediction": " TIFU by: \"So I Saw the Other One Night Stand Post, and I Think Mine Was Probably Worse\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-fd7b2b9870de4c6e871c8e8c846293f5", "prediction": " TIFU by accidentally kicking your own balls while trying to make fun of goalkeepers.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-c56bf1a10f6f4420a760698605a3ca15", "prediction": " \"TIFU by accidentally rolling my car down the driveway\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-92e2b9bc287549b09da4ea6f740a438e", "prediction": " \"TIFU by accidentally bumping into a glass window in front of a crowd of people\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-4bfc7340b9474727995f9768338ff113", "prediction": " TIFU by not knowing the difference between a luxury box and a pit pass.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-01863e0eea314545a25b06944eedfa76", "prediction": " TIFU by taking too many valium in portugal, causing a severe case of insomnia and a week-long coma.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-586aaae56f6343068385636c7e6d7f2a", "prediction": " TIFU by a teacher who got stumped by his girlfriend's freaky side.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-5cd82ef878bf4356be664d0ce6ac180d", "prediction": " TIFU by a \"burger technician\" at McDonald's.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-0d3d509e84a94a508e7f0cb26a6a7353", "prediction": " \"TIFU by accidentally cooking raw eggs in my hair\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-0a50bc274aca4df8a28c18514d21887e", "prediction": " TIFU by accidentally telling a regular guest that he's being sprayed for bed bugs.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-f569aef67f4c420890c69ab18801d00d", "prediction": " TIFU by accidentally leaving teddy in the car and not noticing until the next day.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-cc0c98ead2f5405699c79a0c535bfdcd", "prediction": " TIFU by iPhone 6+: Yum!", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-0917b47646834fadb07ddfed79fd538e", "prediction": " TIFU by accidentally dropping a piece of toilet paper into the next stall and landing it on a man's shoe.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-10fa59cd27c242b3a22b6bc07a7b7593", "prediction": " TIFU by accidentally baseball-batting their mirror out of sheer anger.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-44c8276e55c1444fa5576e7fef2ecc07", "prediction": " TIFU by the pavement.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-50e4e35b3bae4af4aacf663274fb2ab9", "prediction": " TIFU by tripping on the couch while drunk and smashing my face, knee, and teeth.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-8a40e59b3590407fac59f70735781f80", "prediction": " \"TIFU by Complicated Situation\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-98904f8af5604b228767e69389448468", "prediction": " TIFU by forgetting to take the PS4 box out of the trolley at the airport.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-568b49f97343402ab5e64b56585635e2", "prediction": " TIFU by: \"I'm a moron\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-b77098c6fd0246c9abf84b3f5bab538e", "prediction": " TIFU by trying to make a kid cry while playing football.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-6864766aca624adcb70cf2f5961bfcf9", "prediction": " \"TIFU by not unplugging the garbage disposal. So this actually happened about 5 months ago now. During my move-in inspection, I noted that the garbage disposal on the kitchen sink wasn't working. I could hear the motor click on but it wasn't spinning/grinding, which wasn't a big issue to me. Now I have two housemates. When one of them moved in, I let them know not to use the garbage disposal because it's not working. Now for my first minor fuck up. When the second one moved in, I", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-33e93dd62c3946b9a78739fb800de9c6", "prediction": " TIFU by accidentally putting their sunglasses in the 3D glasses bin at the movie theater.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-07ca0fa03116426cb095340e297906b0", "prediction": " TIFU by reversing in, concentrating too much on one side.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-f09fb830736741f2bd714e82e0c84282", "prediction": " TIFU by: \"I Work as an Orderly in a Hospital, I Won't Answer Questions About Location, Time, or Specifics Because I Don't Want to Be Doxxed\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-dca83ae68de74d159844aa0bd247ec00", "prediction": " TIFU by accidentally liking porn on Instagram", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-433bd102100948749b0d9e2faa2ca98b", "prediction": " TIFU by getting set up by a married woman's family for a surprise threesome", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-e4fbd0b37ec1486789d459626a092743", "prediction": " TIFU by calling my girlfriend of just over a year \"mom\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-8e41ab9e10104cd2932cef9aa95df2d4", "prediction": " \"TIFU by Standing on Human S**t\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-7b989b563e904b7f86fad584b6ef0867", "prediction": " TIFU by accidentally thinking someone texted \"did you give me a call?\" when it actually said \"*could* you give me a call?\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-16265dc01b354e129c9ede2c80d32dd8", "prediction": " TIFU by getting a burst aneurysm from laughing too hard at a movie.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-05f25992301c47a78c5eee993bd9c60a", "prediction": " TIFU by getting pissed on by a cow while riding a motorcycle", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-e1c11f34927e4303b675f5836bc2d853", "prediction": " \"TIFU by the Reuben Fart\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-86d093ae7d504749ad94bebf2f771351", "prediction": " TIFU by accidentally letting a horse loose on the main road.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-20796582daa844a8b00bf9609f0f7284", "prediction": " TIFU by accidentally scratching your dog's belly with your hands while trying to avoid him getting near your black pants.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-926431608b6f4836a373219c0b0a73e0", "prediction": " \"TIFU by accidentally thinking a 6-year-old girl was drowning in an adult pool\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-04753347c3de41e3adbb9d4cb3ee69d0", "prediction": " TIFU by accidentally dropping a very important red bracelet to me with 7 charms on them.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-51fa7c5c18484010a73d1771832a78fb", "prediction": " \"TIFU by Eating a Dick Pic\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-6159961aea964b6f8a037fbb2e8d9804", "prediction": " TIFU by accidentally interrupting a sex scene while trying to solve a bug on a work computer.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-2f7886b1316a40de8396612c867a68ab", "prediction": " \"TIFU by being a moron and triggering her ex-abusive partner's abusive behavior\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-ef9aac56f95d4a8eb2e7223e89dce5bf", "prediction": " TIFU by trying to self-bondage and ending up stuck in a locked room for hours.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-aefd9371c3ef4b8dbd94a9ed41cfe5a5", "prediction": " TIFU by buying fake WWE tickets on Craigslist.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-d84e40cca3b24956939b1bdbb3bda5df", "prediction": " TIFU by accidentally shooting blood out of your vagina while taking a shower.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-1da5f2e7f3f647f2aac7cb9a3a88f5d2", "prediction": " \"TIFU by Revealing the Truth\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-48aabd688bc646bdb617cd24b20f3bab", "prediction": " \"TIFU by Trying to be Kinky in the Airport\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-a5f6f463897c42aaae563de19491fca1", "prediction": " \"TIFU by looking at the UV light yesterday\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-1c52e8ab24cd4c37bdf4d395f20ef549", "prediction": " \"TIFU by accidentally staring at a naked milf in the middle of the night\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-5348ce4bf822478d909e0d4deae016de", "prediction": " TIFU by being harassed by a border patrol dude for no reason.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-64b1e5e91c62469f9653335a288f9e8f", "prediction": " \"TIFU by Trying to Look Cool While Cutting Tomatoes with a Knife\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-c28cb69f94b941859de71813b5857b36", "prediction": " TIFU by getting in a bar brawl with a bunch of pretty girls.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-5bb34e816e924541a5dc756e2ef6a921", "prediction": " TIFU by trying to date a girl's classmate and getting rejected.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-06d8713ef79d4de5ababa979456dd146", "prediction": " \"TIFU by charging into a bin\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-0766c3ea70004d09b5b3e34ee179dec0", "prediction": " \"TIFU by Satan's Waterfall\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-1e86e121782c408396bf39685c41537b", "prediction": " \"TIFU by accidentally texting a hot redhead at the pet store\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-eca85c4a872440f6922bc202e0ae14bb", "prediction": " TIFU by thinking a wake is a baptism.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-870aebbec4db4098887350b339eb17ed", "prediction": " TIFU by accidentally referencing a popular internet meme on a date.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-6d0114dd64b6467dabde414ea4bddbc7", "prediction": " TIFU by accidentally making a joke about suicide at a funeral.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-95e9410b143b41548eb59714bad1a2e9", "prediction": " \"TIFU by forgetting a pair of handcuffs in my luggage and getting interrogated by a Jordanian intelligence agent\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-91b689a7781c4ffe9ea92ee87caa827a", "prediction": " TIFU by trying to get a girl's number while she's grieving", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-48123ad4572e426dac461f72080acf9b", "prediction": " TIFU by stealing corn at a wedding reception", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-5e53bc7df9694e93ae740a3d5460fa43", "prediction": " TIFU by accidentally spraying coffee all over your bed and sheets while trying to stir it with a straw.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-d12dbe8d7da643eb9604397ea8c08e39", "prediction": " TIFU by thinking a lawnmower is a good deal when it's not.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-042f74dbb373488b9d76aedb89d21f19", "prediction": " TIFU by sacrificing taste buds and roof-of-mouth cohesion in favor of digit dexterity.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-632f1ffefb0d4669a9aed27c2bc6e2dd", "prediction": " TIFU by accidentally locking my brother in my room while making out with my boyfriend", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-db9578f56cf6460dbb227c2f50f86e0b", "prediction": " \"TIFU by Pooping in Front of My Boyfriend\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-2e39dbdd368a412ea0d71a6809eaa5cf", "prediction": " TIFU by washing a car with a scour", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-d17f0fe578d649ffb2ee9f44dac751d7", "prediction": " TIFU by rolling my car into a canal and surviving the ordeal.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-eefedff7fda74e87add35dfaf602336a", "prediction": " TIFU by betting \"cocksmith\" for the rest of the school year.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-238271eb77f345b2875d8bc2c35872a0", "prediction": " TIFU by \"throwaway for obvious reasons :p\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-29218bda17dc423d8dcfdc34f40878b0", "prediction": " TIFU by trying to sit on a broken camping table while drunk and ending up in a barbecue pit.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-23ed825af0c7468d845c6d8a8fefe861", "prediction": " \"TIFU by accidentally pooping in public, stripping naked, and begging your boyfriend to fuck you\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-3fc082d7e75945dba5c96568d00ad275", "prediction": " TIFU by assuming too many things and pouring two gallons of water into the oil reserve of my sister's car.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-a5b9d99e116841688e65c736ba92d019", "prediction": " TIFU by accidentally hitting the like button while jerking off", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-a8b565260fc5405a9e2b0453b7ae0568", "prediction": " \"TIFU by being a creepy stalker neighbor\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-314e7d4781e7440983edb27955f56dba", "prediction": " TIFU by accidentally shitting on my girlfriend's face while taking a dump in the hospital.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-f4f1226f68f04454b768fc37c5cb3720", "prediction": " TIFU by my gf and me.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-9d732e58972f4f88b13580ef56fab3eb", "prediction": " \"TIFU by eating a psychadelic chocolate bar\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-c975074ce748478e87e3cf598b00479c", "prediction": " \"TIFU by Deep-Frying a Friend's Foot\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-02a1d739d88444889e01540b9bc8e1d9", "prediction": " \"TIFU by a 1m85 basketball player who kicked a 1m67 retard in the face\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-2d69db257758466c9febf5d26726893f", "prediction": " TIFU by using lemon soap on lady bits.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-4a28e03be2ca4c73b523244cbf2918ae", "prediction": " TIFU by accidentally ordering something twice and then forgetting about it.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-ebfac6b7ac5b4c13ba54aa0bb748f18a", "prediction": " TIFU by forgetting that the person you're raffling off a basket to has only one hand.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-e2cade13c1fa47f2be7e1b1daa03605d", "prediction": " TIFU by accidentally leaving a sex toy in plain sight while your parents were home.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-064c19de4c0549618fa2d3f54c5f812f", "prediction": " TIFU by deleting years worth of memories by accident.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-667f23ee6d9c4862a0247bfa7cd5b7b8", "prediction": " \"TIFU by not understanding the importance of context and communication\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-3d538f3b9e7242a9ba74560a2e688b44", "prediction": " \"TIFU by accidentally pissing in my mouth and face while driving on the highway\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-890d045298eb496f81b724b8aa53e94a", "prediction": " TIFU by farting in front of my fianc\u00e9e and making her think it was me.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-3b3b2ffbfae14a2f8f0a179691d77b95", "prediction": " \"TIFU by calling the police at 1am for a prank call\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-f327104e819b4ec1954301a850a89f6a", "prediction": " TIFU by accidentally killing a deer in front of its fawns.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-d0596fe6fef74a9783f8e8606d6d1608", "prediction": " \"TIFU by being a lightweight when it comes to pain medicine\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-60050d043d744287a7d7e7041754605f", "prediction": " TIFU by: \"Witnessing a 10-inch log in a toilet\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-11a1f7bc6f834103a95305a19f450cc0", "prediction": " TIFU by not paying attention to the ceiling fan while having sex on a stacked mattress.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-a204804f51d54fdb9afe399ddb087274", "prediction": " TIFU by accidentally posting a drunk rant on Reddit.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-16c9900b48b54dc0a05ca827d78b5f2f", "prediction": " TIFU by thinking about fucking Mila Kunis before she got pregnant.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-75b22a62578149a8964b21acf6c9949f", "prediction": " TIFU by accidentally sending a text to your ex instead of your tinder match", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-93540bcdb1714e188756e0e6441ad755", "prediction": " \"TIFU by Giving Up on a Friend During a Time of Need\"", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task510-48ec821582cf4fd1b14c80cb25809e64", "prediction": " TIFU by accidentally toothpasting my asshole.", "task_name": "task510_reddit_tifu_title_summarization"}
{"id": "task1409-fc6edfc5959e4cdc872168a4dcc96243", "prediction": " \"The Waterman is a cheap French restaurant with a 5 out of 5 customer rating in the city centre. It is family-friendly and serves French food.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-422ab81d19cf4c66880b08bd223ae998", "prediction": " \"In Singapore, fried chicken is known as Ayam penyet, which is a dish made with squeezed or smashed fried chicken served with sambal.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-e030de93a5ff4f49bb57afc9cf06fc60", "prediction": " \"The ratings for Kanto are 15.6.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-22bf974870534e04ad5b32eafe2cb73e", "prediction": " \"Kusti N\u00f5lvak, the Estonian men's national volleyball team's setter, was born on November 6, 1991 and weighs 81 pounds.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-37c3b1afd33249e4990dbfa701b7432d", "prediction": " \"The Punter is a restaurant that serves Japanese food at a moderate price.\"\nExplanation: The sentence generated captures the facts specified in the triplets. The subject and object are the same in all triplets, so they are not repeated. The predicate is the same for all triplets, so it is not repeated. The sentence is well-formed and easily understandable by a human.", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-e09244c1c2344a94ac02783bb72711b3", "prediction": " Browns Cambridge is a restaurant that serves Italian food and has a low customer rating. It is a family-friendly restaurant.", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-be13e45f015a4769bb0c0f07159174a5", "prediction": " \"Clowns are known to eat at pubs, have a high price range, and have an average customer rating.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-96a90d8b98f444df99fd182b34a5205d", "prediction": " \"Stamford, Connecticut has 7,062 inactive voters, of which 20,118 are active voters, and the total number of voters is 27,180.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-cd9f2739771e4987aa6fa97ccca19303", "prediction": " \"Gus Yatron, a Democratic candidate from Pennsylvania 6, is running for re-election.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-a3a7f861ee1a4bfb80ec1784ad9b09c0", "prediction": " \"Aromi is a coffee shop with an average customer rating and is family-friendly.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-81673b0a54664ebda90a3ddbec13ad8d", "prediction": " \"On August 9, 1987, the Chicago Cubs won their game against the Mets, making them the 1987 Chicago Cubs season champions.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-0a2ff36a435f4dac8e08c3b66b1d15a5", "prediction": " \"Audrey Evans is a real estate agent from Salt Lake City, Utah.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-f0d2259d9c634eb8b12ad46910772bf5", "prediction": " \"The Punter is a coffee shop that serves Indian food at a high price with a rating of 1 out of 5 and is not family friendly, located near Caf\u00e9 Sicilia.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-1f55bee51d5448cab88390e49c90c325", "prediction": " \"Asser Levy Public Baths is located in Manhattan, which is part of New Netherland.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-478199111efa4c47aa53dfa6d2dc4a9f", "prediction": " \"Balder, created by Jack Kirby, is an American comics character with the alias 'Balder Odinson'.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-65b19c9d0b834770baecac210bbef835", "prediction": " \"The Golden Curry is a Chinese food with a rating of 5 out of 5, located in riverside, and is family-friendly, but not near Caf\u00e9 Rouge.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-caf71d305d58409fb733eb474cf77a92", "prediction": " \"In the 1996-97 FA Cup, Nottingham Forest beat Chesterfield 1-0 in the first round.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-a891ea7dc9df4a218b664e6b9a2068c2", "prediction": " Jamie McMurray will be racing in the NASCAR Cup Series on November 2.", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-8174c984717048c8991b61ca64ac79a8", "prediction": " Mary Bridget Meehan disappeared on September 15, 1982, at the age of 18.", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-ba6c7268eca147178460ce3d9c89c758", "prediction": " \"Strada is a Japanese restaurant with a price range of \u00a320-25, rated 5 out of 5, and family-friendly. It is near Rainbow Vegetarian Caf\u00e9.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-85c947bdc78746cb9c7b69c2349f4d54", "prediction": " \"The Cardinals defeated their opponent at St. Louis Cardinals, resulting in a win.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-1da5f3967a9748138e77d13a925615b7", "prediction": " Chicago Road is located in Bronson Township, Michigan, and is designated as US-12 and Prairie River Road.", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-9297769d1c924cde83132205f1d36628", "prediction": " \"Zizzi is a pub that serves English food and is not family friendly.\"\nExplanation: The sentence \"Zizzi is a pub that serves English food and is not family friendly\" captures the facts specified in the triplets [['Zizzi', 'eatType', 'pub'], ['Zizzi', 'food', 'English'], ['Zizzi', 'familyFriendly', 'no']].", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-2056444d5ae9449d996222340c2f8b27", "prediction": " \"Batchoy is a popular Filipino dish made with chicken and ingredients from the Philippines.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-83f99ab0c36247458d1e0e5204b31f48", "prediction": " \"Asam pedas is the national dish of Malaysia, and Putrajaya is the capital city of Malaysia. Arifin Zakaria is the leader of Malaysia.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-3ffbe7696ea44995bb4b9bbe47cf0e5e", "prediction": " \"Alan Bean was selected by NASA in 1963 and was the backup pilot of Apollo 12. He was also the commander of Apollo 12 and the operator of NASA.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-b40d22f8f6e04882a4945e517a8167fa", "prediction": " \"The Waterman is a restaurant that serves food in English, has a price range of \u00a320-25, has a high customer rating, is located on the riverside, and is not family-friendly.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-8bcaa9d028d04a128518251186b05241", "prediction": " \"The Bruins scored 41 points on September 10th, resulting in a win, and their record is now 2-0-0.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-216a4e56737c4fe7a6b6dcbf0887ed68", "prediction": " \"Fitzbillies is a moderately priced Indian restaurant located in the riverside area. It has a 1 out of 5 customer rating and is family friendly.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-695a0814e9a04df7b1e33648597f976a", "prediction": " \"Bandeja paisa is a popular dish in the Paisa Region of Colombia, made with pork belly and other ingredients.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-838ff94eb2594f1a832b8af91798af65", "prediction": " \"Cocum is a moderately priced restaurant with a low customer rating and is family-friendly.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-6c6b8ee81eae4bdca2c3844d38cd8d57", "prediction": " \"The doors' song 'Light My Fire' was originally performed by the Doors.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-1923890b24d14fd1818fe703c92391b6", "prediction": " \"Wildwood is a pub that serves food and has a high price range. It has a rating of 3 out of 5.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-87539532b2444a0eb988cb28fce214a0", "prediction": " \"Wildwood is a coffee shop that serves French food in the price range of \u00a320-25 and has a high customer rating. It is located near Ranch.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-e980605c3a7b45849aef946cf01c765c", "prediction": " \"Finland lost the match to 5th place team.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-c6da9cb4eb884433b1384900ddd29a22", "prediction": " \"We're Callin U is a single by Silk (group) released in 2001.\"\nExplanation: The input triplet is [['[TABLECONTEXT]', 'SINGLE', \"We're Callin U\"], [\"We're Callin U\", 'YEAR', '2001'], ['[TABLECONTEXT]', '[TITLE]', 'Silk (group)'], [\"We're Callin U\", 'ALBUM', 'Love Session']].\nThe sentence generated is \"We're Callin U is a single", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-4be7023024fe4eeb901eca91900bcd9a", "prediction": " \"Cocum is a coffee shop with a rating of 5 out of 5 and is family-friendly.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-9051095002ef4b26a90fe99dc0cc7d8d", "prediction": " \"On October 31, 2015, the site of the Memorial Stadium in Champaign, IL, hosted a football game between Illinois and their opponent, where Illinois lost by a score of 7-24.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-26c1d40c7bac4e9ca755003cc583d39f", "prediction": " \"Dell Curry played for the Toronto Raptors from 1999 to 2002.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-3ff2279201eb41b4b33bba7d49c842de", "prediction": " \"Browns Cambridge is a coffee shop that serves Italian food, has an average customer rating, and is family-friendly. It is located near the Crowne Plaza Hotel.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-3e3407eb1b3c42edadfc861859ae772a", "prediction": " \"Rolando Maran is the manager of A.C. Chievo Verona and Varese Calcio S.S.D.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-07c9c18a6ca341b697a7d585e6bd7e40", "prediction": " \"The Washington Nationals won their game against the Marlins on July 2, 2008, with a score of 4-2.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-f2246caa61784aca82825656980d4881", "prediction": " Browns Cambridge is a Chinese food restaurant located in the city centre, which is family-friendly and near The Sorrento.", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-aa6602e6c34d44988e6a7158edc7701a", "prediction": " \"The Wrestlers is a restaurant that serves food, has a high customer rating, and is family-friendly.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-e541c3668f2943bc84a25c171de13e48", "prediction": " \"Green Man is a cheap restaurant located in riverside with family-friendly environment and near All Bar One.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-6e8a7de88e7842bfa250a170bbaf8ba7", "prediction": " Bolt, also known as Larry Bolatinsky, is a comics character.", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-1d431678636c42689673ab3a5cc610d8", "prediction": " \"The Cambridge Blue is a pub that serves Indian food at a high price range and is near Caf\u00e9 Brazil.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-f90483d0f26a4e888588d76feefee2ee", "prediction": " \"In 2004, Boogie Woogie-Chalo America was a Dance Contestant in the Mauli Dave.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-c4399c66944c4521b9dbbb2747d91da2", "prediction": " \"The Cambridge Blue is a pub that serves Japanese food for less than \u00a320 and is near Caf\u00e9 Brazil.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-849e10740df342eb9f7be724035d2ac3", "prediction": " \"The Dumpling Tree is a pub that serves Indian food and is near The Portland Arms.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-024f908c946243b4a7a1a25a457e3885", "prediction": " \"The standard HTML 3.2 document contains a list of XML and HTML character entity references, including the nbsp character.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-7be636413e4542d7b51b38b1a6f4432a", "prediction": " \"The United States Air Force operates the Al Asad Airbase, which is equipped with Lockheed AC-130 attack aircraft. During the Korean War, the United States Air Force used General Dynamics F-16 Fighting Falcons and Lockheed C-130 Hercules transport aircraft.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-9439c5d2bafe46bd84f22f7d737179da", "prediction": " \"Merrill Bobolit directed by Charles Haid, written by Sean Jablonski and Brad Falchuk, and has a number of 54.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-6ea1ccb5e2ac43169947d0b3668fe2ac", "prediction": " \"The Cricketers are a coffee shop with a 5-star customer rating and a family-friendly atmosphere that is near The Portland Arms.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-8e07437eb74b422588dc12393ea70768", "prediction": " \"The Cricketers are known to eat at the coffee shop, and they enjoy Indian food. They have an average customer rating, and they are near The Portland Arms.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-3d6b16e945c2423495e50b01e1ab1ab6", "prediction": " \"Larry Perkins and Tomas Mezera, representing Perkins Engineering, drove the Holden VL Commodore SV in the 1990 Nissan Sydney 500.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-6eea7e195c824fbdb2f8dc947d81f4da", "prediction": " \"The Rice Boat is a food item that is Indian in origin, has a price range of less than \u00a320, has a low customer rating, is located on the riverside, is not family-friendly, and is near Express by Holiday Inn.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-2e3f91142deb417e9b97395bc19b2e04", "prediction": " \"Bakso is a Chinese dish made with tapioca as an ingredient.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-6e4e1e33787b4c3fa384d61051cc9b61", "prediction": " \"The New Model Police Revolver was manufactured between 1865 and 1873.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-1d57b85043f948b1a696ddfe8fdd459d", "prediction": " \"Korea Republic has 14 CLUBS.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-00139a31bb004b2cae11cce11d704ad6", "prediction": " \"The Cambridge Blue is a restaurant that serves the Cambridge Blue and has an average customer rating.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-834a4858278b4f78b51935484a8316f8", "prediction": " \"The Golden Curry is a popular Italian restaurant in the city centre, with an average customer rating and a family-friendly atmosphere. It is located near Caf\u00e9 Rouge.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-d5638a227cbd40ed9ecb9c9f86bceff9", "prediction": " \"The Eagle is a cheap, family-friendly coffee shop located in the city centre, with a 5-star customer rating and a fast food menu.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-b1f08d990ddb45e7adc4fd27a4eb2800", "prediction": " \"Dick Campbell from Furman won 15 games.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-b6aa8e467cfc482fab8e66057b9131a7", "prediction": " \"In July 2013, Net Market Share was 0.05%.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-448d142519834d36832811300ee30f47", "prediction": " \"The Akita Museum of Art is located in Akita, Japan, which is part of Akita Prefecture.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-daa492d2a9e444ce9eb939eef335a579", "prediction": " AWH Engineering College, a premier engineering college in India, was established in 2001 with a student body of 250. It is located in the city of Kuttikkattoor in the state of Kerala.", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-8e3054d4a8694a29bdac349f5392406f", "prediction": " \"Loch Fyne is a Chinese food restaurant with a 3 out of 5 customer rating, located riverside and near The Rice Boat.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-63deaa57b5fc459b87ebacf88324e2c7", "prediction": " \"Shallan Meiers is the CENTERFOLD_MODEL for the 9-02 issue, while Jordan is the COVER_MODEL for the same issue.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-a9473bb108564dd9be6879bfba95d171", "prediction": " \"In 1991, Alexi Lalas, Rutgers Mike Lapper, UCLA Cam Rast, Santa Clara were named to the NCAA Division I men's soccer First-Team All-America teams. Gerell Elliott, Fresno State, Henry Gutierrez, NC State, Manny Lagos, Milwaukee, and Dante Washington, Radford were named to the NCAA Division I men's soccer", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-8d93a2eaa26246b084879277089d448b", "prediction": " \"The Mill is a pub that serves Italian food and is located riverside.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-557d090292b1448d8d5c89e20a4d9240", "prediction": " \"The Wrestlers are a restaurant that serves French food at a price range of \u00a320-25. They have a rating of 3 out of 5 and are family-friendly.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-76c48ff8265643ffa00dff3e924201c5", "prediction": " \"Why Can't This Be Love is the third song in the album.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-5c18122cdbec42b093f9ef4aef79fdd8", "prediction": " \"The WebKit version is 525.21.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-88d56296e8e84c24a7ee513299b8d1ef", "prediction": " \"The Rice Boat is a fast food restaurant with a low customer rating and located on the riverside.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-1b57d43a09b64466b5ffc31d6b848e57", "prediction": " \"The Complete Seventh Season is available in Region_1_(US) and Region_4_(AUSTRALIA) on January 16, 2007 and April 1, 2010.\"\nExplanation: The sentence generated captures the facts specified in the triplets. It is a well-formed sentence that is easily understandable by a human.", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-6f05c0d9879e416fae92347d52d81c37", "prediction": " \"In 1634, the Ram Rebellion occurred in the United States, which was led by Barack Obama, a leader of Native Americans in the United States.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-39ecf0e3f0b14895b050d15fc573b6a2", "prediction": " Curitiba is a city in Brazil.", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-88d1a4366dc644c49c88fa8123897ceb", "prediction": " \"The Rice Boat is a restaurant that serves Italian food and has a high customer rating. It is located on the riverside and is family-friendly.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-bd93901079454c978fd2fc5f89aa3728", "prediction": " \"The Waterman is a restaurant that serves French food, has a price range of more than \u00a330, has a low customer rating, is located on the riverside, and is not family-friendly.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-9f92e53b7b264f6ca3ea3aa107f506c3", "prediction": " \"The Waterman is a restaurant in the city centre that serves Indian food and has a low customer rating.\"\nExplanation: The sentence captures the facts that The Waterman is a restaurant, it is located in the city centre, it serves Indian food, and it has a low customer rating.", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-13c07c8a2dc84a35be5a33481c6f7ddc", "prediction": " \"The Eagle is a coffee shop located in riverside, family-friendly, and near Burger King.\"\nExplanation: The sentence generated captures the facts specified in the triplets. It is a well-formed sentence that is easily understandable by a human.", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-a0ce01a5508e4869beeaa86e40fc034f", "prediction": " \"Zizzi is a restaurant that serves food in English, has a 5 out of 5 customer rating, and is family-friendly.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-b60e05edaa0e4d1c864e1295bf294856", "prediction": " \"The Cricketers are a group of people who enjoy eating at a coffee shop, and they often eat fast food. They have a rating of 3 out of 5, and they are a family-friendly group of people. They are near The Portland Arms.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-3de3a67ea86a45ab9e03b507c167c9ea", "prediction": " Alan Bean was born on March 15, 1932 and was selected by NASA in 1963. He is now retired.", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-e2370b50a2f242d5aa114fa7c7e93c02", "prediction": " Cotto is a restaurant that serves Chinese food and has an average customer rating. It is family-friendly and is near Ranch.", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-820dc098591b4fa382c456f9f8e15e3f", "prediction": " \"Zizzi is a restaurant that serves pub food, has a high customer rating, and is family-friendly.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-d8a82a2ab4da494787635d3a79df71eb", "prediction": " \"Alimentum is a restaurant that serves food of the eatType and has a priceRange of more than \u00a330.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-b07a23d86028481ea75ede39384f928b", "prediction": " Andrew Jackson was the seventh President of the United States and a member of the Democratic Party.", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-693db24051ef4e5b8eec491a6a357513", "prediction": " \"The United States Air Force operates the Al Asad Airbase, which is equipped with Lockheed AC-130 attack aircraft, Boeing C-17 Globemaster III transport aircraft, and McDonnell Douglas F-15 Eagle fighter aircraft. The United States Air Force also participated in the United States invasion of Panama.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-8b8bcda0521843fbb4502372cc1154bb", "prediction": " \"The United States is a country where English is the official language, and Alcatraz Versus the Evil Librarians is a book set in the United States, which is home to Native Americans.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-11061972421d4f739794dbe9f9e9fd1a", "prediction": " \"The Golden Curry is a restaurant in the city centre that serves low-rated English food to its customers.\"\nExplanation: The sentence captures the facts that The Golden Curry is a restaurant, it is located in the city centre, it serves low-rated English food, and it is near Caf\u00e9 Rouge.", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-207175ad8e5c4ac68ffd4de52d55cfe6", "prediction": " \"Suicide Tour was held at Old Trafford Cricket Ground in September 2009. It was a tour by [TITLE] and supported by [SUPPORTING_ACT(S)].\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-110a9717a9534ce6b153e6e1dd52420c", "prediction": " \"The Wrestlers is a restaurant that serves Italian food, has a high price range, has a 1 out of 5 customer rating, and is family-friendly.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-47d06da12e654e289fd4cad8e826c175", "prediction": " \"The Phoenix is a moderately priced restaurant located in riverside that serves French food.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-21618a4bea054393b1531139954b0da6", "prediction": " Jupiter, the BUILDER, completed construction of Yarrow & Company, Scotstoun on 25 June 1939. Its PENNANT_NUMBER is F85.", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-0c928da3beb74a3a8197c803c2d8921c", "prediction": " \"Al Asad Airbase is located in Al Anbar Province, Iraq.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-2fe583de43094fe782e00607b73653e4", "prediction": " \"Catalonia's LEADER_NAME is Carles Puigdemont.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-26b34bb4763047ad95f3121a9b456c55", "prediction": " \"A.F.C. Blackpool is a football club based in Blackpool, England. It plays in the English Football League Championship. The club has a membership of 1500 people and its full name is Association Football Club Blackpool.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task1409-225a6997be374f41a9741df5f7f12dfe", "prediction": " \"Browns Cambridge is a coffee shop that serves Italian food, has a high customer rating, is located in a riverside area, is family-friendly, and is near the Crowne Plaza Hotel.\"", "task_name": "task1409_dart_text_generation"}
{"id": "task738-4e27b40530384e26907b6b3899c65e9e", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-f405a108255a49829258dc875871d403", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-5f23c6a5dcf04f4dbc99d60a0de3f8b1", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-705166c3e89c4592a76cff0ba9d23412", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-a3af15e4f4bd471785d2502f83d9d23b", "prediction": " The perspective supports the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-4be21800240d4845ba740476bbb883c3", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-2880786d4b814c529969d5d6413a7ef1", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-d58e8aefaf6d41cb9637639710099c77", "prediction": " The perspective supports the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-61c1284d412945a6aec9923715a26071", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-ab31af403fda4d1eab24feb986bb2eed", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-d7517b82636845ccaf581710c1786b73", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-4c25946db33a4796999b5573c73fc7f9", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-05029cee5715413f87a2c27f04464cff", "prediction": " The perspective supports the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-8a219f3055ac435eb6c6579811e91f32", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-0dc2969f647f46ea85d2f9c7272372c3", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-183df33ccdc1498b8c07ddaa229cdcd2", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-05372a3b37b44a12816e461b53a90b14", "prediction": " The perspective supports the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-694985eb3c6847c08420c452fbe96bbd", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-fe49cea7bf4149c9b63ae83b1d0620cd", "prediction": " The perspective supports the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-ad88c8240bef408cb2a5a5e912d45cdc", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-c4fef502968e4f16bcb8a877605baf5c", "prediction": " Supporting", "task_name": "task738_perspectrum_classification"}
{"id": "task738-cb70b52f6a6f4371913f896026fe57e6", "prediction": " Supporting", "task_name": "task738_perspectrum_classification"}
{"id": "task738-9be783cf525b476aa7e23d67a2b751d3", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-f70ff5d28f9d42728d88247771f250a3", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-ce444d1ebe2d4d829618be61f0c886da", "prediction": " Supporting", "task_name": "task738_perspectrum_classification"}
{"id": "task738-dfc66c15d1a14df1aebb98a944e0505e", "prediction": " Supporting or Undermining?\nSupporting: The perspective supports the claim because it implies that surveillance is acceptable if you have done nothing wrong.\nUndermining: The perspective undermines the claim because it implies that surveillance can be abused and that there is a possibility of innocent people being targeted.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-90511c7758f64ae09894542bbc96ddfe", "prediction": " Supporting. The perspective that humans need to consume meat supports the claim that humans should eat meat.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-2a82544cc2ec4e0290ba18b2ee7e57e9", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-924bf0c3735448ff94bf7087b828a0bb", "prediction": " Supporting", "task_name": "task738_perspectrum_classification"}
{"id": "task738-24f205f58a094d179df4798f7316ac61", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-04abc488078a4f3fa2c4a502e7942bb7", "prediction": " The perspective supports the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-3a33c9f6a00e466488e49f9eda563126", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-b7ab2d4c9f884f52b72b3dc6f1a3defc", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-5f49b6b345e34690a24b71cb7d0551be", "prediction": " The perspective supports the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-bc3936552f8d489baf6b2f18ceb6ab5c", "prediction": " Supporting", "task_name": "task738_perspectrum_classification"}
{"id": "task738-4e929db6ffd4450ab7814fcc14073ab1", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-bfab79adc763431bb63855906e314f1c", "prediction": " Supporting. The perspective suggests that banning alcohol would lead to healthier individuals, which could potentially convince someone with a different view.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-efaceb7aa4f145f0a510b28f5d9d70e1", "prediction": " Supporting or Undermining", "task_name": "task738_perspectrum_classification"}
{"id": "task738-cb8d66580b4f4aadbc4aa9ddbd366f79", "prediction": " Supporting", "task_name": "task738_perspectrum_classification"}
{"id": "task738-80243833e4d248c2b7fb0a12a424e157", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-f48b10cec59b49d9afa4b15d9b0d326a", "prediction": " The perspective supports the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-c75b640e4706416eafe88d5bd54acc9d", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-5ddd2368009f452da30f169df5dee9f1", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-ca1c09f574b94fbab3da6dec416b3c1f", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-3f27f9f37fd74704b22c784e4250075b", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-046847ba357d43b9aadddb7c3a009d2e", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-0481c6b7ef394384ac321241e23ab4a5", "prediction": " Supporting.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-01eaf133ca75462e9c1f712b48fe2db1", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-1635378eae0b46b8853e0bb193dfb752", "prediction": " supporting or undermining.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-2a2757cc6263492ca8e0aaea405c6d69", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-613d9f03c7d94867ab9abac4d9406e0e", "prediction": " The perspective supports the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-ef2f90bc859847788887c2e400f426c3", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-7cf95279b01840daba772486648f97e4", "prediction": " Supporting", "task_name": "task738_perspectrum_classification"}
{"id": "task738-5e91692bd2e547108a36b3b447078839", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-285b908c090a435f921fad33b9edaed0", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-1abc6119ecf644b49c4f6a4672dfbd96", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-703144a133834e44bd79be27b00accd8", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-3a5a3d4000994cd9b6fde3c7bc17c64e", "prediction": " The perspective supports the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-04811aea8fdc4cbd87cbcfee331c9872", "prediction": " Supporting", "task_name": "task738_perspectrum_classification"}
{"id": "task738-6285e70451cf4bfd831d1b17653723de", "prediction": " supporting", "task_name": "task738_perspectrum_classification"}
{"id": "task738-9b37bb8d89034801a6b3366da8d774aa", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-408d337e89f7458e8bf77a7078879ed8", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-15ec496db4be413ca8c05575e2b40bdc", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-c6f89b1e704e412aaeb3179dcb2b6494", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-f67ccafc05d642feb3b4153fadc745d1", "prediction": " The perspective supports the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-eed35b061e2e4bf3b8547c541a701478", "prediction": " Supporting or Undermining?\nSupporting.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-a25caa376d214ff7ad22e27b8b2a63e6", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-1ff5ffc142b74f26abed7b6da56d3c2b", "prediction": " Supporting or Undermining?\nSupporting: Yes, the perspective supports the claim.\nUndermining: No, the perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-0fc93564f84d4f68aec8f5ac429477f6", "prediction": " Supporting. The perspective supports the claim because it suggests that the glorification of violent and criminal lifestyles in music can lead to bad things in real life.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-edb228512b8a463f8c4bc9992076d18f", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-592a0d48711d4aa8b2dea61466baf742", "prediction": " Supporting or Undermining?\nSupporting: Yes, the perspective supports the claim.\nUndermining: No, the perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-e02afe27adbe48d0b4018e13de7d4035", "prediction": " The perspective supports the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-f9c138a1ebab4d50ba4979bb9d314402", "prediction": " The perspective supports the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-328aa877c2a642cfbfbbf461aef73b1f", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-6f5df07291a24b3a9003a039a057e604", "prediction": " Supporting", "task_name": "task738_perspectrum_classification"}
{"id": "task738-fd1195b245f543439141b06b9a5f042b", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-6422541622a84c75b4ec94066187429f", "prediction": " The perspective supports the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-d641b85ab0154756828dc77d6013bd9e", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-8e3f581806364958a14888fe6342d248", "prediction": " Supporting or Undermining? Supporting.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-f0c7fe8bc8d04635a86f7d618285043d", "prediction": " Supporting or Undermining?\nSupporting: If the perspective is that Africa has invited ICC intervention, then it supports the claim that the ICC is biased against Africa.\nUndermining: If the perspective is that Africa has not invited ICC intervention, then it undermines the claim that the ICC is biased against Africa.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-255ca0e96f5344399f1a5c8a33b4f5c2", "prediction": " The perspective supports the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-31cd8bb5e01a4a02a5b4a31adf84d0ec", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-f63d8c45af8d46b1998a503fc3c8b0f7", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-650a2eb129ba4b53adf2c7393e9d14aa", "prediction": " Supporting.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-9ce764745c054184b2f47654f26eeb9c", "prediction": " Supporting", "task_name": "task738_perspectrum_classification"}
{"id": "task738-77d316a0904542439472366ea7f1a4fa", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-d640cd098dd04be792f0e73ea1643984", "prediction": " Supporting. The perspective supports the claim that affirmative action is necessary.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-243dddb8199a4e8aa04cdc4922be0f22", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-5f37866ce4f24a90a9cb5bec93a341a4", "prediction": " Supporting", "task_name": "task738_perspectrum_classification"}
{"id": "task738-9dbc9bdf98554de58accde2692e3eb6b", "prediction": " Supporting or Undermining", "task_name": "task738_perspectrum_classification"}
{"id": "task738-b86ab64267e942228798937afc1bd5f5", "prediction": " The perspective supports the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-ee2c8592232042efb49aa9e3ff87fbba", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-a0aa720c34944ad19a0b46dc975282fa", "prediction": " Supporting", "task_name": "task738_perspectrum_classification"}
{"id": "task738-2d7c5772545c4cacb623814e9d6bc634", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-692e4963655f4394afb2799c1aea7f87", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-56fe6b3d71d34a6f918510cecae8401f", "prediction": " The perspective supports the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-8ed90077392746db8cdfcd41b0abc52a", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-ba6edbe237a94472bf85b6d62edef8cd", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task738-a8ac62c1d93d4ccf8941d01c0ebbbf81", "prediction": " Supporting or Undermining", "task_name": "task738_perspectrum_classification"}
{"id": "task738-52b0a86447f843f2a5054ce0c597f71a", "prediction": " The perspective undermines the claim.", "task_name": "task738_perspectrum_classification"}
{"id": "task1158-0c8bafcca2104450938f7df7f0650960", "prediction": " bottle : pour.\nExplanation: The phrase \"jug : pour\" relates the jug to its appropriate manipulation, which is pouring. The phrase \"bottle : ?\" relates the bottle to its appropriate manipulation, which is pouring. The answer to the question mark is \"pour\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-01372125a3184bf8a5efc2d12b645fea", "prediction": " peel.\nExplanation: The phrase \"bag : unzip\" relates the bag to its appropriate manipulation, which is unzipping. The phrase \"banana : ?\" relates the banana to its appropriate manipulation, which is peeling. The correct answer is \"peel\" as it is the verb that shows a way to manipulate the given object.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-69b4b8e7686942b493a8986f51b78a7d", "prediction": " jar : open. walnut : crack.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-c9f513e61fc443409aaa7ac8cdec5bce", "prediction": " turn on.\nInput: letter : write. envelope : ?\nOutput: seal.\nInput: letter : read. book : ?\nOutput: open.\nInput: letter : send. envelope : ?\nOutput: address.\nInput: letter : receive. envelope : ?\nOutput: open.\nInput: letter : write. envelope : seal.\nOutput: address.\nInput: letter : read. book : open.\nOutput: read.\nInput: letter : send. envelope : address.\nOutput: write.\nInput: letter : receive. envelope :", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-1a9d708e8cee4f9191b2925c37d55332", "prediction": " open : jar.\nExplanation: The phrase \"present : open\" relates the present to its appropriate manipulation, which is opening a jar. The phrase \"jar : ?\" relates the jar to its appropriate manipulation, which is opening. The answer to the question mark is \"open\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-2bc833f8dfa549b588905da97b67bdca", "prediction": " peanut : shell. safe : store.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-8ea7fd0af709421eb72674b955d854c9", "prediction": " pump : fill.\nExplanation: The phrase \"box : open\" relates the box to its appropriate manipulation, which is opening. The phrase \"pump : ?\" relates the pump to its appropriate manipulation, which is filling. The answer to the question mark is \"fill\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-0b4abd509d824b6eb2e2880c4681120a", "prediction": " backpack : open.\nExplanation: The phrase \"candy : unwrap\" relates candy to its appropriate manipulation, which is \"unwrap\". The phrase \"backpack : ?\" relates backpack to its appropriate manipulation, which is \"open\". The answer to the question mark is \"open\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-13bb7360f3b04602a9ea6ec7bc13c31f", "prediction": " shine : polish.\nExplanation: The phrase \"box : open\" relates box to its appropriate manipulation, which is opening. The phrase \"shoe : ?\" relates shoe to its appropriate manipulation, which is polishing. The answer to the question mark is \"shine\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-79a7e91db6bf417d88645d1fc51adbd5", "prediction": " bag : open. candy : eat.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-f4bcf5b7233e46ffb08179a72b887866", "prediction": " bottle : open.\nExplanation: The phrase \"bag : unzip\" relates the bag to its appropriate manipulation, which is unzipping. The phrase \"bottle : ?\" relates the bottle to its appropriate manipulation, which is opening. The answer to the question mark is \"open\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-8af05413ad2c42268f8bbffe9dd6b71a", "prediction": " peel : remove.\nInput: bag : open. banana : peel.\nOutput: peel : remove.\nInput: bag : open. banana : peel.\nOutput: peel : remove.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-654f36857f644dc293beb9a1ed276186", "prediction": " pump : pump. walnut : crack.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-b1d4a24263ee44659b5165d5bdafb9bc", "prediction": " turn on.\nInput: knife : sharpen. spoon : ?\nOutput: wash.\nInput: pan : ?.\nOutput: ?", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-a918941347ff42adaaebdf176a874e47", "prediction": " jar : open.\nExplanation: The phrase \"backpack : unzip\" relates the backpack to its appropriate manipulation, which is unzipping. The phrase \"jar : ?\" relates the jar to its appropriate manipulation, which is opening. The answer to the question mark is \"open\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-1b9e18b9670543989fc0881c1c314364", "prediction": " safe : lock.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-c75c20c3c7a547148a63ea472452fe14", "prediction": " backpack : unzip. pump : inflate.\nInput: backpack : unzip. pump : inflate.\nOutput: backpack : unzip. pump : deflate.\nInput: backpack : unzip. pump : inflate.\nOutput: backpack : unzip. pump : deflate.\nInput: backpack : unzip. pump : inflate.\nOutput: backpack : unzip. pump : deflate.\nInput: backpack : unzip. pump : inflate.\nOutput: backpack : unzip. pump", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-c85ecb8405c14dc8854908e40bc9e401", "prediction": " unpack.\nExplanation: The phrase \"safe : unlock\" relates the safe to its appropriate manipulation, which is unlocking it. The phrase \"backpack : ?\" relates the backpack to its appropriate manipulation, which is unpacking it.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-551af2d4ca6b4c83a7ed2b637fcc49c3", "prediction": " shine : polish.\nExplanation: The phrase \"faucet : turn\" relates turning the faucet to its appropriate manipulation, which is \"turn\". The phrase \"shoe : ?\" relates to the shoe, but the relation is not clear. The phrase \"shine : polish\" relates to the shoe, and the appropriate manipulation is \"polish\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-63764fd0d3414b10a1c171b8c943fb70", "prediction": " candy : eat.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-2f528e9b2edf48f7a907809c85cd756a", "prediction": " pour.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-70afd284560f4ff89274c1b4c175d865", "prediction": " peel : banana.\nExplanation: The phrase \"pump : pump\" relates pumping to its appropriate manipulation, which is pumping. The phrase \"banana : ?\" relates banana to its appropriate manipulation, which is peeling. The answer to the question mark is \"peel\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-7aed27d5da9c48149aedde5d66cdded8", "prediction": " peas : cook.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-dbb9abf217094efea5511af4a07ec61f", "prediction": " faucet : turn on.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-41c06058a6fa4012924158cb8452458b", "prediction": " box : open.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-0bb33112bfee403696ca47f48818872b", "prediction": " safe : unlock.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-de7e5d34229547cfbea210998ade4a8a", "prediction": " pump : close.\njar : open. pump : close.\njar : open. pump : close.\njar : open. pump : open.\njar : open. pump : open.\njar : open. pump : close.\njar : open. pump : close.\njar : open. pump : close.\njar : open. pump : close.\njar : open. pump : close.\njar : open. pump : close.\njar : open. pump : close.\njar : open. pump : close.\njar :", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-b4722692a4074c6eba1d502cc7d8e672", "prediction": " backpack : unpack.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-f6e2fa6d103e437fad1372fe2fd49a8d", "prediction": " shine : polish.\nExplanation: The phrase \"letter : open\" relates the letter to its appropriate manipulation, which is opening. The phrase \"shoe : ?\" relates the shoe to its appropriate manipulation, which is polishing. The answer to the question mark is \"shine\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-f1eabcee86a6464c9837aa5a24c71a7e", "prediction": " box : open, candy : eat, candy : lick, candy : taste, candy : chew, candy : suck, candy : nibble, candy : munch, candy : gobble, candy : devour, candy : chomp, candy : crunch, candy : munch, candy : nibble, candy : nibble, candy : nibble, candy : nibble, candy : nibble, candy : nibble, candy : nibble, candy :", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-7d8bef46d1d74df290f06ed8a3336a19", "prediction": " pour : ?", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-0680bca794114f8090ea1e66b7b2f362", "prediction": " peel : remove.\nfaucet : turn\nbanana : ?\npeel : remove", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-33108e1b79cf4fe29129f8ae079f35e0", "prediction": " walnut : crack.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-7f4a8f0f71954cb583d4ef0e84d4e9ed", "prediction": " faucet : turn on.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-19a44b8e70ed4e43a037af5eec04f23c", "prediction": " write : letter.\nbanana : peel.\nletter : ?\nwrite : letter.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-50cb958e495e4e2eb29803d60cb536ad", "prediction": " safe : open.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-e810e7eec2ec444a9a889387a0c1f313", "prediction": " pump : fill.\nExplanation: The phrase \"banana : peel\" relates the banana to its appropriate manipulation, which is peeling. The phrase \"pump : ?\" relates the pump to its appropriate manipulation, which is filling. The answer to the question mark is \"fill\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-d6dc55f35aa24cc7b2ad720b56d920aa", "prediction": " pack.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-699b911ad53f4f73994097c59aacb401", "prediction": " shine : polish.\nExplanation: The phrase \"candy : unwrap\" relates candy to its appropriate manipulation, which is unwrapping. The phrase \"shoe : ?\" relates shoe to its appropriate manipulation, which is polishing. The answer to the question mark is \"polish\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-f8dcefda96034379997f22956debb930", "prediction": " peanut : shell. candy : eat.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-f398a9193ad644db9f2cec022dd45edb", "prediction": " bottle : pour.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-e502023c9d83403fba611984fb0bc94a", "prediction": " peel.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-bde2a4df7b20403dbc893ac5b8429700", "prediction": " peas : shell.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-aec7f6e6eb60449e83fe0e96f36e87fb", "prediction": " turn on.\nInput: pan : flip. lid : ?\nOutput: remove.\nInput: bowl : fill. spoon : ?\nOutput: stir.\nInput: plate : place. fork : ?\nOutput: eat.\nInput: knife : cut. vegetable : ?\nOutput: slice.\nInput: spoon : stir. soup : ?\nOutput: serve.\nInput: spoon : stir. soup : ?\nOutput: serve.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-dbede4a01d0e4c54b12447222512c541", "prediction": " peas : shell. drawer : remove.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-55ee8007a7324e64b09f20b1b1016f0c", "prediction": " safe : unlock.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-6e65ef1edd1445fb83c909fdb2ffa453", "prediction": " pump : fill.\nExplanation: The phrase \"suitcase : open\" relates the suitcase to its appropriate manipulation, which is opening. The phrase \"pump : ?\" relates the pump to its appropriate manipulation, which is filling. The answer to the question mark is \"fill\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-a912639bcce748c39aa930262bbe446c", "prediction": " backpack : close.\nExplanation: The phrase \"bag : open\" relates the bag to its appropriate manipulation, which is \"open\". The phrase \"backpack : ?\" relates the backpack to its appropriate manipulation, which is \"close\". The answer to the question mark is \"close\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-0bb50c984e3e4de59053cdf90db613cc", "prediction": " shine.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-8fb702653a9943eb89e76673c785b8d9", "prediction": " jar : open, candy : eat.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-26def4d0cdfe4577a9a1c4dd47d8c623", "prediction": " pour : fill.\nTask definition: The phrase \"safe : unlock\" relates the safe to its appropriate manipulation, which is unlocking it. Your task is to replace the question mark (?) with the appropriate manipulation of the jug, following the \"safe : unlock\" relation. Your answer should be a verb which shows a way to manipulate the given object.\nInput: safe : unlock. jug : fill.\nOutput: safe : unlock. jug : fill.\nTask definition: The phrase \"safe : unlock\" relates the safe to its appropriate manipulation, which is unlock", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-94c40a26fdaf4c9c8c71c160aa20618a", "prediction": " letter : peel. banana : peel.\nInput: letter : open. banana : peel.\nOutput: letter : peel. banana : peel.\nInput: letter : open. banana : peel.\nOutput: letter : peel. banana : peel.\nInput: letter : open. banana : peel.\nOutput: letter : peel. banana : peel.\nInput: letter : open. banana : peel.\nOutput: letter : peel. banana : peel.\nInput: letter : open. banana", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-4c56c365b27944799038aefed7de2d19", "prediction": " boil : cook.\nExplanation: The phrase \"present : open\" relates the present to its appropriate manipulation, which is opening. The phrase \"peas : ?\" relates the peas to its appropriate manipulation, which is cooking. The answer to the question mark is \"boil\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-c1af1731a3ba48e8acdb189a6f89d66b", "prediction": " tap : turn on.\nInput: bag : unzip. faucet : turn on.\nOutput: bag : unzip. faucet : ?\nInput: bag : unzip. faucet : turn on.\nOutput: bag : unzip. faucet : unzip.\nInput: bag : unzip. faucet : unzip.\nOutput: bag : unzip. faucet : turn on.\nInput: bag : unzip. faucet : turn on.\nOutput: bag : unzip. faucet : unzip.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-03aab65a22f94b35b72a34f3f3c7493c", "prediction": " letter : write.\nExplanation: The phrase \"bottle : pour\" relates the bottle to its appropriate manipulation, which is pouring. The phrase \"letter : ?\" relates the letter to its appropriate manipulation, which is writing. The answer to the question mark is \"write\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-ab66abaf28e3476b8d995a7fab55a615", "prediction": " lock : open.\nExplanation: The phrase \"present : open\" relates the present to its appropriate manipulation, which is opening. The phrase \"safe : ?\" relates the safe to its appropriate manipulation, which is opening. The answer to the question mark is \"lock\", as the safe can be opened by opening the lock.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-2f4fbea4ac1c43e58015e3b38d8aef9a", "prediction": " pump : open.\nExplanation: The phrase \"faucet : turn\" relates turning the faucet to its appropriate manipulation. The phrase \"pump : ?\" relates the pump to its appropriate manipulation. The answer to the question mark is \"open\" because opening the pump is the appropriate manipulation.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-52cccf2a9f27425faac282f8f7191345", "prediction": " backpack : fill.\nExplanation: The phrase \"jug : pour\" relates the jug to its appropriate manipulation, which is pouring. The phrase \"backpack : ?\" relates the backpack to its appropriate manipulation, which is not given. The correct answer is \"backpack : fill\" because filling is a way to manipulate a backpack.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-32b3f3a28d9143ae8901ba95910f3cf0", "prediction": " polish : shine.\nwalnut : shell. shoe : ?\npolish : shine.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-6c4f943556e743aeb4244d0132c4b462", "prediction": " candy : melt.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-ee2fa42738684b35aed57cad21d7c072", "prediction": " bottle : pour.\nExplanation: The phrase \"suitcase : open\" relates the suitcase to its appropriate manipulation, which is \"open\". The phrase \"bottle : ?\" relates the bottle to its appropriate manipulation, which is \"pour\". The answer to the question mark is \"pour\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-a609632fc8d146cd938a1b53cacd2ae3", "prediction": " peel : open.\nExplanation: The phrase \"present : open\" relates the present to its appropriate manipulation, which is opening. The phrase \"banana : ?\" relates the banana to its appropriate manipulation, which is peeling. Your answer should be a verb which shows a way to manipulate the given object.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-bf2666d2a4e449d8aa16bbf25f435031", "prediction": " peas : cook.\nExplanation: The phrase \"drawer : open\" relates the drawer to its appropriate manipulation, which is opening. The phrase \"peas : ?\" relates the peas to their appropriate manipulation, which is cooking. The answer to the question mark is \"cook\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-56337b83bc134565829f696513af6760", "prediction": " turn on.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-6752e236a71842a5ab36a1664bd37e2f", "prediction": " open.\nExplanation: The phrase \"backpack : unzip\" relates the backpack to its appropriate manipulation, which is unzipping. The phrase \"drawer : ?\" relates the drawer to its appropriate manipulation, which is opening.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-28d287a7a5304c2b97b3ad29b8e77029", "prediction": " safe : open.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-bd4b0561b1504ada9989ad8ad31bb067", "prediction": " bottle : pump.\nExplanation: The phrase \"bag : unzip\" relates bag to its appropriate manipulation, which is unzipping. The phrase \"pump : ?\" relates pump to its appropriate manipulation, which is unknown. Your answer should be a verb which shows a way to manipulate the given object. The answer is \"pump\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-714ae4ecef874f6d836f5f7f2cd14d30", "prediction": " backpack : fill.\nExplanation: The phrase \"bottle : pour\" relates the bottle to its appropriate manipulation, which is \"pour\". The phrase \"backpack : ?\" relates the backpack to its appropriate manipulation, which is \"fill\". The answer to the question mark is \"fill\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-97e991bf607a456a9466b6cd5861d0fe", "prediction": " shine : polish.\nExplanation: The phrase \"suitcase : open\" relates the suitcase to its appropriate manipulation, which is opening. The phrase \"shoe : ?\" relates the shoe to its appropriate manipulation, which is not given. Your answer should be a verb which shows a way to manipulate the given object. The verb \"shine\" relates to the shoe, and the verb \"polish\" relates to the suitcase.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-43608022df6c48ffbb3855701d8877c1", "prediction": " melt.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-54dbd67ffa484643be09b36e9478ba2c", "prediction": " bottle : pour.\nExplanation: The phrase \"bag : open\" relates the bag to its appropriate manipulation \"open\". The phrase \"bottle : ?\" relates the bottle to its appropriate manipulation. The answer to the question mark is \"pour\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-e5e8a422e2474363817f01edc2c98c10", "prediction": " peel : remove.\nTask definition: The phrase \"suitcase : open\" relates the manipulation of opening a suitcase to the item \"suitcase\". Your task is to replace the question mark (?) with the appropriate manipulation of the item \"banana\", following the \"suitcase : open\" relation. Your answer should be a verb which shows a way to manipulate the given object.\nInput: suitcase : open. banana : peel.\nOutput: suitcase : open. banana : remove.\nTask definition: The phrase \"suitcase : open\" relates the manipulation of opening", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-7aaf030679c14a3dac91a938b12bbfae", "prediction": " peanut : shell, walnut : crack.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-c97ac0cd1fd14171aee04124fa2cf333", "prediction": " turn on.\nExplanation: The phrase \"candy : unwrap\" relates candy to its appropriate manipulation, which is \"unwrap\". The phrase \"faucet : ?\" relates faucet to its appropriate manipulation, which is \"turn on\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-e16de93cc1fb4276a755f4ffedf86787", "prediction": " open.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-2adb53b591c64e4da0fb9b0909a7043c", "prediction": " safe : open.\nExplanation: The phrase \"candy : unwrap\" relates candy to its appropriate manipulation, which is \"unwrap\". The phrase \"safe : ?\" relates safe to its appropriate manipulation, which is \"open\". Therefore, the answer to the question mark is \"open\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-3b803c92b0bc43b7b2b496e350ce7cac", "prediction": " bottle : pump.\nExplanation: The phrase \"bottle : squeeze\" relates bottle to its appropriate manipulation, squeeze. The phrase \"pump : ?\" relates pump to its appropriate manipulation, which is not given. The answer to the question mark is \"pump\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-00c4956375fa4019ad6148afd81a86b3", "prediction": " backpack : unzip.\nExplanation: The phrase \"faucet : turn\" relates turning the faucet to its appropriate manipulation. The phrase \"backpack : ?\" relates to the backpack, but the relation is not clear. The phrase \"backpack : unzip\" is the appropriate manipulation of the backpack, following the \"faucet : turn\" relation.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-312f6298be8b41e789f9f1f37ed381bc", "prediction": " shoe : close.\nExplanation: The phrase \"jar : open\" relates the jar to its appropriate manipulation, which is opening. The phrase \"shoe : ?\" relates the shoe to its appropriate manipulation, which is closing. The answer to the question mark is \"close\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-477001f7fdc844a4ab9ad514afaf93e2", "prediction": " candy : unwrap.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-604c5888e5f94ce9bcb18c0dc032208b", "prediction": " pour : jug.\nwalnut : shell.\njug : pour.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-d72fa3f72d3a4dc889f21a8da708f5c5", "prediction": " peel : remove.\nInput: box : open. banana : peel.\nOutput: peel : remove.\nInput: box : open. banana : peel.\nOutput: peel : remove.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-218775d57e604f089c654bd7051af93a", "prediction": " peas : boil.\nExplanation: The phrase \"bottle : pour\" relates the bottle to its appropriate manipulation, which is pouring. The phrase \"peas : ?\" relates the peas to their appropriate manipulation, which is boiling. The answer to the question mark is \"boil\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-aa94670bd1c14da095dd591a70e3ee6b", "prediction": " faucet : turn.\nExplanation: The phrase \"bottle : pour\" relates the bottle to its appropriate manipulation, which is \"pour\". The phrase \"faucet : ?\" relates the faucet to its appropriate manipulation, which is \"turn\". The answer to the question mark is \"turn\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-50e0a84a64364bb58638bd7bb3b98369", "prediction": " present : wrap.\nExplanation: The phrase \"jar : open\" relates the jar to its appropriate manipulation, which is \"open\". The phrase \"present : ?\" relates the present to its appropriate manipulation, which is \"wrap\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-3fc91a10baf04e40b9ca230eab1dd0e4", "prediction": " safe : unlock.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-ef689ae29bc249df917d03327cbae19b", "prediction": " pump : close.\nExplanation: The phrase \"drawer : open\" relates the drawer to its appropriate manipulation, which is \"open\". The phrase \"pump : ?\" relates the pump to its appropriate manipulation, which is \"close\". The answer to the question mark is \"close\".", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-1060ebca3e9a4743aedee4cac9789164", "prediction": " backpack : close.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-526d760ccab740c4af60985466efb5dc", "prediction": " shine : polish.\nExplanation: The phrase \"backpack : unzip\" relates backpack to its appropriate manipulation, which is unzipping. The phrase \"shoe : ?\" relates shoe to its appropriate manipulation, which is ?. The answer to the question mark is \"shine\", which is a verb that shows a way to manipulate the given object.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-c8bf0f6039714c58994d44e22eb3613a", "prediction": " melt.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-33a2fe0e03f04e63a471a75ecd72ee40", "prediction": " pour : open.\nTask definition: The phrase \"present : open\" relates the verb \"present\" to the action of opening a bottle. Your task is to replace the question mark (?) with the appropriate verb that shows a way to open the bottle.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-3fc1c1daae8c42f4a30eceef03affe94", "prediction": " peel.\nExplanation: The phrase \"backpack : unzip\" relates the backpack to its appropriate manipulation, which is unzipping. The phrase \"banana : ?\" relates the banana to its appropriate manipulation, which is peeling. The correct answer is \"peel\" as it follows the \"backpack : unzip\" relation.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-6ce8add5144a4acfb5363f0361d88a44", "prediction": " walnut : crack.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-72c72f54ee9a41e88ed8f3fc6beb1687", "prediction": " peanut : shell. faucet : turn on.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-9a9180e243984703b2ed507e762830ca", "prediction": " letter : read.\nTask definition: A suitcase is a type of luggage that is used to carry clothes and other personal belongings. The phrase \"suitcase : open\" relates the manipulation of opening a suitcase to the verb \"open\". Your task is to replace the question mark (?) with the appropriate verb that shows a way to manipulate the given object.\nInput: book : ?\nOutput: book : read.\nTask definition: A book is a written or printed work, usually on paper, that is bound between covers. The phrase \"book : ?\" relates the manipulation of reading", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-81c39a7432c84796b95488d63c8f0ca0", "prediction": " store : wrap.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-946d2212f9d54ec5af678715caf36519", "prediction": " oil : ?", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-3ef60cb0f02647cea418347a207891ba", "prediction": " backpack : unzip.\nExplanation: The phrase \"shoe : untie\" relates the shoe to its appropriate manipulation, which is untying. The phrase \"backpack : ?\" relates the backpack to its appropriate manipulation, which is unzipping.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-06edc36b8536455496a670067997de98", "prediction": " shine.\nExplanation: The phrase \"bottle : pour\" relates the bottle to its appropriate manipulation, which is pouring. The phrase \"shoe : ?\" relates the shoe to its appropriate manipulation, which is ?. The answer to the question mark is \"shine\", which is a verb that shows a way to manipulate the shoe.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1158-42136ac75538478794311d4f519dbdd7", "prediction": " letter : open, candy : eat.", "task_name": "task1158_bard_analogical_reasoning_manipulating_items"}
{"id": "task1344-5925ce3a48824551b33de169e9f3ae70", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-e7ed9df3b2384e02a517fb56e0b344d6", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-839ff7440c3247ed935da29b6d13bb12", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-b2fd8a8719874afd88c1c547133db090", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-19a740cea9db4f0c897e27f06475b38b", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-ab7ad27e565c4374bd4d207ce72657a6", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-ac0d068e327749b9a838d27051533dfa", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-71eff1241b2244a685021249062edf8a", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-ce8f2d2d30524768b12a3430b69821ae", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-08465f2a734640c8b4f082ecf6a5aa65", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-e6413d44910448179c225c4491f9fc08", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-b5047845452e41b9843a47d178fa94ee", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-2fafc229522842b58b338ff5c2a85dec", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-e58a1c99733c4c1cb74e2dd60bd94bec", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-f327e19d0aa84edf88ee6a133d6043d2", "prediction": " 0", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-b6b9bc5e83bf4be0846476e54e1182d6", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-11ce109d008540aabeb5c04ac2c20ab2", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-622923702f5a43e59718ef3dc89ae96c", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-1d6fef87ccfc409583330d24dc9ac210", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-d16d3484a2894b7cb8340aeb1a325fcc", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-611ffe11a057483eb59f2ac5ce2731b8", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-f959e24b54134e31b61086294a518e9e", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-9e4037dc1fc7461098883f49c83a9011", "prediction": " 0", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-cce4793327d344fbb7216eb389d19c8a", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-7a627c9490ed4372a52d50bc7c5336e5", "prediction": " 0", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-d3630f78dacd4a88bbb4937921fdd407", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-c9b230fc7d69447793c264e1a3b6432a", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-ee4f8e4cd680468c9360a6f96b57e8e0", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-86a02f3440704f218cc350f656fea967", "prediction": " 0", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-57c9a56fadda491ba733add85b878c69", "prediction": " 0", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-35cc40c358084bea9b2cfaf65ea66600", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-ffe756bc01e24170ae5568d4cf4f2b51", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-2fedc74e50e64e9d8d1e1aedeb11ef5e", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-f6e5b7432cfc4e84b91452e1f9de771e", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-d531dcefde094b1c8b86bfd74008fbed", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-0aaa89c2ef984470a46bbf4c3bb7b237", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-2dd694e6405e4f2fb6122d69322a00c2", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-28f5fd0a8405467fb25b3ef559059df5", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-69294dbcf74b4f9fa523033cfb0da913", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-b5c5e663de5440a79fd54adbff7fb2e2", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-d32886d71ef54a6e95ce6070c98422b9", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-fcbb8320c000437b965cb688859e1f09", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-f2875da4bbf2411cab8e8ec7072db855", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-867982a3fb924393bfc0f1fc4b87ffa9", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-d78b20463b054320adc2dfebbf3c8113", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-53843f2deaf44919860a32d4a42cc6e7", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-0ac75081836b41e49b0ce1589af6f511", "prediction": " 0", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-8e2412394e4d4b35a88c6d8e6c6b6adf", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-7c01584a1a764d088ef8b69f0690f404", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-46bb5c58e98946bc9ec2407bd30f06aa", "prediction": " 0", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-b6d1bc20966e439d9593f44bee2e7511", "prediction": " 0", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-78cc63b0441b46d499b4c83559e602bc", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-4736bdd96a69446c8991adb4c1241be3", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-15c2333ff04c4e4c82d9d3182bd56912", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-34443927fc8f4c7a83d9d8c343462b5d", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-c02681227b4045f08d4dc6a5be5c5fae", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-3f616cf240ee428f900ddfd17df836f4", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-ba6c71c2edae44fdbb41047bbe94c29e", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-0d16231c98f541e0bca6403b76b4cc85", "prediction": " 0", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-9131c562d5c445639ae72306f811b6ff", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-a5f4d4e601db446ab659a7243e5fa012", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-127db52a97c84a34a170658f52fb83da", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-5b064b2daf384dc0911ba220099259d4", "prediction": " 0", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-53e736929cf24a398b01bd3641a8e41e", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-91cc54d017e44532b2149a6e72393ebd", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-422a13d0b3164e7889f73c0bda159c83", "prediction": "", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-e775ab0d45ed4fada7c6e24a26942af9", "prediction": " 0", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-66bdfe5167514776ba7e4136e686cbae", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-a3902bdc4e554a799975007a84deb73f", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-cd754e001d834340aae8b6d80ec4067a", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-f2ff31ee255343619efdf98390ed5a0c", "prediction": " 0", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-2a92cc915bc54f9992edc3764296550a", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-bac8e28091514b4e822baf3e70bb95fb", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-7d3b111ef5144e979ac2577b38e5e754", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-d9baae3f4d3d4002a1fe9eb65edda0a8", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-7bc8f6a84b2b446d9b6f332d8d0804d0", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-41e991e2f9c44667824908b6dc93c14f", "prediction": " 0", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-fdf0038c163e4593b2196db37b290692", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-8c5b32a00c0b4e129f9121a68ad4f34a", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-e2ec752214204f0bbc4ee5bdce081887", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-5570d46fbd7e406c92cfb583dcd8e863", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-88062ec7ea694059884b2c39a000070f", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-5e5041dc7631465493fcb2a998931e43", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-88cd671ebf1a454daca67dc177f4e7f3", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-5edb9e482977428f95eae8bbefec9180", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-182e9e5a4e3f43ccba7922e6e6e4aff4", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-e524aefebb9545009ac203c1fb7e0ddb", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-3d3f8d0c73d24d96b2511213e646018b", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-4cce316e7c8049d285ad4b0660eaca37", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-040a737cc8824046a6b48ce5962ac94a", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-77a9baab359f4a1fa4b79c5bf009ad9f", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-a3a0971a9d46420190831c2f80d1d562", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-7f170796b1394a8a8baaeb1a1eae3463", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-4b9592275d254aed836e428335f645dd", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-2be999a1cc0e480fb8648d992627c0fc", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-c36acfa9cb67459ba1f5116a05a3d0b2", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-685e0650f6fc424ca0f745922927f36f", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-6e164a9e3c734bc8b6be32e7001cbafe", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-a643374b25bb48909435bf9b496e7d92", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
{"id": "task1344-11df2be664654dc787359a4898ffead0", "prediction": " 1", "task_name": "task1344_glue_entailment_classification"}
