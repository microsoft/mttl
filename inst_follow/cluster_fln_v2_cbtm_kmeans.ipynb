{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import pandas\n",
    "import torch\n",
    "import asyncio\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns   \n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt \n",
    "sys.path.append(\"/home/v-oostapenko/dev/mttl\")    \n",
    "from scipy.stats import entropy as calc_entropy   \n",
    "from mttl.cluster_tuning.encodings import ClusterInfos\n",
    "from mttl.datamodule.alpaca_data_module import AlpacaDataModule\n",
    "from finetune_llama import Config\n",
    "from mttl.cluster_tuning.cluster_reader import ClusterResult\n",
    "from inst_follow.utils import load_model, TopicRouter,disable_torch_init\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "import json\n",
    "import os \n",
    "import sys \n",
    "import numpy as np\n",
    "import click\n",
    "import json\n",
    "import tqdm\n",
    "import copy\n",
    "import torch\n",
    "import datasets\n",
    "from types import SimpleNamespace\n",
    "sys.path.append(\"/home/v-oostapenko/dev/mttl\")\n",
    "from inst_follow.models.clm import CLM  \n",
    "from transformers import LlamaTokenizer  \n",
    "from mttl.models.poly import get_selector     \n",
    "from mttl.models.modify_model import modify_transformer  \n",
    "from finetune_llama import parse_config, Config\n",
    "from inst_follow.utils import load_model, TopicRouter,disable_torch_init\n",
    "from mttl.cluster_tuning.cluster_reader import ClusterResult\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM\n",
    "import openai\n",
    "openai.api_key = \"f48b5a4f15dc4e58991738ab066ba465\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 4\n",
    "# loda cluster data\n",
    "path_to_clusterers = f\"/home/v-oostapenko/dev/mttl/inst_follow/cluster_infos/cbtm/clustered_{nc}_distances.json\"\n",
    "df = pd.read_json(path_to_clusterers, lines=True)\n",
    "df[\"probs\"]=df[\"cluster\"].apply(lambda x: np.array(F.softmax(-torch.tensor(x) / 0.1, dim=-1).tolist()))\n",
    "df[\"entropy\"] = df[\"probs\"].apply(lambda x: calc_entropy(x, axis=-1)/ np.log2(len(x)))\n",
    "# convert to dict, with key being sp_id\n",
    "hash_to_cluster = df.set_index(\"sp_id\").to_dict()[\"cluster\"]\n",
    "# probs = np.array(df[\"probs\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "entr = df[\"entropy\"].values\n",
    "low_entr_idxs = np.where(entr <= 0.1)[0]\n",
    "high_entr_idxs = np.where(entr > 0.4)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset json (/home/v-oostapenko/.cache/huggingface/datasets/json/default-770862d88586cb63/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "100%|██████████| 1/1 [00:00<00:00, 370.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from mttl.dataloader.alpaca_dataset_readers import AlpacaDataset               \n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"yahma/llama-7b-hf\", add_eos_token=True)\n",
    "tokenizer.pad_token_id = 0 \n",
    "max_input_length=512\n",
    "max_output_length=128 \n",
    "data_dir=None          \n",
    "from mttl.dataloader.human_dataset_readers import HumanMixDataset\n",
    "# dataset = AlpacaDataset(tokenizer, max_input_length, max_output_length, data_dir, train_on_inputs=False)    \n",
    "dataset = HumanMixDataset(tokenizer, max_input_length, max_output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rng = np.random.default_rng(42)\n",
    "# all_test_idxs = rng.choice(len(dataset), size=1000, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [10:15<00:00, 162.58it/s]\n",
      "100%|██████████| 100000/100000 [11:03<00:00, 150.66it/s]\n",
      "100%|██████████| 100000/100000 [10:34<00:00, 157.54it/s]\n",
      "100%|██████████| 100000/100000 [09:48<00:00, 169.98it/s]\n"
     ]
    }
   ],
   "source": [
    "nc = 4\n",
    "for nc in [4,8,16,32]:\n",
    "    # loda cluster data\n",
    "    path_to_clusterers = f\"/home/v-oostapenko/dev/mttl/inst_follow/cluster_infos/cbtm/clustered_{nc}_distances_flnv2.json\"\n",
    "    df = pd.read_json(path_to_clusterers, lines=True)\n",
    "    df[\"probs\"]=df[\"cluster\"].apply(lambda x: np.array(F.softmax(-torch.tensor(x) / 0.1, dim=-1).tolist()))\n",
    "    df[\"entropy\"] = df[\"probs\"].apply(lambda x: calc_entropy(x, axis=-1)/ np.log2(len(x)))\n",
    "    # convert to dict, with key being sp_id\n",
    "    hash_to_cluster = df.set_index(\"sp_id\").to_dict()[\"cluster\"]\n",
    "    # probs = np.array(df[\"probs\"].tolist())\n",
    "    cluster_infos_new = ClusterInfos()     \n",
    "    for i,(example, ex_dict_raw) in tqdm.tqdm(enumerate(zip(dataset,dataset.dataset)), total=len(dataset)):\n",
    "        # topics_example = example[\"skill_set\"]\n",
    "        # assert example.input_text.split(\"\\n### Instruction:\")[1].split(\"###\")[0].strip() == ex_dict_raw[\"instruction\"].strip(), (example.input_text.split(\"Instruction:\")[1].split(\"###\")[0].strip(), ex_dict_raw[\"instruction\"])\n",
    "        hash = example.hash     \n",
    "        probs = hash_to_cluster[hash]\n",
    "        is_test = 0\n",
    "        # assert np.sum(probs)==1\n",
    "        # print(np.sum(probs))    \n",
    "        main_t = np.argmin(probs)    \n",
    "        cluster_infos_new.is_test.extend([is_test])     \n",
    "        cluster_infos_new.task_names.extend([str(main_t)])\n",
    "        cluster_infos_new.cluster_ids.extend([int(main_t)])    \n",
    "        cluster_infos_new.hashes.extend([hash])\n",
    "        cluster_infos_new.cluster_dists.extend([probs])\n",
    "    # assert sum(cluster_infos_new.is_test) == len(all_test_idxs)\n",
    "    cluster_infos_new.save(f\"/home/v-oostapenko/dev/mttl/inst_follow/cluster_infos/cbtm/cbtm_tfidf_kmeans_{nc}_flnv2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp_3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
