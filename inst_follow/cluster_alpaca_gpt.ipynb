{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import pandas\n",
    "import torch\n",
    "import asyncio\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns   \n",
    "from matplotlib import pyplot as plt\n",
    "sys.path.append(\"/home/v-oostapenko/dev/mttl\")    \n",
    "from scipy.stats import entropy as calc_entropy\n",
    "from mttl.cluster_tuning.encodings import ClusterInfos\n",
    "from mttl.datamodule.alpaca_data_module import AlpacaDataModule\n",
    "from finetune_llama import Config\n",
    "from mttl.cluster_tuning.cluster_reader import ClusterResult\n",
    "from inst_follow.utils import load_model, TopicRouter,disable_torch_init\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "import json\n",
    "import os \n",
    "import sys \n",
    "import numpy as np\n",
    "import click\n",
    "import json\n",
    "import tqdm\n",
    "import copy\n",
    "import torch\n",
    "import datasets\n",
    "from types import SimpleNamespace\n",
    "sys.path.append(\"/home/v-oostapenko/dev/mttl\")\n",
    "from inst_follow.models.clm import CLM  \n",
    "from transformers import LlamaTokenizer  \n",
    "from mttl.models.poly import get_selector     \n",
    "from mttl.models.modify_model import modify_transformer  \n",
    "from finetune_llama import parse_config, Config\n",
    "from inst_follow.utils import load_model, TopicRouter,disable_torch_init\n",
    "from mttl.cluster_tuning.cluster_reader import ClusterResult\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM\n",
    "import openai\n",
    "openai.api_key = \"f48b5a4f15dc4e58991738ab066ba465\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each \n",
    "df = AlpacaDataModule(Config())\n",
    "dataset=df.get_dataset()\n",
    "def get_samples(dataset, idxs):\n",
    "    samples = []\n",
    "    gt_responses = []\n",
    "    for i in idxs:\n",
    "        sample = dataset[i].input_text\n",
    "        #discard everything after \\n### Response:\n",
    "        sample, response = sample.split(\"\\n### Response:\")\n",
    "        sample+=\"\\n### Response:\"\n",
    "        samples.append(sample)\n",
    "        gt_responses.append(response)\n",
    "    return samples, gt_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_set = [\"writing\",\n",
    "             \"roleplay\", \n",
    "             \"comprehension\", \n",
    "             \"common-sense reasoning\", \n",
    "             \"coding\", \n",
    "             \"math\", \n",
    "             \"factual knowledge\", \n",
    "             \"conterfactual reasoning\", \n",
    "             \"navigation\",\n",
    "             \"planning\",\n",
    "             \"social skills\",\n",
    "             \"communication\",\n",
    "             \"coordination\",\n",
    "             \"negotiation\",\n",
    "             \"leadership\",]\n",
    "\n",
    "skill_set = (\n",
    "    \"invent\",\n",
    "    \"solve\",          \n",
    "    \"factual knowledge\",\n",
    "    \"common-sense reasoning\",\n",
    "    \"reflect\",\n",
    "    \"communicate\",\n",
    "    \"categorize\",\n",
    "    \"plan\",\n",
    "    \"program/code\",\n",
    "    \"math\", \n",
    "    \"write\",\n",
    "    \"reading comprehension\",\n",
    "    \"summarize\",\n",
    "    \"storytell\",\n",
    "    \"explain\",\n",
    ")\n",
    "skill_set=set(skill_set)\n",
    "def construct_gpt_template(example:dict):  \n",
    "    prompt = f\"I am trying to cluster instructions/questions based on which skill is needed to answer them. \\\n",
    "                \\n To do this, given an instruction, please provide a comprehensive set of skills needed to answer it. Here is an incomplete set of possible skills needed to answer this instruction: {[skill for skill in skill_set]}. Feel free to use skills not in the list.\\\n",
    "                For each skill you list, please provide a score from 0 to 1 indicating how important that skill is to answer the instruction, the scores of all skills together must sum to 1.\\\n",
    "                \\n First, think step by step why would you need each of the skills. At the end of your output provide a comprehensive set of skills as a JSON-formatted dictionary, where the keys are the skills and values are the scores for the following instruction:\\\n",
    "                \\n### Instruction: {example['instruction']}.\"\n",
    "    if example['input']:\n",
    "        prompt+=f\" ### Input: {example['input']}.\"\n",
    "    prompt+=\"\\n### Skills:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://gcrgpt4aoai7.openai.azure.com/\"\n",
    "openai.api_version = \"2023-05-15\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inst_follow.operator import GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operator = GPT(model_name=\"gpt-4\")   \n",
    "inputs = [construct_gpt_template(example) for example in dataset.dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = asyncio.create_task(operator.gather_chat_response(inputs[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp in tqdm.tqdm(range(0, len(inputs), 10)):\n",
    "    print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = {\n",
    "    \"instruction\": [],\n",
    "    \"input\": [],\n",
    "    \"output\": [],\n",
    "    \"skill_set\": [],\n",
    "}\n",
    "import time\n",
    "for i,example in tqdm.tqdm(enumerate(dataset.dataset)):\n",
    "    if i==10:\n",
    "        break\n",
    "    template_to_fill = construct_gpt_template(example)\n",
    "    print(f\"PROMPT:\\n{template_to_fill}\")\n",
    "    failure = True\n",
    "    num_retries = 0        \n",
    "    while failure and num_retries < 3:\n",
    "        cache_row = None\n",
    "        try:\n",
    "            start = time.perf_counter()  \n",
    "            response = openai.ChatCompletion.create(\n",
    "                # model=\"gpt-3.5-turbo\",                \n",
    "                deployment_id=\"gpt-35-turbo\",\n",
    "                # model_name=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": template_to_fill},\n",
    "                ],\n",
    "            )\n",
    "            message = json.loads(str(response.choices[0]))[\"message\"][\"content\"]\n",
    "            print(message)\n",
    "            try:\n",
    "                # extract JSON from message\n",
    "                json_start = message.find(\"{\")\n",
    "                json_end = message.find(\"}\")   \n",
    "                json_string = message[json_start:json_end+1]\n",
    "                #parse JSON\n",
    "                skills_dict = json.loads(json_string)\n",
    "                skills_used = [skill.strip() for skill in skills_dict.keys() if skills_dict[skill]>0]\n",
    "                # append new skills to skill set\n",
    "                skill_set = set(list(skill_set)+skills_used)\n",
    "                \n",
    "                num_retries += 1\n",
    "                failure = False\n",
    "                new_dataset[\"instruction\"].append(example[\"instruction\"])\n",
    "                new_dataset[\"input\"].append(example[\"input\"])\n",
    "                new_dataset[\"output\"].append(example[\"output\"])\n",
    "                new_dataset[\"skill_set\"].append(skills_dict)\n",
    "            except:                \n",
    "                num_retries += 1\n",
    "                \n",
    "            end = time.perf_counter()\n",
    "            if end - start < 1:\n",
    "                time.sleep(1 - (end - start))\n",
    "        except:\n",
    "            time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new_dataset.json\", \"w\") as f:\n",
    "    json.dump(new_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"factual_knowledge\".strip().replace(\"_\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create cluster files fore specialized topics created by GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nomic import atlas\n",
    "import json\n",
    "import sys\n",
    "import tqdm\n",
    "import numpy as np\n",
    "sys.path.append(\"/home/v-oostapenko/dev/mttl\")\n",
    "from mttl.utils import hash_example\n",
    "from mttl.dataloader.alpaca_dataset_readers import AlpacaTemplateForHash as AlpacaTemplate\n",
    "from mttl.cluster_tuning.encodings import ClusterInfos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset    \n",
    "                  \n",
    "with open(\"data/SI_30k_GPT4_clust.jsonl\", \"r\") as f:\n",
    "    new_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Give three tips for staying healthy.',\n",
       " 'input': '',\n",
       " 'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.',\n",
       " 'gpt_output': 'Instruction 0: Give three tips for staying healthy.\\n\\nFor this instruction, the main action required is to \"list\" three tips that are related to the topic of health. The topic is more general than the ones provided in the pre-defined set, so a new topic keyword \"health\" is created.\\n\\n### JSON dict actions 0: {\"list\": 1}\\n### JSON dict topics 0: {\"health\": 1}\\n\\nInstruction 1: What are the three primary colors?\\n\\nIn this case, the main action required is to \"list\" the three primary colors. The topic for this instruction is \"colors\" from the pre-defined set of topics.\\n\\n### JSON dict actions 1: {\"list\": 1}\\n### JSON dict topics 1: {\"colors\": 1}\\n\\nInstruction 2: Describe the structure of an atom.\\n\\nFor this instruction, the main action required is to \"explain\" the structure of an atom. The topic is more general than the ones provided in the pre-defined set, so a new topic keyword \"chemistry\" is created.\\n\\n### JSON dict actions 2: {\"explain\": 1}\\n### JSON dict topics 2: {\"chemistry\": 1}',\n",
       " 'skill_set': {'list': 1},\n",
       " 'topic_set': {'health': 1}}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "topics = defaultdict(list)\n",
    "dataset = []  \n",
    "for i,d in enumerate(new_dataset):\n",
    "    # sample = {}\n",
    "    # sample[\"instruction\"] = d[\"instruction\"]\n",
    "    # sample[\"input\"] = d[\"input\"]\n",
    "    # sample[\"output\"] = d[\"output\"]\n",
    "    # sample[\"topics\"] = \" \".join(list(d[\"topic_set\"].keys()))\n",
    "    tt = d[\"skill_set\"]\n",
    "    tt = d[\"topic_set\"]\n",
    "    for t,v in tt.items():       \n",
    "        if v>0.5:\n",
    "            topics[t].extend([i])\n",
    "    # dataset.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_topics = []\n",
    "small_topics = []\n",
    "for t in topics:\n",
    "    # print(t, len(topics[t]))\n",
    "    if len(topics[t]) > 500:\n",
    "        large_topics.append(t)\n",
    "    else:\n",
    "        small_topics.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "math 1835\n",
      "language 1706\n",
      "computer science 3161\n",
      "grammar 990\n",
      "animals 928\n",
      "literature 552\n",
      "food 1110\n"
     ]
    }
   ],
   "source": [
    "for t in large_topics:\n",
    "    print(t, len(topics[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_idxs = []\n",
    "training_idxs = []\n",
    "for t in large_topics:\n",
    "    #shuffle topics[t]\n",
    "    print(t)\n",
    "    np.random.shuffle(topics[t])   \n",
    "    training_idxs.extend(topics[t][:700])\n",
    "    training_idxs.extend(topics[t][840:1590])\n",
    "    testing_idxs.extend(topics[t][700:840])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5040"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testing_idxs)+len(training_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_of_6_topics = []\n",
    "for i in training_idxs:   \n",
    "    dataset_of_6_topics.append(new_dataset[i])\n",
    "for i in testing_idxs:\n",
    "    dataset_of_6_topics.append(new_dataset[i])\n",
    "with open(\"data/SI_6_skills_5k.jsonl\", \"w\") as f:\n",
    "    json.dump(dataset_of_6_topics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4200/4200 [00:00<00:00, 6519.70it/s]\n",
      "100%|██████████| 840/840 [00:00<00:00, 6280.83it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cluster_infos = ClusterInfos() \n",
    "idxs={}\n",
    "for i in tqdm.tqdm(training_idxs):\n",
    "    example = new_dataset[i]\n",
    "    # topics_example = example[\"skill_set\"]\n",
    "    topics_example = example[\"topic_set\"]\n",
    "    enc_input = AlpacaTemplate.apply(example)    \n",
    "    hash = hash_example(enc_input)  \n",
    "        \n",
    "    probs = np.zeros((len(topics.keys())))\n",
    "    for ii, k in enumerate(topics_example): \n",
    "        if topics_example[k]==1.:  \n",
    "            idx_of_k_in_topics = list(topics.keys()).index(k) \n",
    "            probs[idx_of_k_in_topics]=topics_example[k]\n",
    "            idxs[k]=idx_of_k_in_topics\n",
    "            # print(k, idx_of_k_in_topics, topics_example[k])\n",
    "            break\n",
    "    assert np.sum(probs)==1\n",
    "    # print(np.sum(probs))    \n",
    "    main_t = np.argmax(probs)\n",
    "    cluster_infos.is_test.extend([0])   \n",
    "    cluster_infos.task_names.extend(\" \".join(list(topics_example.keys())[0]))\n",
    "    cluster_infos.cluster_ids.extend([int(main_t)])    \n",
    "    cluster_infos.hashes.extend([hash])\n",
    "    cluster_infos.cluster_dists.extend([probs.tolist()])\n",
    "for i in tqdm.tqdm(testing_idxs):\n",
    "    example = new_dataset[i]\n",
    "    # topics_example = example[\"skill_set\"]    \n",
    "    topics_example = example[\"topic_set\"]\n",
    "    enc_input = AlpacaTemplate.apply(example)    \n",
    "    hash = hash_example(enc_input)  \n",
    "        \n",
    "    probs = np.zeros((len(topics.keys())))\n",
    "    for i, k in enumerate(topics_example): \n",
    "        if topics_example[k]==1:\n",
    "            idx_of_k_in_topics = list(topics.keys()).index(k) \n",
    "            probs[idx_of_k_in_topics]=topics_example[k]\n",
    "            break\n",
    "    assert np.sum(probs)==1\n",
    "    # print(np.sum(probs))    \n",
    "    main_t = np.argmax(probs)  \n",
    "    cluster_infos.is_test.extend([1])   \n",
    "    cluster_infos.task_names.extend(\" \".join(list(topics_example.keys())))\n",
    "    cluster_infos.cluster_ids.extend([int(main_t)])    \n",
    "    cluster_infos.hashes.extend([hash])\n",
    "    cluster_infos.cluster_dists.extend([probs.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'math': 5,\n",
       " 'language': 7,\n",
       " 'computer science': 8,\n",
       " 'grammar': 15,\n",
       " 'animals': 18,\n",
       " 'food': 31}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_to_idx_path=\"/home/v-oostapenko/dev/mttl/inst_follow/data/hash_to_idx_si.json\"\n",
    "with open(hash_to_idx_path, \"r\") as f:\n",
    "                    hash_to_idx = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'afbd51db7f2e597e5fb551871ce002a4'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_infos.hashes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash 1713, d7ed467861871816805aa6c3884c1c4f not in hash_to_idx\n",
      "hash 1778, b299b97b87efdf0e552dc59a201fe3c0 not in hash_to_idx\n"
     ]
    }
   ],
   "source": [
    "for i,hash in enumerate(cluster_infos.hashes):\n",
    "    if hash not in hash_to_idx:\n",
    "        print(f\"hash {i}, {hash} not in hash_to_idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27527"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_to_idx[\"45af3def42ed8270b2e16d6e42611be8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx = training_idxs[423]\n",
    "example = new_dataset[idx]\n",
    "topics_example = example[\"topic_set\"]\n",
    "enc_input = AlpacaTemplate.apply(example)    \n",
    "hash = hash_example(enc_input)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request.            \\n### Instruction: Rewrite the formula $y = \\\\frac{2x-1},{3}$ so that it can be solved for x.            \\n### Response: The formula $y = \\\\frac{2x-1},{3}$ can be rewritten to solve for x as follows:\\n\\nFirst, we start by isolating the term involving x on one side of the equation. To do this, we multiply both sides by 3:\\n\\n$3y = \\\\frac{2x-1},{3}\\\\times 3$\\n\\n$3y = 2x-1$\\n\\nNext, we add 1 to both sides to isolate the 2x term:\\n\\n$3y + 1 = 2x -1 + 1$\\n\\n$3y + 1 = 2x$\\n\\nFinally, we divide both sides by 2 to solve for x:\\n\\n$\\\\frac{3y+1},{2} = \\\\frac{2x},{2}$\\n\\n$x =\\\\frac{3y+1},{2}$\\n\\nSo the formula $y = \\\\frac{2x-1},{3}$ can be rewritten to solve for x as $x = \\\\frac{3y+1},{2}$.'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_probs=np.array(cluster_infos.cluster_dists)\n",
    "cluster_probs=cluster_probs[:,cluster_probs.sum(0).nonzero()].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5040, 7)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5040, 7)\n"
     ]
    }
   ],
   "source": [
    "skill_ids_to_keep = np.where((np.array(cluster_infos.cluster_dists).sum(0))> 0)[0]  \n",
    "for i,d in enumerate(cluster_infos.cluster_dists):\n",
    "    cluster_infos.cluster_dists[i] = np.array(d)[skill_ids_to_keep].tolist()\n",
    "print(np.array(cluster_infos.cluster_dists).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_to_ids_path=\"/home/v-oostapenko/dev/mttl/inst_follow/cluster_infos/gpt_6_specialized_skills.pkl\"\n",
    "cluster_infos.save(example_to_ids_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset of 6 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dolly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mttl.dataloader.db_dolly_ds_reader import DB_DollyDataset\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"yahma/llama-7b-hf\", add_eos_token=True)\n",
    "tokenizer.pad_token_id = 0 \n",
    "max_input_length=256   \n",
    "max_output_length=128 \n",
    "data_dir=None               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset json (/home/v-oostapenko/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "100%|██████████| 1/1 [00:00<00:00, 832.20it/s]\n"
     ]
    }
   ],
   "source": [
    "dolly_ds = DB_DollyDataset(tokenizer, max_input_length, max_output_length, data_dir, train_on_inputs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset json (/home/v-oostapenko/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "100%|██████████| 1/1 [00:00<00:00, 1028.52it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\")[\"train\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'When did Virgin Australia start operating?',\n",
       " 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\",\n",
       " 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.',\n",
       " 'category': 'closed_qa'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closed_qa 100 1773\n",
      "classification 100 2136\n",
      "open_qa 100 3742\n",
      "information_extraction 100 1506\n",
      "brainstorming 100 1766\n",
      "general_qa 100 2191\n",
      "summarization 100 1188\n",
      "creative_writing 100 709\n"
     ]
    }
   ],
   "source": [
    "# topic idxs\n",
    "from collections import defaultdict\n",
    "topics = defaultdict(list)\n",
    "for i,ex in enumerate(dataset):\n",
    "    topics[ex[\"category\"]].append(i)\n",
    "rng = np.random.default_rng(1234)\n",
    "test_idxs = []        \n",
    "for t,v in topics.items():\n",
    "    ti = rng.choice(topics[t], size=100, replace=False)\n",
    "    test_idxs.extend(ti)\n",
    "    print(t, len(ti), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15011it [00:19, 753.32it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cluster_infos = ClusterInfos() \n",
    "idxs={}\n",
    "for i,example in tqdm.tqdm(enumerate(dolly_ds)):\n",
    "    # topics_example = example[\"skill_set\"]\n",
    "    topics_example = example.category\n",
    "    hash = example.hash        \n",
    "    probs = np.zeros((len(topics.keys())))           \n",
    "    topic_id = list(topics.keys()).index(topics_example)\n",
    "    probs[topic_id]=1\n",
    "    is_test = int(i in test_idxs)\n",
    "    assert np.sum(probs)==1\n",
    "    # print(np.sum(probs))    \n",
    "    main_t = np.argmax(probs)\n",
    "    cluster_infos.is_test.extend([is_test])     \n",
    "    cluster_infos.task_names.extend([topics_example])\n",
    "    cluster_infos.cluster_ids.extend([int(main_t)])    \n",
    "    cluster_infos.hashes.extend([hash])\n",
    "    cluster_infos.cluster_dists.extend([probs.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1773., 2136., 3742., 1506., 1766., 2191., 1188.,  709.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(cluster_infos.cluster_dists).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_to_ids_path=\"/home/v-oostapenko/dev/mttl/inst_follow/cluster_infos/db_dolly.pkl\"\n",
    "cluster_infos.save(example_to_ids_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter SI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set:\n",
    "# - only low entropy\n",
    "# - both, low and high\n",
    "# for both we need test examples with low entropy, i.e. per cluster\n",
    "# for both we need train examples with high entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset json (/home/v-oostapenko/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "100%|██████████| 1/1 [00:00<00:00, 753.02it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\xfe\\xf4\\xcdv\\xf5Qq\\x02\\xde<\\t\\'\\x97\\r\\x9e\\x1d\\xf5\\x1a \\x9f\\x05\\xd1\\x04.\\xf4\\xf12\\xdf\"\\xc5S\\xa3T^\\x83\\xb6\\x0e\\xae\\xb3to\\xa5\\x9a\\xe9\\xe8a=|\\xb2\\x8c\\x9e\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01', b'']\n",
      "Bad pipe message: %s [b'\\x1b\\xd3\\xdb\\xd0\\xa9\\xd0\\xcaat\\xf7\\xe8R\\xcd<\\xcd\\xf5\\xf4t \\xe2\\xef\\xa0\\xc3\\x0f<c\\xcf\\xe5\\xccm\\x11yF)\\xd0\\xb9\\x13\\xa2#\\xcf\\x89)T\\x9ed\\xd4\\nG}f\\xb8\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03']\n",
      "Bad pipe message: %s [b\"\\xd2%\\x0b\\x9c\\xb0f\\x15\\xd6#u\\x93\\x1b/\\\\B\\xb0d\\x88\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\"]\n",
      "Bad pipe message: %s [b'!S\\x9c\\xaa\\x0c\\x17N\\x1cAi\\x88(\\x00Y\\xb3\\xc0\\xef\\xdc\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005']\n",
      "Bad pipe message: %s [b'\\xf3\\x8c\\xa5\\xc55\\xf2\\x92\\xb8\\xee.h\\n\\x88=\\x90{\\\\\\xc2\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0']\n",
      "Bad pipe message: %s [b\"\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "# clsuter results\n",
    "from mttl.cluster_tuning.cluster_reader import ClusterResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp_3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
